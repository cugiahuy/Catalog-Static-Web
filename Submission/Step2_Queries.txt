Queries:

Courses: search=machine+learning&select=description&$count=true
{
  "@odata.context": "https://trainingcatalog.search.windows.net/indexes/course-index3/$metadata#docs(*)",
  "@odata.count": 61,
  "value": [
    {
      "@search.score": 17.251606,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm4zZjUzMDZhMS0yNDYwLTQ2MjgtOWFkYS04ZjA2MjhiMjBiYmQ1",
      "description": "Monitor models with Azure Machine Learning",
      "duration": 39,
      "instructor": "",
      "level": "intermediate",
      "product": "azure",
      "rating_average": 4.75,
      "rating_count": 504,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Monitor models with Azure Machine Learning",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 16.959589,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm5jMWY3ZDBlYi04Nzg2LTRmOTktOGViOS1kYjU1ZWY4Zjc4OWI1",
      "description": "Train a machine learning model with Azure Machine Learning",
      "duration": 40,
      "instructor": "",
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.63,
      "rating_count": 1998,
      "role": "student",
      "source": "MS Learn",
      "title": "Train a machine learning model with Azure Machine Learning",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 16.648943,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm5jZjhiNTViNy0yM2Q4LTQ3NjMtYWUzZC0zNjk2NmJjMDBlMzI1",
      "description": "Use automated machine learning in Azure Machine Learning",
      "duration": 45,
      "instructor": "",
      "level": "beginner",
      "product": "azure-machine-learning",
      "rating_average": 4.75,
      "rating_count": 3459,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Use automated machine learning in Azure Machine Learning",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 16.497307,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm5iMTQxNjQ5YS1lNTA5LTRmMGYtYjg0Yi1hNDM3YjZmMDRhZjc1",
      "description": "Explain machine learning models with Azure Machine Learning",
      "duration": 47,
      "instructor": "",
      "level": "intermediate",
      "product": "azure-portal",
      "rating_average": 4.76,
      "rating_count": 522,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Explain machine learning models with Azure Machine Learning",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 16.033827,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm5jOWVmNTdkYi0wN2NhLTQwNGYtOTg2Yi1lOWFhMTQ0ZmQ1NjI1",
      "description": "Deploy real-time machine learning services with Azure Machine Learning",
      "duration": 40,
      "instructor": "",
      "level": "beginner",
      "product": "azure-machine-learning",
      "rating_average": 4.65,
      "rating_count": 1340,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Deploy real-time machine learning services with Azure Machine Learning",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 15.321591,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm40NThjOGNjMy0xZDVmLTRmM2ItYTcxMy0yNWM3YjYyMDU1ZDY1",
      "description": "Use automated machine learning in Azure Machine Learning",
      "duration": 45,
      "instructor": "",
      "level": "beginner",
      "product": "azure-machine-learning",
      "rating_average": 4.75,
      "rating_count": 3459,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Use automated machine learning in Azure Machine Learning",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 14.149118,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm5hNWI1ZTdmYy0wNGM5LTQ0YTYtOGY2ZC0yZmZlNzY0NTE2Yzk1",
      "description": "Work with Data in Azure Machine Learning",
      "duration": 45,
      "instructor": "",
      "level": "beginner",
      "product": "azure-machine-learning",
      "rating_average": 4.66,
      "rating_count": 1119,
      "role": "student",
      "source": "MS Learn",
      "title": "Work with Data in Azure Machine Learning",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 13.919374,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm42NDU2YjZkOC0wYWYxLTRkZmQtYjg5NS0zN2VjZmIzZGY0MmI1",
      "description": "Introduction to the Azure Machine Learning SDK",
      "duration": 60,
      "instructor": "",
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.65,
      "rating_count": 2869,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Introduction to the Azure Machine Learning SDK",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 13.798687,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm45NGE4YWJmMi02ZTM3LTQwZWQtOGZlZC02YzIzZDUwYTc0MjQ1",
      "description": "Monitor data drift with Azure Machine Learning",
      "duration": 42,
      "instructor": "",
      "level": "intermediate",
      "product": "azure",
      "rating_average": 4.8,
      "rating_count": 814,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Monitor data drift with Azure Machine Learning",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 13.500952,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm45OGIyNGM5My0xMWJhLTQ1ZmItYmU4Yi0wN2ZjNDBiZWJkMTk1",
      "description": "Automate machine learning model selection with Azure Machine Learning",
      "duration": 25,
      "instructor": "",
      "level": "beginner",
      "product": "azure-machine-learning",
      "rating_average": 4.66,
      "rating_count": 1289,
      "role": "student",
      "source": "MS Learn",
      "title": "Automate machine learning model selection with Azure Machine Learning",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 13.500952,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm5jYTIyZmUwNS0yYWJjLTQ3ZDQtYTI3NS0yOGYzM2ZmYmY5MzI1",
      "description": "Automate machine learning model selection with Azure Machine Learning",
      "duration": 25,
      "instructor": "",
      "level": "beginner",
      "product": "azure-machine-learning",
      "rating_average": 4.66,
      "rating_count": 1289,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Automate machine learning model selection with Azure Machine Learning",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 13.500952,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm5kZDJiOGIwYS0wYzM0LTQ5M2QtYWVjYi1lZTM4Zjc3MTFlOWQ1",
      "description": "Deploy real-time machine learning services with Azure Machine Learning",
      "duration": 40,
      "instructor": "",
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.65,
      "rating_count": 1340,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Deploy real-time machine learning services with Azure Machine Learning",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 13.4892435,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm41ZGU0YmI4YS05MDYzLTRhMzgtYWRiNi00ZDhmNmYwYzEyYjE1",
      "description": "Work with Data in Azure Machine Learning",
      "duration": 45,
      "instructor": "",
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.66,
      "rating_count": 1119,
      "role": "student",
      "source": "MS Learn",
      "title": "Work with Data in Azure Machine Learning",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 13.4892435,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm42YjMyZTA0OC1kNGE4LTQyNmMtYWJmNy05NTYzNjk2NzA1NDk1",
      "description": "Work with Compute in Azure Machine Learning",
      "duration": 45,
      "instructor": "",
      "level": "beginner",
      "product": "azure-machine-learning",
      "rating_average": 4.68,
      "rating_count": 992,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Work with Compute in Azure Machine Learning",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 13.4892435,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm44MWQxZWM3ZC05NThmLTRlMGYtOTYwNy05ZWMxMzQ0YjY3YWI1",
      "description": "Work with Compute in Azure Machine Learning",
      "duration": 45,
      "instructor": "",
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.68,
      "rating_count": 992,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Work with Compute in Azure Machine Learning",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 13.364328,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm4xODFiYmNmMC02NzEzLTRlZTctYjg1OC0zY2Y1NDY3NGVkMTg1",
      "description": "Monitor models with Azure Machine Learning",
      "duration": 39,
      "instructor": "",
      "level": "intermediate",
      "product": "azure-portal",
      "rating_average": 4.75,
      "rating_count": 504,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Monitor models with Azure Machine Learning",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 13.137961,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm4zZjA0ODAzMC1kODAwLTRjYmUtOTI2MS0wMjRhNjQyZWNiZjc1",
      "description": "Automate machine learning model selection with Azure Machine Learning",
      "duration": 25,
      "instructor": "",
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.66,
      "rating_count": 1289,
      "role": "student",
      "source": "MS Learn",
      "title": "Automate machine learning model selection with Azure Machine Learning",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 13.137961,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm45MDQ3ZWMxZC0zMTkwLTRjOGItYmNiNC0wODE4YTY5ZWY4MTM1",
      "description": "Explain machine learning models with Azure Machine Learning",
      "duration": 47,
      "instructor": "",
      "level": "intermediate",
      "product": "azure-machine-learning",
      "rating_average": 4.76,
      "rating_count": 522,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Explain machine learning models with Azure Machine Learning",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 13.095675,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm4xNWYwMmM2MC0zMDFiLTQ0MTgtOTM2OC0wNDRjMGJiY2QzNmY1",
      "description": "Train a machine learning model with Azure Machine Learning",
      "duration": 40,
      "instructor": "",
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.63,
      "rating_count": 1998,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Train a machine learning model with Azure Machine Learning",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 13.095675,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm43OWU3ZTk5MC1jOTAyLTQ1OWUtODYzOS01NmQ5NjQ2NTIwZmM1",
      "description": "Explain machine learning models with Azure Machine Learning",
      "duration": 47,
      "instructor": "",
      "level": "intermediate",
      "product": "azure",
      "rating_average": 4.76,
      "rating_count": 522,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Explain machine learning models with Azure Machine Learning",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 13.095675,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm5mNTQ5MmExMC0wNzRlLTQ0MzYtYmRlZS05NzFjMjg1ZTU4MTE1",
      "description": "Train a machine learning model with Azure Machine Learning",
      "duration": 40,
      "instructor": "",
      "level": "beginner",
      "product": "azure-machine-learning",
      "rating_average": 4.63,
      "rating_count": 1998,
      "role": "student",
      "source": "MS Learn",
      "title": "Train a machine learning model with Azure Machine Learning",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 12.80204,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm4yZWI2NDk1My01ZjdhLTQ5ODUtODYzMi0zMTQwMTRjOGE4YzI1",
      "description": "Introduction to the Azure Machine Learning SDK",
      "duration": 60,
      "instructor": "",
      "level": "beginner",
      "product": "azure-machine-learning",
      "rating_average": 4.65,
      "rating_count": 2869,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Introduction to the Azure Machine Learning SDK",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 12.80204,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm41OWM1OGVmYS1kODMwLTRjNjUtOGFjNC01NDMyYmIwZWM3M2U1",
      "description": "Introduction to the Azure Machine Learning SDK",
      "duration": 60,
      "instructor": "",
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.65,
      "rating_count": 2869,
      "role": "student",
      "source": "MS Learn",
      "title": "Introduction to the Azure Machine Learning SDK",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 12.7746105,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm44NjhiMGZmNy05NWU1LTQ5ZjQtYjQ5MC0wZWI1MWIxMzZkODc1",
      "description": "Automate machine learning model selection with Azure Machine Learning",
      "duration": 25,
      "instructor": "",
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.66,
      "rating_count": 1289,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Automate machine learning model selection with Azure Machine Learning",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 12.7746105,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm5hZmUxNDE0Ny1lMDVhLTRjZmUtYjA4Ni05M2I0ODQzZmU1ZjA1",
      "description": "Train a machine learning model with Azure Machine Learning",
      "duration": 40,
      "instructor": "",
      "level": "beginner",
      "product": "azure-machine-learning",
      "rating_average": 4.63,
      "rating_count": 1998,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Train a machine learning model with Azure Machine Learning",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 12.485799,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm4zMzQ4YjkxMy04MWQ4LTQyYzgtYTUxMS0zNmI5ZDcwMDRmYTk1",
      "description": "Work with Azure Machine Learning to deploy serving models",
      "duration": 23,
      "instructor": "",
      "level": "intermediate",
      "product": "azure-databricks",
      "rating_average": 4.67,
      "rating_count": 49,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Work with Azure Machine Learning to deploy serving models",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 12.34379,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm4xNDViZjBlYi1lZGYzLTQ1MTgtYjg1OC00YWQxYTJmZmIwOGQ1",
      "description": "Learn enterprise AI management with our free open online course, including machine learning.",
      "duration": 30,
      "instructor": "",
      "level": "intermediate",
      "product": "m365",
      "rating_average": 4.75,
      "rating_count": 68,
      "role": "business-user",
      "source": "MS Learn",
      "title": "Start the machine learning lifecycle with MLOps",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 12.229389,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm4xOGVlN2M1OS05NWQwLTQ3MDktYTVhMy02ZDEzNjcyOTE4YTM1",
      "description": "Learn enterprise AI management with our free open online course, including machine learning.",
      "duration": 30,
      "instructor": "",
      "level": "intermediate",
      "product": "dynamics-365",
      "rating_average": 4.75,
      "rating_count": 68,
      "role": "functional-consultant",
      "source": "MS Learn",
      "title": "Start the machine learning lifecycle with MLOps",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 12.189766,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm42ZGUwOTQyNy0wYWRjLTQ5NGUtYTU4Yy02NTMxOWMwM2E3M2E1",
      "description": "Learn enterprise AI management with our free open online course, including machine learning.",
      "duration": 30,
      "instructor": "",
      "level": "intermediate",
      "product": "azure",
      "rating_average": 4.75,
      "rating_count": 68,
      "role": "functional-consultant",
      "source": "MS Learn",
      "title": "Start the machine learning lifecycle with MLOps",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 11.9646845,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm43ZGZjMWJiNC1hYmUyLTQyNzctYTczMi0xNjJmMWIzOGI0MTI1",
      "description": "Work with Compute in Azure Machine Learning",
      "duration": 45,
      "instructor": "",
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.68,
      "rating_count": 992,
      "role": "student",
      "source": "MS Learn",
      "title": "Work with Compute in Azure Machine Learning",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 11.9646845,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm45ZTJiZDUxZS0wZjI2LTQ4N2ItYjgyOC00YmQ2ZmQ4NTA3NDM1",
      "description": "Tune hyperparameters with Azure Machine Learning",
      "duration": 46,
      "instructor": "",
      "level": "intermediate",
      "product": "azure",
      "rating_average": 4.76,
      "rating_count": 544,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Tune hyperparameters with Azure Machine Learning",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 11.9646845,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm5iZjVhZjFkMS0xOTI5LTQyOTEtYWQ1MS0xZTUyMWQxOTI1NWQ1",
      "description": "Work with Compute in Azure Machine Learning",
      "duration": 45,
      "instructor": "",
      "level": "beginner",
      "product": "azure-machine-learning",
      "rating_average": 4.68,
      "rating_count": 992,
      "role": "student",
      "source": "MS Learn",
      "title": "Work with Compute in Azure Machine Learning",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 11.33527,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm41NTU2ZTQxMS1lYWRmLTQ1ODItYTkwNi0zNTc0NjEzNmNhYTQ1",
      "description": "Create a classification model with Azure Machine Learning designer",
      "duration": 60,
      "instructor": "",
      "level": "beginner",
      "product": "azure-machine-learning",
      "rating_average": 4.76,
      "rating_count": 1848,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Create a classification model with Azure Machine Learning designer",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 11.259609,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm4wYjBiOTIwYi1hMWQ3LTRlZTMtOGQ4Yy01NjcwYjIxNDQ1MDg1",
      "description": "Learn enterprise AI management with our free open online course, including machine learning.",
      "duration": 30,
      "instructor": "",
      "level": "intermediate",
      "product": "m365",
      "rating_average": 4.75,
      "rating_count": 68,
      "role": "functional-consultant",
      "source": "MS Learn",
      "title": "Start the machine learning lifecycle with MLOps",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 10.953273,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm4xNjY4MWMwNC1mODg1LTQ5ZDgtYWI3ZS1lNDc3OTM2YTZiZGM1",
      "description": "Tune hyperparameters with Azure Machine Learning",
      "duration": 46,
      "instructor": "",
      "level": "intermediate",
      "product": "azure-portal",
      "rating_average": 4.76,
      "rating_count": 544,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Tune hyperparameters with Azure Machine Learning",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 10.953273,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm4yYjBlZjE5NS1iMzk5LTRlMzAtYTYxZC0wN2IyYmNmOTA4NTc1",
      "description": "Monitor data drift with Azure Machine Learning",
      "duration": 42,
      "instructor": "",
      "level": "intermediate",
      "product": "azure-machine-learning",
      "rating_average": 4.8,
      "rating_count": 814,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Monitor data drift with Azure Machine Learning",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 10.953273,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm5hY2JiYTA1ZC00OTNhLTQ1NDgtYTVkZi1lY2YzYmExYzdhMDU1",
      "description": "Work with Data in Azure Machine Learning",
      "duration": 45,
      "instructor": "",
      "level": "beginner",
      "product": "azure-machine-learning",
      "rating_average": 4.66,
      "rating_count": 1119,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Work with Data in Azure Machine Learning",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 10.950797,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm40ZmZiN2JhMC0wNzcxLTRhMDEtOWRhNy05MTU2MzVmYjZiYTQ1",
      "description": "Monitor models with Azure Machine Learning",
      "duration": 39,
      "instructor": "",
      "level": "intermediate",
      "product": "azure-machine-learning",
      "rating_average": 4.75,
      "rating_count": 504,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Monitor models with Azure Machine Learning",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 10.950797,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm5hMjc0ZTg5Ny01YjJlLTRmZjMtYWVjOS0yMDY2ZTk0ZDlhNTE1",
      "description": "Work with Data in Azure Machine Learning",
      "duration": 45,
      "instructor": "",
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.66,
      "rating_count": 1119,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Work with Data in Azure Machine Learning",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 10.950797,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm5hNzE4MjE2OC00NjI1LTRmM2YtODM0OC1iZDBkZGM1NDJmNjM1",
      "description": "Introduction to the Azure Machine Learning SDK",
      "duration": 60,
      "instructor": "",
      "level": "beginner",
      "product": "azure-machine-learning",
      "rating_average": 4.65,
      "rating_count": 2869,
      "role": "student",
      "source": "MS Learn",
      "title": "Introduction to the Azure Machine Learning SDK",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 10.950797,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm5jM2U4MzlkNC0yOTEzLTRkNDQtODQzZC05ZWQ1MjUwYWZlODM1",
      "description": "Tune hyperparameters with Azure Machine Learning",
      "duration": 46,
      "instructor": "",
      "level": "intermediate",
      "product": "azure-machine-learning",
      "rating_average": 4.76,
      "rating_count": 544,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Tune hyperparameters with Azure Machine Learning",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 10.672424,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm5hNWZmZTAxNi02OWRmLTRiZTktYmU5YS03NmJkZmM0MDkyYzI1",
      "description": "Monitor data drift with Azure Machine Learning",
      "duration": 42,
      "instructor": "",
      "level": "intermediate",
      "product": "azure-portal",
      "rating_average": 4.8,
      "rating_count": 814,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Monitor data drift with Azure Machine Learning",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 10.571505,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm5jYTBiNWIzNS0yMjc2LTQ0YTctYjcwNC1jNTE5Y2U2YzhjMWE1",
      "description": "Detect and mitigate unfairness in models with Azure Machine Learning",
      "duration": 45,
      "instructor": "",
      "level": "intermediate",
      "product": "azure-machine-learning",
      "rating_average": 4.68,
      "rating_count": 429,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Detect and mitigate unfairness in models with Azure Machine Learning",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 10.524429,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm42YTY2ZDdlNi1hMmM5LTQ4OTYtYmEwZC03MzQ3NDRiZjZlN2U1",
      "description": "Learn enterprise AI management with our free open online course, including machine learning.",
      "duration": 30,
      "instructor": "",
      "level": "intermediate",
      "product": "dynamics-365",
      "rating_average": 4.75,
      "rating_count": 68,
      "role": "business-user",
      "source": "MS Learn",
      "title": "Start the machine learning lifecycle with MLOps",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 9.986819,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm42MmFhNmE0NC05MTI5LTQ2MzYtOGM3MC04OTRlMjMxMWJlNzI1",
      "description": "Deploy batch inference pipelines with Azure Machine Learning",
      "duration": 44,
      "instructor": "",
      "level": "intermediate",
      "product": "azure-machine-learning",
      "rating_average": 4.71,
      "rating_count": 572,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Deploy batch inference pipelines with Azure Machine Learning",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 9.986819,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm5kNmE4ZmQyZS1kM2E1LTQ0OWUtOGQwNy04MDRiNjFkMjBhNjA1",
      "description": "Deploy batch inference pipelines with Azure Machine Learning",
      "duration": 44,
      "instructor": "",
      "level": "intermediate",
      "product": "azure",
      "rating_average": 4.71,
      "rating_count": 572,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Deploy batch inference pipelines with Azure Machine Learning",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 9.92215,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm41MDM1OWE1Zi1jYjVkLTQ5MTYtYjYyYS1iYjgwZjZjZTYzMzk1",
      "description": "Build a web app powered by machine learning models trained using Custom Vision AI service, and learn how to refresh them with GitHub Actions.",
      "duration": 82,
      "instructor": "",
      "level": "intermediate",
      "product": "azure",
      "rating_average": 4.31,
      "rating_count": 32,
      "role": "developer",
      "source": "MS Learn",
      "title": "Build a Web App with Refreshable Machine Learning Models",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 9.716334,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm4yYjk0NDQ2Zi1hNjFiLTRmYmMtYTIwZS00ZjZjNjMwNjcyMmQ1",
      "description": "Create a Clustering Model with Azure Machine Learning designer",
      "duration": 49,
      "instructor": "",
      "level": "beginner",
      "product": "azure-machine-learning",
      "rating_average": 4.76,
      "rating_count": 2435,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Create a Clustering Model with Azure Machine Learning designer",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 9.716334,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm5iYmU2ZmZjYi0zYTU2LTRlYTItODcxNS1lODFlNWY0NDFhMDU1",
      "description": "Create a Regression Model with Azure Machine Learning designer",
      "duration": 55,
      "instructor": "",
      "level": "beginner",
      "product": "azure-machine-learning",
      "rating_average": 4.75,
      "rating_count": 2352,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Create a Regression Model with Azure Machine Learning designer",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    },
    {
      "@search.score": 9.716334,
      "PartitionKey": "ms-learn",
      "Key": "bXMtbGVhcm5mYWM4YTQ4NS0zMjFlLTQ3NzctYmY1OS1iNjFkNmQwMmQyNWQ1",
      "description": "Create a Regression Model with Azure Machine Learning designer",
      "duration": 55,
      "instructor": "",
      "level": "beginner",
      "product": "azure-machine-learning",
      "rating_average": 4.75,
      "rating_count": 2352,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Create a Regression Model with Azure Machine Learning designer",
      "people": [],
      "organizations": [],
      "locations": [],
      "keyphrases": [],
      "entities": []
    }
  ],
  "@odata.nextLink": "https://trainingcatalog.search.windows.net/indexes/course-index3/docs?api-version=2021-04-30-Preview&search=machine%2Blearning&select=description&$count=true&$skip=50"
}


Library: search=mobile+market&$count=true
{
  "@odata.context": "https://trainingcatalog.search.windows.net/indexes/paper-index1/$metadata#docs(*)",
  "@odata.count": 4,
  "value": [
    {
      "@search.score": 7.8768826,
      "content": "\nImproving prediction with enhanced \nDistributed Memory‑based Resilient Dataset \nFilter\nSandhya Narayanan1*, Philip Samuel2 and Mariamma Chacko3\n\nIntroduction\nAnalyzing and processing massive volumes of data in different applications like sensor \ndata, health care and e-Commerce require big data processing technologies. Extracting \nuseful information from the enormous size of unstructured data is a crucial thing. As the \namount of data becomes more extensive, sophisticated pre-processing techniques are \nrequired to analyze the data. In social networking sites and other online shopping sites, \na massive volume of online product reviews from a large size of customers are available \n[1]. The impact of online product reviews affects 90% of the current e-Commerce mar-\nket [2]. Customer reviews contribute the product sale to an extent and product life in the \nmarket depends on online product recommendations.\n\nOnline feedback is one of the communication methods which gives direct suggestions \nfrom the customers [3, 4]. Online reviews and ratings from customers are another infor-\nmation source about product quality [5, 6]. Customer reviews can help to decide on a new \nsuccessful product launch. Online shopping has several advantages over retail shopping. In \nretail shopping, the customers visit the shop and receive price information but less product \n\nAbstract \n\nLaunching new products in the consumer electronics market is challenging. Develop-\ning and marketing the same in limited time affect the sustainability of such companies. \nThis research work introduces a model that can predict the success of a product. A \nFeature Information Gain (FIG) measure is used for significant feature identification \nand Distributed Memory-based Resilient Dataset Filter (DMRDF) is used to eliminate \nduplicate reviews, which in turn improves the reliability of the product reviews. The \npre-processed dataset is used for prediction of product pre-launch in the market using \nclassifiers such as Logistic regression and Support vector machine. DMRDF method is \nfault-tolerant because of its resilience property and also reduces the dataset redun-\ndancy; hence, it increases the prediction accuracy of the model. The proposed model \nworks in a distributed environment to handle a massive volume of the dataset and \ntherefore, it is scalable. The output of this feature modelling and prediction allows the \nmanufacturer to optimize the design of his new product.\n\nKeywords: Distributed Memory-based, Resilient Distribution Dataset, Redundancy\n\nOpen Access\n\n© The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and \nthe source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material \nin this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material \nis not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the \npermitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creat iveco \nmmons .org/licen ses/by/4.0/.\n\nRESEARCH\n\nNarayanan et al. J Big Data            (2020) 7:13  \nhttps://doi.org/10.1186/s40537‑020‑00292‑y\n\n*Correspondence:   \nnairsands@gmail.com \n1 Information Technology, \nSchool of Engineering, \nCochin University of Science \n& Technology, Kochi 682022, \nIndia\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-020-00292-y&domain=pdf\n\n\nPage 2 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\ninformation from shop owners. On the other hand, online shopping sites give product \nreviews and previous customer feedbacks without extra cost and effort for the customers \n[7–10].\n\nInvesting in poor quality products potentially affects an industry’s brand loyalty and this \nstrategy should be changed by the eCommerce firms [5, 11]. Consumer product success \ndepends on different criteria, such as the quality of the product and marketing strategies. \nThe users should provide their valuable and accurate reviews about the products [12]. Cus-\ntomers bother to give reviews about products, whether they liked it or not. If the users \nprovide reviews, then other retailers can create some duplicated reviews [13, 14]. In online \nmarketing, the volume and value of product reviews are examined [15, 16]. The number \nof the product reviews on the shopping sites, blogs and forums has increased awareness \namong the users. This large volume of the reviews leads to the need for significant data \nprocessing methods [17, 18]. The value is the rating on the products. The ratio of positive to \nnegative reviews about the product leads to the quality of the product [19, 20].\n\nFeature selection is a crucial phase in data pre-processing [21]. Selecting features from \nan un-structured massive volume of data reduce the model complexity and improves the \nprediction accuracy. Different feature selection methods existing are the filter, wrapper and \nembedded. The wrapper feature selection method evaluates the usefulness of the feature \nand it depends on the performance of the classifier [22]. The filter method calculates the \nrelevance of the features and analyzes data in a univariate manner. The embedded process \nis similar to the wrapper method. Embedded and wrapper methods are more expensive \ncompared to the filter method. The state-of-art methods in customer review analysis gener-\nally discuss on categorizing positive and negative reviews using different natural language \nprocessing techniques and spam reviews recognition [23]. Feature selection of customer \nreviews increases prediction accuracy, thereby improves the model performance.\n\nAn enhanced method, which is a combination of filter and wrapper method is proposed \nin this work, which focuses on product pre-launch prediction with enhanced distributive \nfeature selection method. Since many redundant reviews are available on the web in large \nvolumes, a big data processing model has been implemented to filter out duplicated and \nunreliable data from customer reviews in-order to increase prediction accuracy. A scalable \nbig data processing model has been applied to predict the success or failure of a new prod-\nuct. The realization of the model has been done by Distributed Memory-based Resilient \nDataset Filter with prediction classifiers.\n\nThis paper is organized as follows. “Related work” section discusses related work. “Meth-\nodology” section contains the proposed methodology with System design, Resilient Distrib-\nuted Dataset and Prediction using classifiers. “Results and discussions” section summarizes \nresults and discussion. The conclusion of the paper is shown in “Conclusion and future \nwork” section.\n\nRelated work\nMakridakis et al. [24] illustrate that machine learning methods are alternative methods \nfor statistical analysis of multiple forecasting field. Author claims that statistical methods \nare more accurate than machine learning [25] methods. The reason for less accuracy is \nthe unknown values of data i.e., improper knowledge and pre-processing of data.\n\n\n\nPage 3 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nDifferent works have been implemented using the Matrix factorization (MF) [14] \nmethod with collaborative filtering [26]. Hao et al. [15] focused on a work based on the \nfactorization of the user rating matrix into two vectors, i.e., user latent and item latent \nwith low dimensionality. The sum of squared distance can be minimized by training a \nmodel that can find a solution using Stochastic Gradient Decent [27] or by least squares \n[28]. Salakhutdinov et al. [29] proposed a method that can be scaled linearly by probabil-\nity related matrix factorization on a big volume of datasets and then comparing it with \nthe single value decomposition method. This matrix factorization outperforms other \nprobability factorization methods like Bayesian-based probabilistic analysis [29] and \nstandard probability-based matrix factorization methods. A conventional approach, like \ntraditional collaborative Filtering [13, 30] method depends on customers and items. The \nuser item matrix factorization technique has been used for implementation purpose. \nIn the recommender system, there is a limitation in the sparsity problem and cold start \nproblem. In addition to the user item matrix factorization method, various analyses and \napproaches have been implemented to solve these recommendation issues.\n\nWietsma et al. [31] proposed a recommender system that gives information about the \nmobile decision aid and filtering function. This has been implemented with a study of \n29 features of student user behavior. The result shows the correlation among the user \nreviews and product reviews from different websites. Jianguo Chen et al. [32] proposed \na recommendation system for the treatment and diagnosis of the diseases. For cluster \nanalysis of disease symptoms, a density-peaked method is adopted. A rule-based apriori \nalgorithm is used for the diagnosis of disease and treatment. Asha et al. [33] proposed \nthe Gini-index feature method using movie review dataset. The sentimental analysis \nof the reviews are performed and opinion extraction of the sentences are done. Gini-\nindex impurity measure improves the accuracy of the polarity prediction by sentimental \nanalysis using Support vector machine [34, 35]. Depending on the frequency of occur-\nrence of a word in the document, the term frequency is calculated and opinion words \nare extracted using the Gini-index method. In this method, high term frequency words \nare not included, as it decreases the precision. The disadvantage of this method is that \nfor the huge volume of data, the prediction accuracy decreases.\n\nLuo et al. [36] proposed a method based on historical data to analyze the quality of \nservice for automatic service selection. Liu et al. [37] proposed a system in a mobile envi-\nronment for movie rating and review summarization. The authors used Latent Semantic \nAnalysis (LSA-based) method for product feature identification and feature-based sum-\nmarization. Statistical methods [38] have been used for identifying opinion words. The \ndisadvantage of this method is that LSA-based method cannot be represented efficiently; \nhence, it is difficult to index based on individual dimensions. This reduces the prediction \naccuracy in large datasets.\n\nLack of appropriate computing models for handling huge volume and redundancy in \ncustomer review datasets is a major challenge. Another major challenge handled in the \nproposed work is the existence of a pre-launch product in the industry based on the \nproduct features, which can be predicted based on the customer feedback in the form \nof reviews and ratings of the existing products. This prediction helps to optimize the \ndesign of the product to improve its quality with the required product features. Many \nof the relational database management systems are handling structured data, which is \n\n\n\nPage 4 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nnot scalable for big data that handles a large volume of unstructured data. This proposed \nmodel solves the problem of redundancy in a huge volume of the dataset for better pre-\ndiction accuracy.\n\nMethodology\nA pre-launch product prediction using different classifiers has been analysed by huge \ncustomer review and rating dataset. The product prediction is done through the phases \nconsisting of data collection phase, feature selection and duplicate data removal, build-\ning prediction classifier, training as well as testing.\n\nFigure 1 describes the various stages in system design of the model. The input dataset \nconsists of multivariate data which includes categorical, real and text data. Input dataset \nis fed for data pre-processing. Data pre-processing consists of feature selection, redun-\ndancy elimination and data integration which is done using Feature Information Gain \nand Distributed Memory-based Resilient Dataset Filter approach. The cleaned dataset \nis trained using classification algorithms. The classifiers considered for training are Sup-\nport Vector Machine (SVM) and Logistic Regression (LR). Further the dataset is tested \nfor pre-launch prediction using LR and SVM.\n\nData collection phase\n\nThis methodology can be applied for different products. Several datasets like Ama-\nzon and flip cart customer reviews are available as public datasets [39–41]. The data-\nset of customer reviews and ratings of seven brands of mobile phones for a period of \n24 months are considered in this work. The mobile phones product reviews are chosen \nbecause of two reasons. New mobile phones are launched into the market industry day \nby day which is one of the unavoidable items in everyone’s life. Market sustainability for \nthe mobile phones is very low.\n\nTable  1 shows a sample set of product reviews in which input dataset consists of \nuser features and product features. User features consists of Author, ReviewID and \nTitle depending on the user. Product feature consists of Product categories, Overall \nratings and Review Content. Since mobile phone is taken as the product, the catego-\nrization is done according to the features such as Battery life, price, camera, RAM, \n\nData collection \n\nCategorical\n\nText\n\nReal\n\nData Pre-\nprocessing\n\nFeature \nIdentification\n\nRedundancy\nRemoval\n\nData \nIntegration\n\nTraining \nDataset Using \nclassification \nalgorithms\n\nSupport \nVector \n\nLogistic \nRegression\n\nTesting Dataset \nUsing \n\nclassification \nalgorithms\n\nLogistic \nRegression\n\nSupport \nVector \n\nFig. 1 Product prelaunch prediction System Design\n\n\n\n\n\nPage 5 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nprocessor, weight etc. Some features are given a priority weightage depending on the \nproduct and user requirements. Input dataset with JSON file format is taken.\n\nDataset pre‑processing\n\nIn data pre-processing, feature selection plays a major role. In the product review \ndataset of a mobile phone, a large number of features exist. Identifying a feature from \ncustomer reviews is important for this model to improve the prediction accuracy. \nEnhanced Feature Information Gain measure has been implemented to identify sig-\nnificant feature.\n\nFeatures are identified based on the content of the product reviews, ratings of the \nproduct reviews and opinion identification of the reviews. Ratings of the product \nreviews can be further categorized based on a rating scale of 5 (1—Bad, 2—Average, \n3—Good, 4—very good, 5—Excellent). For opinion identification of the product, the \npolarity of extracted opinions for each review is classified using Senti-WordNet [42].\n\nFeature Information Gain measures the amount of information of a feature \nretrieved from a particular review. Impurity which is the measure of reliability of fea-\ntures in the input dataset should be reduced to get significant features. To measure \nfeature impurity, the best information of a feature obtained from each review is calcu-\nlated as follows\n\n• Let Pi be the probability of any feature instance \n(\n\nf\n)\n\n of k feature set F =\n{\n\nf1, f2, . . . fk\n}\n\n \nbelonging to  ith customer review Ri , where i varies from 1 to N.\n\n• Let N denotes the total number of customer reviews.\n• Let OR denotes the polarity of extracted opinions of the Review.\n• Let SR denotes product rating scale of review (R).\n\nTable 1 Sample set of Product Reviews\n\n\n\nPage 6 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nThe information of a feature with respect to review rating and opinion is denoted by \nIf\n\nExpected information gain of the feature denoted as Ef\n\nReview Feature Impurity R(I) is calculated as\n\nThen Feature Information Gain (�G) to find out significant features are calculated \nas\n\nFeatures are selected based on the �G value and those with an Information gain \ngreater than 0.5 is selected as a significant feature. Table 2 shows the significant fea-\nture from customer reviews and ratings.\n\nNext step is to eliminate the redundant reviews and to replace null values of an \nactive customer from the customer review dataset using an enhanced big data pro-\ncessing approach. Reviews with significant features obtained from feature identifica-\ntion are considered for further processing.\n\n(1)If = log2\n\n(\n\n1\n\nP(R = F)\n\n)\n\n∗ OR ∗ SR.\n\n(2)Ef =\n\nN\n∑\n\ni=1\n\n−Pi(R = F).\n∥\n\n∥If\n∥\n\n∥\n\n1\n.\n\n(3)R(I) = −\n\nN\n∑\n\ni=1\n\nPi.log2Ef .\n\n(4)�G = R(I)−\n\nN\n∑\n\ni=1\n\n[(\n\nOR\n\nN\n∗ Ef\n\n)\n\n−\n\n(\n\nSR\n\nN\n∗ Ef\n\n)]\n\n.\n\nTable 2 Significant Features from Customer Reviews and Ratings\n\nNo Customer reviewed features No Customer reviewed features\n\n1 Author 17 RAM\n\n2 Title 18 Sim type\n\n3 ReviewID 19 Product category\n\n4 Content 20 Thickness\n\n5 Product brand 21 Weight of mobile phone\n\n6 Ratings 22 Height\n\n7 Battery life 23 Product type\n\n8 Price 24 Product rating\n\n9 Feature information gain 25 Front camera\n\n10 Review type 26 Back camera\n\n11 Product display 27 Opinion of review\n\n12 Processor 28 Multi-band\n\n13 Operating system 29 Network support\n\n14 Water proof 30 Quick charging\n\n15 Rear camera 31 Finger sensor\n\n16 Applications inbuilt 32 Internal storage\n\n\n\nPage 7 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nResilient Distributed Dataset\n\nResilient Distributed Dataset (RDD) [43] is a big data processing approach, which allows \nto store cache chunks of data on memory and persevere it as per the requirements. \nThe in-memory data caching is supported by RDD. Variety of jobs at a point of time \nis another challenge which is handled by RDD. This method deals with chunks of data \nduring processing and analysis. RDD can also be used for machine learning supported \nsystems as well as in big data processing and analysis, which happens to be an almost \npervasive requirement in the industry.\n\nIn the proposed method the main actions of RDD are:\n\n• Reduce (β): Combine all the elements of the dataset using the function β.\n• First (): This function will return the first element\n• takeOrdered(n): RDD is returned with first ‘n’ elements.\n• saveAsSequenceFile(path): the elements in the dataset to be written to the local file \n\nsystem with given path.\n\nThe main Transformations of RDD are:\n\n• map(β): Elements from the input file is mapped and new dataset is returned through \nfunction β.\n\n• filter(β): New dataset is returned if the function β returns true.\n• groupBykey(): When called a dataset of (key, value) pairs, this function returns a \n\ndataset of (key, value) pairs.\n• ReduceBykey(β): A (key, value) pair dataset is returned, where the values of each key \n\nare combined using the given reduce function β.\n\nIn the proposed work an enhanced Distributed Memory-based Resilience Dataset \nFilter (DMRDF) is applied. DMRDF method have long Lineage and it is recomputed \nthemselves using prior information, thus it achieves fault-tolerance. DMRDF has been \nimplemented to remove the redundancy in the dataset for product pre-launch predic-\ntion. This enhanced method is simple and fast.\n\n• Let the list of n customers represented as C = {c1, c2, c3 . . . , cn}\n\n• Let the list of N reviews be represented as R = {r1, r2, r3 . . . , rN }\n\n• Let x significant features are identified from feature set (F  ) represented as Fx ⊂ F\n\n• An active customer consists of significant feature having information Gain value \ndenoted by �G\n\nIn the DMRDF method, a product is chosen and its customer reviews are found out. \nEliminate customers with similar reviews on the selected product and also reviews \nwith insignificant features. Calculate the memory-based Resilient Dataset Filter score \nbetween each of the customer reviews with significant features.\n\nLet us consider a set C of ‘n’ number of customers, the set R of ‘N’ number of reviews and \na set of significant features ′F ′\n\nx are considered. The corresponding vectors are represented \nas KC , KR and KFx . Then KRi is represented using a row vector and KFj is represented using \nthe column vector. Each entry KCm denote the number of times the  mth review arrives in \n\n\n\nPage 8 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\ncustomers. The similarities between ith review of mth customer is found out using  L1 norm \nof KRi and KCm . The Distributed Memory-based resilient filter score δ is calculated using the \nEq. (5).\n\nThe δ score is calculated for each customer review whereas the score lies between [0,1]. \nThe significant features are found out using Eq. 4. For customer reviews without significant \nfeatures, �G value will be zero. The reviews with δ score value 0 are found to be insignificant \nwithout any significant feature or opinion and hence those reviews are eliminated and not \nconsidered for further processing in the work. More than one Distributed Memory-based \nresilient filter score value is identified then the second occurrence of the review is consid-\nered as duplicate.\n\nPrediction classifiers\n\nLogistic regression and Support Vector Machine classifiers are the supervised machine \nlearning approaches used in the proposed work for product pre-launch prediction.\n\nLogistic regression (LR)\n\nWe have implemented proposed model using logistic regression analysis for prediction. \nThis model predicts the failure or success of a new product in the market by analysing \nselected product features from customer reviews. A case study has been conducted using \nthe dataset of customer reviews of mobile phones. Success or failure is the predictor vari-\nable used for training and testing the dataset. For training the model 75% of the dataset is \nused and for testing the model, remaining 25% is used.\n\n• Let p be the prediction variable value, assigning 0 for failure and 1 for success.\n• p0 is the constant value.\n• b is the logarithmic base value.\n\nThen the logit function is,\n\nThen the Logistic regression value γ is shown in Eq. (7),\n\n(5)δ =\n\nN\nn\n�\n\ni = 1\n\nm = 1\n\n\n\n\n\n�\n\nKRi ∗\n\n�\n\n�x\nj=1 KFj\n\n��\n\n∗ KCm\n\nKRi · KCm\n\n\n\n ∗ |�G|\n\n(6)\nL0 = b\n\np0+p\nx\n∑\n\ni=1\n\nfi\n\n(7.1)γ =\nL0\n\n(\n\nbp0+p\n∑x\n\ni=1 fi\n)\n\n+ 1\n\n(7.2)=\n1\n\n1+ b\n−\n\n(\n\nb\np0+p\n\n∑x\ni=1\n\nfi\n)\n\n\n\nPage 9 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nThe probability value of γ lies between [0,1]. In this work, if this value is greater than 0.5 \nthe pre-launch prediction of the product is considered as success and for values less than \n0.5, it is considered as failure.\n\nSupport Vector Machine (SVM)\n\nSVM is the supervised machine learning method, used to learn from set of data to get new \nskills and knowledge. This classification method can learn from data features relationships \n( zi ) and its class \n\n(\n\nyi\n)\n\n that can be applied to predict the success or failure class the product \nbelongs to.\n\n• For a set T  of t training feature vectors, zi ∈ RD, where i = 1 to t.\n• Let yi ∈ {+1,−1} , where +1 belongs to product success class and -1 belongs to product \n\nfailure class.\n• The data separation occurs in the real numbers denoted as X in the D dimensional \n\ninput space.\n• Let w be the hyper plane normal vector element, where w ∈ XD.\n\nThe hyper plane is placed in such a way that distance between the nearest vectors of the \ntwo classes to the hyperplane should be maximum. Thus, the decision hyper plane is calcu-\nlated as,\n\nThe conditions for training dataset d ∈ X , is calculated as\n\nTo maximize the margin the value of w should be minimized.\nThe products in the positive one class (+1) are considered as successful products, [from \n\nEq. (9)] and those in the negative one class (−1) [from Eq. (10)] are in failure class.\n\nExperimental setup\n\nThe proposed system was implemented using Apache Spark 2.2.1 framework. Spark pro-\ngramming for python using PySpark version 2.1.2, which is the Spark python API has been \nused for the application development. An Ubuntu running Apache web server using Web \nServer Gateway Interface is used. Amazon Web Services is used to run some components \nof the software system large servers (nodes), having two Intel Xeon E5-2699V4 2.2 G Hz \nprocessors (VCPUs) with 4 cores and 16 GB of RAM on different Spark cluster configura-\ntions. According to the scalability requirements the software components can be config-\nured and can run on separate servers.\n\n(8)α(w) =\n2\n\n�w�\n\n(9)wtzi + d ≥ 1, where yi = +1.\n\n(10)wtzi + d ≤ −1, whereyi = yi − 1.\n\n\n\nPage 10 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nResults and discussions\nTo evaluate our prediction system several case studies have been conducted. Support \nVector Machine and Logistic regression classifiers are employed to perform the predic-\ntion. Most significant customer review features are used to analyse the system perfor-\nmance. The prediction accuracy evaluation is taken as one of the system design factors. \nThe system response time is another major concern for big data processing system. In \nthe customer review feature identification, we propose feature information gain and \nDMRDF approach to identify significant features and to eliminate redundant customer \nreviews from the input dataset.\n\nFigure  2 illustrates significant features required for the mobile phone sustainability. \nCustomer reviews and ratings of 7 brands of mobile phones are identified and evalu-\nated with DMRDF using SVM and LR. The graph shows the significant features identi-\nfied by the model against the percentage of customers whose reviews are analysed. 88% \nof the customers identified internal storage as a significant feature. Product price has \nbeen identified by 79% of customers as significant feature. With this evaluation customer \nrequirements for a product can be analysed in a better manner, thus can optimize the \ndesign of the product for better product quality and for product sustainability in the \nindustry.\n\nFigure 3 shows the comparison of the processing time taken by the proposed model \nwith different dataset size against that of the state of art techniques. DMRDF method \ntakes less time for completion of the application compared to other gini-index and latent \nsemantic analysis methods. Hence the proposed model is fast and scalable. It provides a \nhigh-speed processing performance with large datasets. This shows the DMRDF applica-\nbility in big data analytics, whereas gini-index and LSA-based methods processing time \nis larger for large volume of dataset. From the Fig. 3 it can be seen that with 9 GB dataset \ntime taken for prediction using LSA-based model, Gini-index model and DMRDF model \nis 342 s, 495 s and 156 s respectively. With 18 GB dataset time taken for prediction using \nLSA-based model, Gini-index model and DMRDF model 740 s, 910 s and 256 s respec-\ntively. Gini-index and LSA-based methods time taken for 18 GB dataset is twice that of \n9 GB dataset. But for DMRDF model time taken for 18 GB dataset is 1.6 times that of \n\n79%\n\n15%\n\n45%\n35%\n\n22%\n\n40%\n\n22%\n\n39%\n\n88%\n\n53%\n\n21%\n\n61%\n\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n\n100%\n\nPe\nrc\n\nen\nta\n\nge\n o\n\nf C\nus\n\nto\nm\n\ner\ns\n\nIden�fied Significant Features\nFig. 2 Identified Significant Features from Customer reviews and Ratings\n\n\n\nPage 11 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\n9 GB dataset and also it is 3 times lesser than Gini-index method. DMRDF model has \nmore advantage compared to the other state of art techniques in the case of application \nexecution and performance.\n\nThe reliability of the methods considered for the pre-launch prediction depends on \nprecision [44], recall and prediction accuracy measurement. Table 5 shows a comparison \nof precision, recall and accuracy measures of DMRDF, Gini-index and LSA-based meth-\nods with Support Vector Machine and Logistic Regression classifiers using customer \nreviews dataset over a period of 24 months. The results shown in Table 3 are best proved \nusing DMRDF with Support Vector Machine classification with prediction accuracy of \n95.4%. The DMRDF outperforms LSA-based and Gini-index methods in P@R, R@R and \nPA measures. Using proposed method, true positive (TP), false positive (FP), true nega-\ntive (TN) and false negative (FN) are found out. The prediction accuracy (PA), precision \n(P@R) and recall (R@R) are computed using Eqs. (10), (11), and (12) respectively.\n\n(10)PA =\nTP + TN\n\nTP + TN + FP + FN\n\n0\n\n100\n\n200\n\n300\n\n400\n\n500\n\n600\n\n700\n\n800\n\n900\n\n1000\n\n1GB 5GB 9GB 13GB 18GB\n\nGini-index\n\nDMRDF\n\nLSA-based\n\nTi\nm\n\ne \nTa\n\nke\nn \n\nin\n se\n\nc\n\nDataset size\nFig. 3 Dataset Size versus Processing Time Graph\n\nTable 3 Performance comparison of the proposed model with state of art techniques\n\nClassifier Support vector machine\n\nMethod used P@R (precision) PA % \n(prediction \naccuracy)\n\nDMRDF 0.941 0.92 95.4\n\nLSA-based 0.894 0.79 87.5\n\nGini-index 0.66 0.567 83.2\n\nClassifier Logistic regression\n\nMethod used P@R R@R % PA %\n\nDMRDF 0.915 0.849 93.5\n\nLSA-based 0.839 0.753 83\n\nGini-index 0.62 0.52 79.8\n\n\n\nPage 12 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nUsing DMRDF with SVM classifier and LR classifier, the prediction accuracy varia-\ntions are less compared to LSA-based and Gini-index methods. Hence DMRDF out-\nperforms the other two methods for customer review feature prediction.\n\nFurthermore Fig.  4, shows the DMRDF, LSA-based and Gini-index approaches as \napplied to the customer reviews and ratings datasets for 3, 6, 12, 18 and 24 months. \nIn DMRDF many features may appear in different customer review aspects, hence \nperformance evaluation will not consider duplicate customer reviews. In Gini- index, \nfeatures are extracted based on the polarity of the reviews and for large dataset P@R \nand R@R are less. The results show that DMRDF method outperforms the other two \nmethods in big data analysis. Gini-index approach does not perform well in customer \nreview feature prediction.\n\nConclusion and future work\nTechnological development in this era brings new challenges in artificial intelligence \nlike prediction, which is the next frontier for innovation and productivity. This work \nproposes the implementation of a scalable and reliable big data processing model \n\n(11)P@R =\nTP\n\nTP + FP\n\n(12)R@R =\nTP\n\nTP + FN\n\na SVM b SVM \n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nP@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nR@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\nc Logistic Regression d Logistic Regression\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nP@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nR@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\nFig. 4 Precision and Recall of DMRDF, LSA-based and Gini-index methods using SVM and LR classifiers\n\n\n\nPage 13 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nwhich identify significant features and eliminates redundant data using Feature Infor-\nmation Gain and Distributed Memory-based Resilient Dataset Filter method with \nLogistic Regression and Support Vector Machine prediction classifiers. A compari-\nson of the analysis has been conducted with state of art techniques like Gini-index \nand LSA-based approaches. The prediction accuracy, precision and recall of DMRDF \nmethod outperforms the other methods. Results show that the prediction accuracy \nof the proposed method increases by 10% using significant feature identification and \nelimination of redundancy from dataset compared to state of art techniques. Large \nfeature dimensionality reduces the prediction accuracy of the LSA-based method \nwhere as number of significant features plays an important role in prediction model-\nling. Results show that proposed DMRDF model is scalable and with huge volume of \ndataset model performance is good as well as time taken for processing the applica-\ntion is less compared to state of art techniques.\n\nResilience property of DMRDF method have long lineage, hence this can achieve \nfault-tolerance. DMRDF model is fast because of the in-memory computation \nmethod. Proposed design can be extended to other product feature identification big \ndata processing domains. As a future work, the model may be developed to make real \ntime streaming predictions through a unified API that searches customer comments, \nratings and surveys from different reliable online websites concurrently to obtain syn-\nthesis of sentiments with an information fusion approach. Since the statistical prop-\nerties of customer reviews and ratings vary over time, the performance of machine \nlearning algorithms can also come down. To cope with the limitations of deep learn-\ning matrix factorization integrated with DMRDF can be adapted.\n\nAbbreviations\nDMRDF: Distributed Memory-based Resilient Dataset Filter; FIG: Feature information gain; RDD: Resilient distributed \ndataset; SVM: Support vector machine; LR: Logistic regression; LSA: Latent semantic analysis; PA: Prediction accuracy; \nP@R: Precision; R@R: Recall; MF: Matrix factorization.\n\nAcknowledgements\nNot applicable.\n\nAuthors’ contributions\nSN designed and implemented the model for Pre-launch product prediction. SN analysed and interpreted the customer \nreviews and ratings dataset regarding the pre-launch product prediction. PS supervised the design, implementation \nand analysis of the model for pre-launch product prediction. MC was a major contributor in writing the manuscript. All \nauthors read and approved the final manuscript.\n\nFunding\nNot applicable.\n\nAvailability of data and materials\nThe datasets generated and/or analysed during the current study are available in the Kaggle repository. [snap.stanford.\nedu/data/web-Amazon.html] [40] and [http://www.kaggl e.com/Promp tClou dHQ/flipk art-produ cts] [39].\n\nCompeting interests\nThe authors declare that they have no competing interests.\n\nAuthor details\n1 Information Technology, School of Engineering, Cochin University of Science & Technology, Kochi 682022, India. \n2 Department of Computer Science, Cochin University of Science & Technology, Kochi 682022, India. 3 Department \nof Ship Technology, Cochin University of Science & Technology, Kochi 682022, India. \n\nReceived: 25 October 2019   Accepted: 17 February 2020\n\n\n\nhttp://www.kaggle.com/PromptCloudHQ/flipkart-products\n\n\nPage 14 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nReferences\n 1. Lau RY, Liao SY, Kwok RC, Xu K, Xia Y, Li Y. Text mining and probabilistic modeling for online review spam detection. \n\nACM Trans Manag Inform Syst. 2011;2(4):25.\n 2. Lin X, Li Y, Wang X. Social commerce research: definition, research themes and the trends. Int J Inform Manag. \n\n2017;37:190–201.\n 3. Matos CAD, Rossi CAV. Word-of-mouth communications in marketing: a meta-analytic review of the antecedents \n\nand moderators. J Acad Market Sci. 2008;36(4):578–96.\n 4. Jeon S, et al. Redundant data removal technique for efficient big data search processing. Int J Softw Eng Appl. \n\n2013;7.4:427–36.\n 5. Dave K, Lawrence S, and Pennock D. Mining the peanut gallery: opinion extraction and semantic classification of \n\nproduct reviews. WWW’2003.\n 6. Zhou Y, Wilkinson D, Schreiber R, Pan R. Large-scale parallel collaborative filtering for the netflix prize. 2008. p. \n\n337–48. https ://doi.org/10.1007/978-3-540-68880 -8_32.\n 7. Zhang KZK, Benyoucef M. Consumer behavior in social commerce: a literature review. Dec Support Syst. \n\n2016;86:95–108.\n 8. Cui Geng, Lui Hon-Kwong, Guo Xiaoning. The effect of online consumer reviews on new product sales. Int J Electron \n\nComm. 2012;17(1):39–58.\n 9. Manek AS, Shenoy PD, Mohan MC, et al. Detection of fraudulent and malicious websites by analysing user reviews \n\nfor online shopping websites. Int J Knowl Web Intell. 2016;5(3):171–89. https ://doi.org/10.1007/s1128 0-015-0381-x.\n 10. Singh S, and Singh N. Big data analytics. In: Proceedings of the 2012 international conference on communication, \n\ninformation & computing technology (ICCICT), institute of electrical and electronics engineers (IEEE). 2012. p. 1–4. \nhttp://dx.doi.org/10.1109/iccic t.2012.63981 80.\n\n 11. Demchenko Yuri et al. Addressing big data challenges for scientific data infrastructure. In: IEEE 4th Int. conference \ncloud computing technology and science (CloudCom). 2012.\n\n 12. Sihong Xie, Guan Wang, Shuyang Lin and Yu Philip S. Review spam detection via time-series pattern discovery. In: \nACM Proceedings of the 21st international conference companion on World Wide Web. 2012. p. 635–6.\n\n 13. Koren Y, Bell R, Volinsky C. matrix factorization technique for recommender systems. Computer. 2009;8:30–7.\n 14. Salakhutdinov R, Mnih A, & Hinton G. Restricted boltzmann machines for collaborative filtering. In: Proc. of the 24th \n\nInt. conference on machine learning. 2007. p. 791–8.\n 15. Hao MA, King I, Lyu MR. Learning to recommend with explicit and implicit social relations. ACM Trans Intell Syst \n\nTechnol. 2011;2(3):29.\n 16. Bandakkanavar V, Ramesh M, Geeta V. A survey on detection of reviews using sentiment classification of methods. \n\nIJRITCC. 2014;2(2):310–4.\n 17. Gu V, and Li H. Memory or time—performance evaluation for iterative operation on hadoop and spark. In: Proc. of \n\nthe 2013 IEEE 10th Int. Con. on high-performance computing and communications. 2013. https ://doi.org/10.1109/\nhpcc.and.euc.2013.106.\n\n 18. Zhang Hanpeng, Wang Zhaohua, Chen Shengjun, Guo Chengqi. Product recommendation in online social net-\nworking communities—an empirical study of antecedents and a mediator. J Inform Manag. 2019;56(2):185–95.\n\n 19. Ghose A, Ipeirotis PG. Designing novel review ranking systems: predicting the usefulness and impact of reviews. In: \nInt Conference Electron Comm ACM. 2007. p. 303–10.\n\n 20. Chong AY, Ch’ng E, Liu MJ, Li B. Predicting consumer product demands via Big Data: the roles of online promotional \nmarketing and online reviews. Int J Prod Res. 2015;55:1–15. https ://doi.org/10.1080/00207 543.2015.10665 19.\n\n 21. Yang H, Fujimaki R, Kusumura Y, & Liu J. Online Feature Selection. In: Proceedings of the 22nd ACM SIGKDD Int. \nConference on KDD ‘16, 2016. https ://doi.org/10.1145/29396 72.29398 81.\n\n 22. Breese JS, Heckerman D, and Kadie C. Empirical analysis of predictive algorithms for collaborative filtering. In: Proc. \nof the 14th Conf. on Uncertainty in Artifical Intelligence, 1998.\n\n 23. Mukherjee A, Kumar A, Liu B, Wang J, Hsu M, Castellanos M, Ghosh R. Spotting opinion spammers using behavioral \nfootprints. In: Proc. of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining \nChicago, ACM. 2013. p. 632–40.\n\n 24. Makridakis S, Spiliotis E, Assimakopoulos V. Statistical and Machine Learning forecasting methods: concerns and \nways forward. PLoS ONE. 2018;13(3):e0194889. https ://doi.org/10.1371/journ al.pone.01948 89.\n\n 25. Imon A, Roy C, Manos C, Bhattacharjee S. Prediction of rainfall using logistic regression. Pak J Stat Oper Res. 2012. \nhttps ://doi.org/10.18187 /pjsor .v8i3.535.\n\n 26. Chen T, Zhang W, Lu Q, Chen K, Zheng Z, Yu Y. SVD Feature: a toolkit for feature-based collaborative filtering. J Mach \nLearn Res. 2012;13(1):3619–22.\n\n 27. Shi Y, Larson M, Hanjalic A. Collaborative filtering beyond the user-item matrix—a survey of the state of art and \nfuture challenges. ACM Comput Surv. 2014;47(1):3.\n\n 28. Shan H, & Banerjee A. Generalized probabilistic matrix factorizations for collaborative filtering, In Data mining \n(ICDM), IEEE 10th international conference. 2010. p. 1025–30.\n\n 29. Salakhutdinov R, & Mnih A. Bayesian probabilistic matrix factorization using Markov chain Monte Carlo. In: Proc. of \nthe 25th int. conference on machine learning. 2008. p. 880–7.\n\n 30. Crawford M, Khoshgoftaar TM, Prusa JD, Richter AN, Al Najada H. Survey of review spam detection using machine \nlearning techniques. J Big Data. 2015;2(1):23.\n\n 31. Wietsma TA, Ricci F. Product reviews in mobile decision aid systems. Francesco: PERMID; 2005. p. 15–8.\n 32. Jianguo C, et al. A disease diagnosis and treatment recommendation system based on big data mining and cloud \n\ncomputing. Inform Sci. 2018;435:124–49.\n 33. Manek AS, Shenoy PD, Mohan MC, Venugopal KR. Aspect term extraction for sentiment analysis in large movie \n\nreviews using Gini-index feature selection method and SVM classifier. World Wide Web. 2017;20:135–54. https ://doi.\norg/10.1007/s1128 0-015-0381-x.\n\n 34. Fan RE, Chang K-W, Hsieh C-J, Wang X-R, Lin C-J. LIBLINEAR: A library for large linear classification. J Mach Learn Res. \n2008;9:1871–4.\n\nhttps://doi.org/10.1007/978-3-540-68880-8_32\nhttps://doi.org/10.1007/s11280-015-0381-x\nhttp://dx.doi.org/10.1109/iccict.2012.6398180\nhttps://doi.org/10.1109/hpcc.and.euc.2013.106\nhttps://doi.org/10.1109/hpcc.and.euc.2013.106\nhttps://doi.org/10.1080/00207543.2015.1066519\nhttps://doi.org/10.1145/2939672.2939881\nhttps://doi.org/10.1371/journal.pone.0194889\nhttps://doi.org/10.18187/pjsor.v8i3.535\nhttps://doi.org/10.1007/s11280-015-0381-x\nhttps://doi.org/10.1007/s11280-015-0381-x\n\n\nPage 15 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\n 35. Ribeiro MT, Singh S, and Guestrin C. Why should I trust you?: Explaining the predictions of any classifier. In: Proc. \nACMSIGKDD Int. Conf. Knowl. Discov. Data Mining. 2016. p. 1135–44.\n\n 36. Luo X, et al. An effective scheme for QoS estimation via alternating direction method-based matrix factorization. \nIEEE Trans Serv Comput. 2019;12(4):503–18.\n\n 37. Liu CL, Hsaio WH, Lee CH, Lu GC and Jou E. Movie rating and review summarization in mobile environment. In: IEEE \ntrans. systems, man and cybernetics, Part C: applications and reviews. 2012. p. 397–407.\n\n 38. Vapnik, VN. The nature of statistical learning theory, Springer, 2nd ed, 1999. Translated by Xu Jianghua, Zhang Xue-\ngong. Beijing: China Machine Press; 2000.\n\n 39. [Dataset] Flipkart-products. http://www.kaggl e.com/Promp tClou dHQ/flipk art-produ cts.\n 40. [Dataset] https ://snap.stanf ord.edu/data/web-Amazo n.html.\n 41. [Dataset] He R, McAuley J. Ups and downs: modeling the visual evolution of fashion trends with one-class collabora-\n\ntive filtering. WWW; 2016.\n 42. Popescu AM, Etzioni O. Extracting product features and opinions from reviews. 2005; EMNLP.\n 43. Zaharia M, Chowdhury M, Das T, Dave A, Ma J, McCauley M, Franklin M, Shenker S, Stoica I. Resilient distributed \n\ndatasets: A fault-tolerant abstraction for in-memory cluster computing Technical Report UCB/EECS-2011-82. UC \nBerkeley: EECS Department; 2011.\n\n 44. Davis J, Goadrich M. The relationship between precision-recall and ROC curves, In ICML. 2006. p. 233–40.\n 45. Lee JS, Lee ES. Exploring the usefulness of predicting people’s locations. Procedia Soc Beh Sci. 2014. https ://doi.\n\norg/10.1016/j.sbspr o.2014.04.451.\n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nhttp://www.kaggle.com/PromptCloudHQ/flipkart-products\nhttps://snap.stanford.edu/data/web-Amazon.html\nhttps://doi.org/10.1016/j.sbspro.2014.04.451\nhttps://doi.org/10.1016/j.sbspro.2014.04.451\n\n\tImproving prediction with enhanced Distributed Memory-based Resilient Dataset Filter\n\tAbstract \n\tIntroduction\n\tRelated work\n\tMethodology\n\tData collection phase\n\tDataset pre-processing\n\tResilient Distributed Dataset\n\n\tPrediction classifiers\n\tLogistic regression (LR)\n\tSupport Vector Machine (SVM)\n\n\tExperimental setup\n\n\tResults and discussions\n\tConclusion and future work\n\tAcknowledgements\n\tReferences\n\n\n\n\n",
      "metadata_storage_path": "aHR0cHM6Ly90cmFpbmluZ2NhdGFsb2dzdG9yYWdlLmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlci9zNDA1MzctMDIwLTAwMjkyLXkucGRm0",
      "metadata_content_type": "application/pdf",
      "metadata_author": "Sandhya Narayanan ",
      "metadata_title": "Improving prediction with enhanced Distributed Memory-based Resilient Dataset Filter",
      "people": [
        "Sandhya Narayanan1",
        "Philip Samuel2",
        "Mariamma Chacko3",
        "ket",
        "dancy",
        "Narayanan",
        "Makridakis",
        "Hao",
        "Salakhutdinov",
        "Wietsma",
        "Jianguo Chen",
        "Asha",
        "Gini",
        "Luo",
        "Liu",
        "log2Ef",
        "�G",
        "Pe",
        "MC",
        "Lau RY",
        "Liao SY",
        "Kwok RC",
        "Xu K",
        "Xia Y",
        "Li Y.",
        "Lin X",
        "Li Y",
        "Wang X",
        "Matos",
        "Rossi",
        "Jeon S",
        "Dave K",
        "Lawrence S",
        "Pennock D.",
        "Zhou Y",
        "Wilkinson D",
        "Schreiber R",
        "Pan R",
        "Zhang KZK",
        "Benyoucef M.",
        "Cui Geng",
        "Lui Hon-Kwong",
        "Guo Xiaoning",
        "Manek AS",
        "Shenoy PD",
        "Mohan MC",
        "Int J Knowl",
        "Singh S",
        "Singh N.",
        "Demchenko Yuri",
        "Sihong Xie",
        "Guan Wang",
        "Shuyang Lin",
        "Yu Philip S.",
        "Koren Y",
        "Bell R",
        "Volinsky C",
        "Salakhutdinov R",
        "Mnih A",
        "Hinton G.",
        "King I",
        "Lyu",
        "Bandakkanavar V",
        "Ramesh M",
        "Geeta V",
        "Gu V",
        "Li H.",
        "Zhang Hanpeng",
        "Wang Zhaohua",
        "Chen Shengjun",
        "Guo Chengqi",
        "Ghose A",
        "Ipeirotis PG",
        "Chong AY",
        "Ch’ng E",
        "Liu MJ",
        "Li B",
        "Yang H",
        "Fujimaki R",
        "Kusumura Y",
        "Liu J.",
        "Breese JS",
        "Heckerman D",
        "Kadie C.",
        "Mukherjee A",
        "Kumar A",
        "Liu B",
        "Wang J",
        "Hsu M",
        "Castellanos M",
        "Ghosh R.",
        "Makridakis S",
        "Spiliotis E",
        "Assimakopoulos V",
        "Imon A",
        "Roy C",
        "Manos C",
        "Bhattacharjee S.",
        "Chen T",
        "Zhang W",
        "Lu Q",
        "Chen K",
        "Zheng Z",
        "Yu Y. SVD",
        "J Mach",
        "Shi Y",
        "Larson M",
        "Hanjalic A",
        "Shan H",
        "Banerjee A",
        "Mnih A.",
        "Crawford M",
        "Khoshgoftaar TM",
        "Prusa JD",
        "Richter AN",
        "Al Najada H.",
        "Ricci F",
        "Francesco",
        "Jianguo C",
        "Venugopal KR",
        "Fan RE",
        "Chang K-W",
        "Hsieh C-J",
        "Wang X-R",
        "Lin C-J",
        "Ribeiro MT",
        "Guestrin C",
        "Luo X",
        "Liu CL",
        "Hsaio WH",
        "Lee CH",
        "Lu GC",
        "Jou E.",
        "Vapnik",
        "Xu Jianghua",
        "Zhang Xue",
        "He R",
        "McAuley J.",
        "Popescu AM",
        "Etzioni O",
        "Zaharia M",
        "Chowdhury M",
        "Das T",
        "Dave A",
        "Ma J",
        "McCauley M",
        "Franklin M",
        "Shenker S",
        "Stoica I.",
        "Davis J",
        "Goadrich M",
        "Lee JS",
        "Lee ES"
      ],
      "organizations": [
        "Creative Commons",
        "iveco",
        "School of Engineering",
        "Cochin University of Science \n& Technology",
        "zon",
        "flip cart",
        "Real",
        "fk",
        "RDD",
        "DMRDF",
        "KRi",
        "KCm",
        "Amazon",
        "Intel",
        "rc",
        "PA",
        "Gini",
        "TP",
        "SVM",
        "Feature",
        "SN",
        "PS",
        "Kaggle",
        "Cochin University of Science & Technology",
        "Department of",
        "of Ship Technology",
        "Manag",
        "Int J Electron",
        "information & computing technology",
        "ICCICT",
        "institute of electrical and electronics engineers",
        "IEEE",
        "CloudCom",
        "ACM",
        "IJRITCC",
        "Int Conference Electron Comm ACM",
        "PLoS ONE",
        "Springer",
        "China Machine Press",
        "Flipkart",
        "WWW",
        "EMNLP",
        "EECS",
        "UC",
        "EECS Department",
        "Springer Nature"
      ],
      "locations": [
        "shop",
        "Kochi",
        "India",
        "stanford",
        "Chicago",
        "Monte Carlo",
        "library",
        "Beijing",
        "Berkeley"
      ],
      "keyphrases": [
        "Distributed Memory‑based Resilient Dataset Filter",
        "Creative Commons Attribution 4.0 International License",
        "Distributed Memory-based Resilient Dataset Filter",
        "other third party material",
        "big data processing technologies",
        "A Feature Information Gain",
        "other online shopping sites",
        "Resilient Distribution Dataset",
        "social networking sites",
        "Creative Commons licence",
        "sophisticated pre-processing techniques",
        "significant feature identification",
        "Support vector machine",
        "Redundancy Open Access",
        "J Big Data",
        "successful product launch",
        "online product recommendations",
        "consumer electronics market",
        "online product reviews",
        "distributed environment",
        "Online reviews",
        "pre-processed dataset",
        "feature modelling",
        "Online feedback",
        "retail shopping",
        "useful information",
        "price information",
        "author information",
        "Customer reviews",
        "duplicate reviews",
        "product sale",
        "product life",
        "product quality",
        "less product",
        "product pre-launch",
        "Sandhya Narayanan1",
        "Philip Samuel2",
        "Mariamma Chacko3",
        "massive volumes",
        "different applications",
        "sensor data",
        "health care",
        "enormous size",
        "unstructured data",
        "crucial thing",
        "large size",
        "communication methods",
        "direct suggestions",
        "several advantages",
        "limited time",
        "research work",
        "FIG) measure",
        "Logistic regression",
        "resilience property",
        "appropriate credit",
        "original author",
        "credit line",
        "statutory regulation",
        "copyright holder",
        "RESEARCH Narayanan",
        "Cochin University",
        "Full list",
        "new product",
        "1 Information Technology",
        "intended use",
        "permitted use",
        "mation source",
        "DMRDF method",
        "The Author",
        "prediction accuracy",
        "Introduction",
        "amount",
        "extensive",
        "customers",
        "impact",
        "extent",
        "ratings",
        "Abstract",
        "sustainability",
        "companies",
        "turn",
        "reliability",
        "classifiers",
        "output",
        "manufacturer",
        "design",
        "Keywords",
        "article",
        "sharing",
        "adaptation",
        "reproduction",
        "medium",
        "link",
        "changes",
        "images",
        "permission",
        "iveco",
        "Correspondence",
        "nairsands",
        "School",
        "Engineering",
        "Science",
        "Kochi",
        "India",
        "org",
        "different natural language processing techniques",
        "significant data processing methods",
        "big data processing model",
        "Different feature selection methods",
        "wrapper feature selection method",
        "multiple forecasting field",
        "previous customer feedbacks",
        "online shopping sites",
        "structured massive volume",
        "customer review analysis",
        "spam reviews recognition",
        "many redundant reviews",
        "machine learning methods",
        "product pre-launch prediction",
        "Consumer product success",
        "user rating matrix",
        "future work” section",
        "poor quality products",
        "different criteria",
        "Different works",
        "wrapper methods",
        "art methods",
        "alternative methods",
        "statistical methods",
        "model complexity",
        "statistical analysis",
        "filter method",
        "customer reviews",
        "enhanced method",
        "unreliable data",
        "model performance",
        "shop owners",
        "other hand",
        "extra cost",
        "brand loyalty",
        "eCommerce firms",
        "other retailers",
        "large volume",
        "crucial phase",
        "univariate manner",
        "System design",
        "less accuracy",
        "unknown values",
        "improper knowledge",
        "Matrix factorization",
        "collaborative filtering",
        "two vectors",
        "accurate reviews",
        "duplicated reviews",
        "negative reviews",
        "product reviews",
        "odology” section",
        "data pre-processing",
        "prediction classifiers",
        "Related work",
        "marketing strategies",
        "embedded process",
        "Page",
        "information",
        "effort",
        "industry",
        "strategy",
        "users",
        "valuable",
        "number",
        "blogs",
        "forums",
        "awareness",
        "need",
        "ratio",
        "positive",
        "features",
        "usefulness",
        "relevance",
        "state",
        "ally",
        "combination",
        "distributive",
        "web",
        "order",
        "scalable",
        "failure",
        "realization",
        "paper",
        "methodology",
        "Results",
        "discussions",
        "conclusion",
        "Makridakis",
        "Author",
        "reason",
        "15Narayanan",
        "MF",
        "Hao",
        "item",
        "low",
        "user item matrix factorization technique",
        "standard probability-based matrix factorization methods",
        "user item matrix factorization method",
        "Gini- index impurity measure",
        "relational database management systems",
        "single value decomposition method",
        "high term frequency words",
        "related matrix factorization",
        "probability factorization methods",
        "student user behavior",
        "Stochastic Gradient Decent",
        "mobile decision aid",
        "rule-based apriori algorithm",
        "mobile envi- ronment",
        "appropriate computing models",
        "traditional collaborative Filtering",
        "product feature identification",
        "Bayesian-based probabilistic analysis",
        "cold start problem",
        "automatic service selection",
        "Latent Semantic Analysis",
        "Gini-index feature method",
        "movie review dataset",
        "customer review datasets",
        "pre-launch product prediction",
        "Statistical methods",
        "user reviews",
        "opinion words",
        "Gini-index method",
        "filtering function",
        "movie rating",
        "review summarization",
        "customer feedback",
        "density-peaked method",
        "LSA-based) method",
        "LSA-based method",
        "squared distance",
        "big volume",
        "conventional approach",
        "implementation purpose",
        "sparsity problem",
        "various analyses",
        "recommendation issues",
        "different websites",
        "Jianguo Chen",
        "opinion extraction",
        "individual dimensions",
        "large datasets",
        "major challenge",
        "existing products",
        "different classifiers",
        "rating dataset",
        "polarity prediction",
        "product features",
        "recommender system",
        "recommendation system",
        "huge volume",
        "historical data",
        "big data",
        "disease symptoms",
        "sentimental analysis",
        "diction accuracy",
        "29 features",
        "solution",
        "squares",
        "Salakhutdinov",
        "other",
        "items",
        "limitation",
        "addition",
        "approaches",
        "Wietsma",
        "study",
        "result",
        "correlation",
        "treatment",
        "diagnosis",
        "diseases",
        "cluster",
        "Asha",
        "sentences",
        "rence",
        "document",
        "precision",
        "disadvantage",
        "Luo",
        "quality",
        "Liu",
        "authors",
        "Lack",
        "redundancy",
        "work",
        "existence",
        "Methodology",
        "phases",
        "Distributed Memory-based Resilient Dataset Filter approach",
        "Identification Redundancy Removal Data Integration Training",
        "classification algorithms Support Vector Logistic",
        "Product prelaunch prediction System Design",
        "Data collection Categorical Text Real",
        "Enhanced Feature Information Gain measure",
        "flip cart customer reviews",
        "Regression Support Vector",
        "duplicate data removal",
        "classification algorithms Logistic",
        "mobile phones product reviews",
        "port Vector Machine",
        "data collection phase",
        "JSON file format",
        "New mobile phones",
        "customer review Ri",
        "product rating scale",
        "Regression Testing Dataset",
        "categorical, real",
        "product review dataset",
        "text data",
        "Logistic Regression",
        "opinion identification",
        "best information",
        "prediction classifier",
        "multivariate data",
        "data pre",
        "pre-launch prediction",
        "input dataset",
        "Product feature",
        "Product categories",
        "feature selection",
        "nificant feature",
        "feature instance",
        "k feature",
        "various stages",
        "dancy elimination",
        "different products",
        "Several datasets",
        "public datasets",
        "data- set",
        "seven brands",
        "two reasons",
        "unavoidable items",
        "sample set",
        "catego- rization",
        "priority weightage",
        "major role",
        "large number",
        "fea- tures",
        "total number",
        "processing Feature",
        "user requirements",
        "particular review",
        "pre‑processing",
        "feature impurity",
        "market industry",
        "Battery life",
        "user features",
        "Review Content",
        "Figure",
        "model",
        "SVM",
        "LR",
        "zon",
        "period",
        "24 months",
        "day",
        "everyone",
        "Table",
        "ReviewID",
        "Title",
        "price",
        "camera",
        "RAM",
        "Fig.",
        "processor",
        "Average",
        "polarity",
        "opinions",
        "Senti-WordNet",
        "probability",
        "SR",
        "Distributed Memory-based Resilience Dataset Filter",
        "15 Rear camera 31 Finger sensor",
        "big data processing approach",
        "Resilient Distributed Dataset",
        "local file system",
        "3 ReviewID 19 Product category",
        "Impurity R(I",
        "value) pair dataset",
        "memory data caching",
        "information Gain value",
        "Feature Information Gain",
        "customer review dataset",
        "Table 2 Significant Features",
        "Ef Review Feature",
        "13 Operating system",
        "input file",
        "P(R",
        "25 Front camera",
        "new dataset",
        "prior information",
        "9 Feature information",
        "Next step",
        "OR N",
        "Sim type",
        "4 Content 20 Thickness",
        "mobile phone",
        "7 Battery life",
        "10 Review type",
        "29 Network support",
        "14 Water proof",
        "Quick charging",
        "32 Internal storage",
        "machine learning",
        "pervasive requirement",
        "main actions",
        "first element",
        "main Transformations",
        "long Lineage",
        "n customers",
        "feature set",
        "active customer",
        "5 Product brand",
        "Product type",
        "11 Product display",
        "redundant reviews",
        "N reviews",
        "value) pairs",
        "�G value",
        "null values",
        "Pi.log2Ef",
        "SR N",
        "cache chunks",
        "24 Product rating",
        "reduce function",
        "SR.",
        "respect",
        "opinion",
        "No",
        "1 Author",
        "17 RAM",
        "2 Title",
        "Weight",
        "8 Price",
        "12 Processor",
        "Multi-band",
        "16 Applications",
        "RDD",
        "requirements",
        "Variety",
        "jobs",
        "point",
        "time",
        "challenge",
        "analysis",
        "systems",
        "elements",
        "path",
        "map",
        "groupBykey",
        "fault-tolerance",
        "list",
        "Fx",
        "∑",
        "β",
        "Distributed Memory-based resilient filter score",
        "hyper plane normal vector element",
        "memory-based Resilient Dataset Filter score",
        "D dimensional input space",
        "resilient filter score value",
        "t training feature vectors",
        "Support Vector Machine classifiers",
        "one Distributed Memory-based",
        "decision hyper plane",
        "positive one class",
        "logarithmic base value",
        "logistic regression analysis",
        "machine learning method",
        "prediction variable value",
        "Logistic regression value",
        "data features relationships",
        "product failure class",
        "product success class",
        "δ score value",
        "row vector",
        "column vector",
        "significant feature",
        "corresponding vectors",
        "nearest vectors",
        "learning approaches",
        "classification method",
        "training dataset",
        "constant value",
        "probability value",
        "mth customer",
        "L1 norm",
        "second occurrence",
        "case study",
        "mobile phones",
        "logit function",
        "+ b",
        "new skills",
        "data separation",
        "real numbers",
        "two classes",
        "mth review",
        "ith review",
        "customer review",
        "similar reviews",
        "successful products",
        "N’ number",
        "KC",
        "KR",
        "KFx",
        "KFj",
        "entry",
        "similarities",
        "The",
        "Eq.",
        "processing",
        "More",
        "market",
        "p0",
        "L0",
        "values",
        "knowledge",
        "RD",
        "XD",
        "way",
        "distance",
        "hyperplane",
        "conditions",
        "margin",
        "γ",
        "different Spark cluster configura- tions",
        "Most significant customer review features",
        "two Intel Xeon E",
        "2699V4 2.2 G Hz processors",
        "Web Server Gateway Interface",
        "customer review feature identification",
        "big data processing system",
        "software system large servers",
        "LSA-based methods processing time",
        "big data analytics",
        "Apache web server",
        "Amazon Web Services",
        "Apache Spark 2.2.1 framework",
        "Logistic regression classifiers",
        "different dataset size",
        "feature information gain",
        "negative one class",
        "semantic analysis methods",
        "high-speed processing performance",
        "Spark python API",
        "several case studies",
        "mobile phone sustainability",
        "prediction accuracy measurement",
        "system response time",
        "redundant customer reviews",
        "system design factors",
        "prediction accuracy evaluation",
        "DMRDF model time",
        "separate servers",
        "proposed system",
        "prediction system",
        "failure class",
        "software components",
        "LSA-based model",
        "less time",
        "product sustainability",
        "Experimental setup",
        "PySpark version",
        "Vector Machine",
        "major concern",
        "internal storage",
        "art techniques",
        "DMRDF approach",
        "9 GB dataset",
        "18 GB dataset",
        "other gini-index",
        "Gini-index model",
        "Product price",
        "scalability requirements",
        "other state",
        "application development",
        "16 GB",
        "Ubuntu",
        "nodes",
        "VCPUs",
        "4 cores",
        "wtzi",
        "Support",
        "7 brands",
        "LR.",
        "graph",
        "percentage",
        "manner",
        "comparison",
        "completion",
        "latent",
        "342 s",
        "495 s",
        "156 s",
        "910 s",
        "advantage",
        "execution",
        "recall",
        "Distributed Memory-based Resilient Dataset Filter method",
        "reliable big data processing model",
        "Support Vector Machine prediction classifiers",
        "Support Vector Machine classification",
        "Classifier Support vector machine",
        "different customer review aspects",
        "customer review feature prediction",
        "Processing Time Graph",
        "big data analysis",
        "Logistic Regression classifiers",
        "other two methods",
        "duplicate customer reviews",
        "LSA-based meth- ods",
        "Classifier Logistic regression",
        "LSA-based DMRDF Gini-index",
        "LR classifiers",
        "redundant data",
        "feature dimensionality",
        "other methods",
        "Gini-index methods",
        "significant features",
        "accuracy measures",
        "P@R",
        "R@R",
        "false negative",
        "SVM classifier",
        "ratings datasets",
        "performance evaluation",
        "Gini- index",
        "Technological development",
        "new challenges",
        "artificial intelligence",
        "next frontier",
        "mation Gain",
        "Gini-index approaches",
        "Dataset Size",
        "large dataset",
        "LSA-based approaches",
        "many features",
        "The DMRDF",
        "Performance comparison",
        "future work",
        "PA measures",
        "results",
        "TP",
        "FP",
        "TN",
        "FN",
        "Eqs",
        "tions",
        "Conclusion",
        "era",
        "innovation",
        "productivity",
        "implementation",
        "elimination",
        "12",
        "18",
        "7",
        "ACM Trans Manag Inform Syst",
        "Int J Softw Eng Appl",
        "efficient big data search processing",
        "Int J Inform Manag.",
        "J Acad Market Sci",
        "Large-scale parallel collaborative filtering",
        "different reliable online websites",
        "Redundant data removal technique",
        "other product feature identification",
        "online review spam detection",
        "Int J Electron",
        "data processing domains",
        "Dec Support Syst",
        "Feature information gain",
        "prediction model- ling",
        "statistical prop- erties",
        "Pre-launch product prediction",
        "new product sales",
        "information fusion approach",
        "memory computation method",
        "online consumer reviews",
        "time streaming predictions",
        "Latent semantic analysis",
        "Social commerce research",
        "dataset model performance",
        "meta-analytic review",
        "literature review",
        "research themes",
        "semantic classification",
        "Consumer behavior",
        "ratings dataset",
        "important role",
        "applica- tion",
        "Resilience property",
        "long lineage",
        "unified API",
        "customer comments",
        "learning algorithms",
        "matrix factorization",
        "major contributor",
        "current study",
        "Promp tClou",
        "Competing interests",
        "Author details",
        "Lau RY",
        "Liao SY",
        "Kwok RC",
        "Xu K",
        "Xia Y",
        "Li Y.",
        "Text mining",
        "probabilistic modeling",
        "Lin X",
        "Wang X",
        "Matos CAD",
        "Rossi CAV",
        "mouth communications",
        "Jeon S",
        "Dave K",
        "Lawrence S",
        "Pennock D.",
        "peanut gallery",
        "Zhou Y",
        "Wilkinson D",
        "Schreiber R",
        "Pan R.",
        "netflix prize",
        "Zhang KZK",
        "Benyoucef M.",
        "Cui Geng",
        "Lui Hon-Kwong",
        "Guo Xiaoning",
        "final manuscript",
        "Kaggle repository",
        "Ship Technology",
        "Authors’ contributions",
        "DMRDF model",
        "Computer Science",
        "real",
        "surveys",
        "thesis",
        "sentiments",
        "limitations",
        "Abbreviations",
        "FIG",
        "LSA",
        "accuracy",
        "Precision",
        "Recall",
        "Acknowledgements",
        "SN",
        "PS",
        "MC",
        "Funding",
        "Availability",
        "materials",
        "datasets",
        "stanford",
        "Amazon",
        "cts",
        "2 Department",
        "3 Department",
        "25 October",
        "PromptCloudHQ",
        "References",
        "definition",
        "trends",
        "Word",
        "marketing",
        "antecedents",
        "moderators",
        "WWW",
        "effect",
        "Comm.",
        "17",
        "28",
        "ACM Trans Intell Syst Technol",
        "Pak J Stat Oper Res",
        "Int J Knowl Web Intell",
        "22nd ACM SIGKDD Int. Conference",
        "19th ACM SIGKDD international conference",
        "Liu J. Online Feature Selection",
        "21st international conference companion",
        "2013 IEEE 10th Int. Con.",
        "novel review ranking systems",
        "Generalized probabilistic matrix factorizations",
        "Int J Prod Res",
        "IEEE 4th Int. conference",
        "IEEE 10th international conference",
        "Yu Y. SVD Feature",
        "Machine Learning forecasting methods",
        "Int Conference Electron",
        "World Wide Web",
        "J Inform Manag.",
        "ACM Comput Surv.",
        "matrix factorization technique",
        "implicit social relations",
        "Yu Philip S.",
        "online shopping websites",
        "scientific data infrastructure",
        "time-series pattern discovery",
        "Ch’ng E",
        "consumer product demands",
        "Big data analytics",
        "cloud computing technology",
        "Li H. Memory",
        "big data challenges",
        "feature-based collaborative filtering",
        "2012 international conference",
        "J Mach",
        "Wang J",
        "recommender systems",
        "Liu MJ",
        "online promotional",
        "Liu B",
        "user-item matrix",
        "malicious websites",
        "Koren Y",
        "Product recommendation",
        "Li B.",
        "online reviews",
        "Kusumura Y",
        "Knowledge discovery",
        "data mining",
        "Spiliotis E",
        "Shi Y",
        "future challenges",
        "ACM Proceedings",
        "high-performance computing",
        "Yang H",
        "Shan H",
        "Singh S",
        "Makridakis S",
        "Bhattacharjee S",
        "Manek AS",
        "Shenoy PD",
        "Mohan MC",
        "Singh N.",
        "electronics engineers",
        "Demchenko Yuri",
        "Sihong Xie",
        "Shuyang Lin",
        "Bell R",
        "Volinsky C.",
        "Salakhutdinov R",
        "Mnih A",
        "Hinton G",
        "boltzmann machines",
        "Hao MA",
        "King I",
        "Lyu MR.",
        "Bandakkanavar V",
        "Ramesh M",
        "Geeta V.",
        "sentiment classification",
        "Gu V",
        "iterative operation",
        "Zhang Hanpeng",
        "Chen Shengjun",
        "Guo Chengqi",
        "working communities",
        "empirical study",
        "Ghose A",
        "Ipeirotis PG",
        "Chong AY",
        "Fujimaki R",
        "Breese JS",
        "Heckerman D",
        "Kadie C.",
        "Empirical analysis",
        "predictive algorithms",
        "14th Conf.",
        "Artifical Intelligence",
        "Mukherjee A",
        "Kumar A",
        "Hsu M",
        "Castellanos M",
        "Ghosh R.",
        "opinion spammers",
        "behavioral footprints",
        "Assimakopoulos V",
        "PLoS ONE",
        "Imon A",
        "Roy C",
        "Manos C",
        "logistic regression",
        "Chen T",
        "Zhang W",
        "Lu Q",
        "Chen K",
        "Zheng Z",
        "Larson M",
        "Hanjalic A.",
        "Banerjee A.",
        "Guan Wang",
        "Res.",
        "spam detection",
        "fraudulent",
        "communication",
        "ICCICT",
        "institute",
        "electrical",
        "science",
        "CloudCom",
        "Computer",
        "Proc.",
        "24th",
        "explicit",
        "survey",
        "IJRITCC",
        "hadoop",
        "spark",
        "Zhaohua",
        "mediator",
        "roles",
        "Uncertainty",
        "Chicago",
        "Statistical",
        "concerns",
        "journ",
        "rainfall",
        "toolkit",
        "ICDM",
        "Mnih A. Bayesian probabilistic matrix factorization",
        "alternating direction method-based matrix factorization",
        "memory cluster computing Technical Report",
        "Markov chain Monte Carlo",
        "Al Najada H. Survey",
        "Gini-index feature selection method",
        "ACMSIGKDD Int. Conf. Knowl.",
        "Jou E. Movie rating",
        "Procedia Soc Beh Sci",
        "mobile decision aid systems",
        "IEEE Trans Serv Comput",
        "Ricci F. Product reviews",
        "A disease diagnosis",
        "Stoica I. Resilient",
        "25th int. conference",
        "treatment recommendation system",
        "Aspect term extraction",
        "large linear classification",
        "large movie reviews",
        "China Machine Press",
        "Data collection phase",
        "Support Vector Machine",
        "statistical learning theory",
        "machine learning techniques",
        "McAuley J. Ups",
        "big data mining",
        "trans. systems",
        "Dave A",
        "cloud computing",
        "Inform Sci",
        "mobile environment",
        "Ma J",
        "Davis J",
        "Crawford M",
        "Khoshgoftaar TM",
        "Prusa JD",
        "Richter AN",
        "Wietsma TA",
        "Jianguo C",
        "Venugopal KR",
        "sentiment analysis",
        "Fan RE",
        "Chang K-W",
        "Hsieh C-J",
        "Wang X-R",
        "Lin C-J.",
        "Ribeiro MT",
        "Guestrin C.",
        "Luo X",
        "effective scheme",
        "QoS estimation",
        "Liu CL",
        "Hsaio WH",
        "Lee CH",
        "Lu GC",
        "Part C",
        "2nd ed",
        "Xu Jianghua",
        "flipk art-produ",
        "He R",
        "visual evolution",
        "fashion trends",
        "Popescu AM",
        "Etzioni O.",
        "Zaharia M",
        "Chowdhury M",
        "Das T",
        "McCauley M",
        "Franklin M",
        "Shenker S",
        "fault-tolerant abstraction",
        "EECS Department",
        "Goadrich M.",
        "ROC curves",
        "Lee JS",
        "Lee ES",
        "jurisdictional claims",
        "institutional affiliations",
        "work Methodology",
        "Acknowledgements References",
        "Springer Nature",
        "Francesco",
        "PERMID",
        "LIBLINEAR",
        "library",
        "journal",
        "pone",
        "predictions",
        "Discov.",
        "summarization",
        "cybernetics",
        "applications",
        "Vapnik",
        "VN.",
        "gong",
        "Beijing",
        "Flipkart-products",
        "kaggl",
        "downs",
        "filtering",
        "EMNLP.",
        "Berkeley",
        "relationship",
        "precision-recall",
        "ICML",
        "people",
        "locations",
        "Publisher",
        "Note",
        "regard",
        "maps",
        "Improving"
      ],
      "masked_text": "\nImproving prediction with enhanced \nDistributed Memory‑based Resilient Dataset \nFilter\n*******************, ************** and ****************\n\nIntroduction\nAnalyzing and processing massive volumes of data in different applications like sensor \ndata, health care and e-Commerce require big data processing technologies. Extracting \nuseful information from the enormous size of unstructured data is a crucial thing. As the \namount of data becomes more extensive, sophisticated pre-processing techniques are \nrequired to analyze the data. In social networking sites and other online shopping sites, \na massive volume of online product reviews from a large size of customers are available \n[1]. The impact of online product reviews affects 90% of the current e-Commerce ***-\nket [2]. Customer reviews contribute the product sale to an extent and product life in the \nmarket depends on online product recommendations.\n\nOnline feedback is one of the communication methods which gives direct suggestions \nfrom the ********* [3, 4]. Online reviews and ratings from ********* are another infor-\nmation source about product quality [5, 6]. Customer reviews can help to decide on a new \nsuccessful product launch. Online shopping has several advantages over retail shopping. In \nretail shopping, the customers visit the shop and receive price information but less product \n\nAbstract \n\nLaunching new products in the consumer electronics market is challenging. Develop-\ning and marketing the same in limited time affect the sustainability of such companies. \nThis research work introduces a model that can predict the success of a product. A \nFeature Information Gain (FIG) measure is used for significant feature identification \nand Distributed Memory-based Resilient Dataset Filter (DMRDF) is used to eliminate \nduplicate reviews, which in turn improves the reliability of the product reviews. The \npre-processed dataset is used for prediction of product pre-launch in the market using \nclassifiers such as Logistic regression and Support vector machine. DMRDF method is \nfault-tolerant because of its resilience property and also reduces the dataset redun-\n*****; hence, it increases the prediction accuracy of the model. The proposed model \nworks in a distributed environment to handle a massive volume of the dataset and \ntherefore, it is scalable. The output of this feature modelling and prediction allows the \n************ to optimize the design of his new product.\n\nKeywords: Distributed Memory-based, Resilient Distribution Dataset, Redundancy\n\nOpen Access\n\n© **********(s) ****. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original ******(s) and \nthe source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material \nin this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material \nis not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the \npermitted use, you will need to obtain permission directly from the ****************. To view a copy of this licence, visit http://creat iveco \nmmons .org/licen ses/by/4.0/.\n\nRESEARCH\n\n********* et al. J Big Data            (****) ****  \n******************************‑020‑00292‑y\n\n*Correspondence:   \n******************* \n1 Information Technology, \n*********************, \n**************************** \n& Technology, Kochi 682022, \nIndia\nFull list of author information \nis available at the end of the \narticle\n\n*******************************************\n*******************************************\n*******************************************************************************\n\n\nPage 2 of *********** et al. J Big Data            (****) **** \n\ninformation from shop ******. On the other hand, online shopping sites give product \nreviews and previous customer feedbacks without extra cost and effort for the customers \n[7–10].\n\nInvesting in poor quality products potentially affects an industry’s brand loyalty and this \nstrategy should be changed by the eCommerce firms [5, 11]. Consumer product success \ndepends on different criteria, such as the quality of the product and marketing strategies. \nThe ***** should provide their valuable and accurate reviews about the products [12]. Cus-\ntomers bother to give reviews about products, whether they liked it or not. If the users \nprovide reviews, then other retailers can create some duplicated reviews [13, 14]. In online \nmarketing, the volume and value of product reviews are examined [15, 16]. The number \nof the product reviews on the shopping sites, blogs and forums has increased awareness \namong the *****. This large volume of the reviews leads to the need for significant data \nprocessing methods [17, 18]. The value is the rating on the products. The ratio of positive to \nnegative reviews about the product leads to the quality of the product [19, 20].\n\nFeature selection is a crucial phase in data pre-processing [21]. Selecting features from \nan un-structured massive volume of data reduce the model complexity and improves the \nprediction accuracy. Different feature selection methods existing are the filter, wrapper and \nembedded. The wrapper feature selection method evaluates the usefulness of the feature \nand it depends on the performance of the classifier [22]. The filter method calculates the \nrelevance of the features and analyzes data in a univariate manner. The embedded process \nis similar to the wrapper method. Embedded and wrapper methods are more expensive \ncompared to the filter method. The state-of-art methods in customer review analysis gener-\nally discuss on categorizing positive and negative reviews using different natural language \nprocessing techniques and spam reviews recognition [23]. Feature selection of customer \nreviews increases prediction accuracy, thereby improves the model performance.\n\nAn enhanced method, which is a combination of filter and wrapper method is proposed \nin this work, which focuses on product pre-launch prediction with enhanced distributive \nfeature selection method. Since many redundant reviews are available on the web in large \nvolumes, a big data processing model has been implemented to filter out duplicated and \nunreliable data from ******** reviews in-order to increase prediction accuracy. A scalable \nbig data processing model has been applied to predict the success or failure of a new prod-\nuct. The realization of the model has been done by Distributed Memory-based ********* \nDataset Filter with prediction classifiers.\n\nThis paper is organized as follows. “Related work” section discusses related work. “Meth-\nodology” section contains the proposed methodology with System design, Resilient Distrib-\nuted Dataset and Prediction using classifiers. “Results and discussions” section summarizes \nresults and discussion. The conclusion of the paper is shown in “Conclusion and future \nwork” section.\n\nRelated work\n********** et al. [24] illustrate that machine learning methods are alternative methods \nfor statistical analysis of multiple forecasting field. Author claims that statistical methods \nare more accurate than machine learning [25] methods. The reason for less accuracy is \nthe unknown values of data i.e., improper knowledge and pre-processing of data.\n\n\n\nPage 3 of *********** et al. J Big Data            (****) ****  \n\nDifferent works have been implemented using the Matrix factorization (MF) [14] \nmethod with collaborative filtering [26]. *** et al. [15] focused on a work based on the \nfactorization of the user rating matrix into two vectors, i.e., user latent and item latent \nwith low dimensionality. The sum of squared distance can be minimized by training a \nmodel that can find a solution using Stochastic Gradient Decent [27] or by least squares \n[28]. ************* et **. [29] proposed a method that can be scaled linearly by probabil-\nity related matrix factorization on a big volume of datasets and then comparing it with \nthe single value decomposition method. This matrix factorization outperforms other \nprobability factorization methods like Bayesian-based probabilistic analysis [29] and \nstandard probability-based matrix factorization methods. A conventional approach, like \ntraditional collaborative Filtering [13, 30] method depends on ********* and items. The \nuser item matrix factorization technique has been used for implementation purpose. \nIn the recommender system, there is a limitation in the sparsity problem and cold start \nproblem. In addition to the user item matrix factorization method, various analyses and \napproaches have been implemented to solve these recommendation issues.\n\n******* et al. [31] proposed a recommender system that gives information about the \nmobile decision aid and filtering function. This has been implemented with a study of \n29 features of ******* **** behavior. The result shows the correlation among the user \nreviews and product reviews from different websites. ************ et al. [32] proposed \na recommendation system for the treatment and diagnosis of the diseases. For cluster \nanalysis of disease symptoms, a density-peaked method is adopted. A rule-based apriori \nalgorithm is used for the diagnosis of disease and treatment. **** et al. [33] proposed \nthe Gini-index feature method using movie review dataset. The sentimental analysis \nof the reviews are performed and opinion extraction of the sentences are done. Gini-\nindex impurity measure improves the accuracy of the polarity prediction by sentimental \nanalysis using Support vector machine [34, 35]. Depending on the frequency of occur-\nrence of a word in the document, the term frequency is calculated and opinion words \nare extracted using the Gini-index method. In this method, high term frequency words \nare not included, as it decreases the precision. The disadvantage of this method is that \nfor the huge volume of data, the prediction accuracy decreases.\n\n*** et al. [36] proposed a method based on historical data to analyze the quality of \nservice for automatic service selection. *** et al. [37] proposed a system in a mobile envi-\nronment for movie rating and review summarization. The authors used Latent Semantic \nAnalysis (LSA-based) method for product feature identification and feature-based sum-\nmarization. Statistical methods [38] have been used for identifying opinion words. The \ndisadvantage of this method is that LSA-based method cannot be represented efficiently; \nhence, it is difficult to index based on individual dimensions. This reduces the prediction \naccuracy in large datasets.\n\nLack of appropriate computing models for handling huge volume and redundancy in \ncustomer review datasets is a major challenge. Another major challenge handled in the \nproposed work is the existence of a pre-launch product in the industry based on the \nproduct features, which can be predicted based on the customer feedback in the form \nof reviews and ratings of the existing products. This prediction helps to optimize the \ndesign of the product to improve its quality with the required product features. Many \nof the relational database management systems are handling structured data, which is \n\n\n\nPage 4 of *********** et al. J Big Data            (****) **** \n\nnot scalable for big data that handles a large volume of unstructured data. This proposed \nmodel solves the problem of redundancy in a huge volume of the dataset for better pre-\ndiction accuracy.\n\nMethodology\nA pre-launch product prediction using different classifiers has been analysed by huge \n******** review and rating dataset. The product prediction is done through the phases \nconsisting of data collection phase, feature selection and duplicate data removal, build-\ning prediction classifier, training as well as testing.\n\nFigure 1 describes the various stages in system design of the model. The input dataset \nconsists of multivariate data which includes categorical, real and text data. Input dataset \nis fed for data pre-processing. Data pre-processing consists of feature selection, redun-\ndancy elimination and data integration which is done using Feature Information Gain \nand Distributed Memory-based Resilient Dataset Filter approach. The cleaned dataset \nis trained using classification algorithms. The classifiers considered for training are Sup-\nport Vector Machine (SVM) and Logistic Regression (LR). Further the dataset is tested \nfor pre-launch prediction using LR and SVM.\n\nData collection phase\n\nThis methodology can be applied for different products. Several datasets like Ama-\nzon and flip cart customer reviews are available as public datasets [39–41]. The data-\nset of customer reviews and ratings of seven brands of mobile phones for a period of \n********* are considered in this work. The mobile phones product reviews are chosen \nbecause of two reasons. New mobile phones are launched into the market industry day \nby day which is one of the unavoidable items in everyone’s life. Market sustainability for \nthe mobile phones is very low.\n\nTable  1 shows a sample set of product reviews in which input dataset consists of \nuser features and product features. User features consists of Author, ReviewID and \nTitle depending on the ****. Product feature consists of Product categories, Overall \nratings and Review Content. Since mobile phone is taken as the product, the catego-\nrization is done according to the features such as Battery life, price, camera, RAM, \n\nData collection \n\nCategorical\n\nText\n\nReal\n\nData Pre-\nprocessing\n\nFeature \nIdentification\n\nRedundancy\nRemoval\n\nData \nIntegration\n\nTraining \nDataset Using \nclassification \nalgorithms\n\nSupport \nVector \n\nLogistic \nRegression\n\nTesting Dataset \nUsing \n\nclassification \nalgorithms\n\nLogistic \nRegression\n\nSupport \nVector \n\nFig. 1 Product prelaunch prediction System Design\n\n  \n\n\n\nPage 5 of *********** et al. J Big Data            (****) ****  \n\nprocessor, weight etc. Some features are given a priority weightage depending on the \nproduct and **** requirements. Input dataset with JSON file format is taken.\n\nDataset pre‑processing\n\nIn data pre-processing, feature selection plays a major role. In the product review \ndataset of a mobile phone, a large number of features exist. Identifying a feature from \ncustomer reviews is important for this model to improve the prediction accuracy. \nEnhanced Feature Information Gain measure has been implemented to identify sig-\nnificant feature.\n\nFeatures are identified based on the content of the product reviews, ratings of the \nproduct reviews and opinion identification of the reviews. Ratings of the product \nreviews can be further categorized based on a rating scale of 5 (1—Bad, 2—Average, \n3—Good, 4—very good, 5—Excellent). For opinion identification of the product, the \npolarity of extracted opinions for each review is classified using Senti-WordNet [42].\n\nFeature Information Gain measures the amount of information of a feature \nretrieved from a particular review. Impurity which is the measure of reliability of fea-\ntures in the input dataset should be reduced to get significant features. To measure \nfeature impurity, the best information of a feature obtained from each review is calcu-\nlated as follows\n\n• Let Pi be the probability of any feature instance \n(\n\nf\n)\n\n of k feature set F =\n{\n\nf1, f2, . . . fk\n}\n\n \nbelonging to  ith customer review Ri , where i varies from 1 to N.\n\n• Let N denotes the total number of ******** reviews.\n• Let OR denotes the polarity of extracted opinions of the Review.\n• Let SR denotes product rating scale of review (R).\n\nTable 1 Sample set of Product Reviews\n\n\n\nPage 6 of *********** et al. J Big Data            (****) **** \n\nThe information of a feature with respect to review rating and opinion is denoted by \nIf\n\nExpected information gain of the feature denoted as Ef\n\nReview Feature Impurity R(I) is calculated as\n\nThen Feature Information Gain (�G) to find out significant features are calculated \nas\n\nFeatures are selected based on the �G value and those with an Information gain \ngreater than 0.5 is selected as a significant feature. Table 2 shows the significant fea-\nture from customer reviews and ratings.\n\nNext step is to eliminate the redundant reviews and to replace null values of an \nactive customer from the customer review dataset using an enhanced big data pro-\ncessing approach. Reviews with significant features obtained from feature identifica-\ntion are considered for further processing.\n\n(1)If = log2\n\n(\n\n1\n\nP(R = F)\n\n)\n\n∗ OR ∗ SR.\n\n(2)Ef =\n\nN\n∑\n\ni=1\n\n−Pi(R = F).\n∥\n\n∥If\n∥\n\n∥\n\n1\n.\n\n(3)R(I) = −\n\nN\n∑\n\ni=1\n\nPi.log2Ef .\n\n(4)�G = R(I)−\n\nN\n∑\n\ni=1\n\n[(\n\nOR\n\nN\n∗ Ef\n\n)\n\n−\n\n(\n\nSR\n\nN\n∗ Ef\n\n)]\n\n.\n\nTable 2 Significant Features from Customer Reviews and Ratings\n\nNo Customer reviewed features No Customer reviewed features\n\n1 Author 17 RAM\n\n2 Title 18 Sim type\n\n3 ReviewID 19 Product category\n\n4 Content 20 Thickness\n\n5 Product brand 21 Weight of mobile phone\n\n6 Ratings 22 Height\n\n7 Battery life 23 Product type\n\n8 Price 24 Product rating\n\n9 Feature information gain 25 Front camera\n\n10 Review type 26 Back camera\n\n11 Product display 27 Opinion of review\n\n12 Processor 28 Multi-band\n\n13 Operating system 29 Network support\n\n14 Water proof 30 Quick charging\n\n15 Rear camera 31 Finger sensor\n\n16 Applications inbuilt 32 Internal storage\n\n\n\nPage 7 of *********** et al. J Big Data            (****) ****  \n\nResilient Distributed Dataset\n\nResilient Distributed Dataset (RDD) [43] is a big data processing approach, which allows \nto store cache chunks of data on memory and persevere it as per the requirements. \nThe in-memory data caching is supported by RDD. Variety of jobs at a point of time \nis another challenge which is handled by ***. This method deals with chunks of data \nduring processing and analysis. RDD can also be used for machine learning supported \nsystems as well as in big data processing and analysis, which happens to be an almost \npervasive requirement in the industry.\n\nIn the proposed method the main actions of RDD are:\n\n• Reduce (β): Combine all the elements of the dataset using the function β.\n• First (): This function will return the first element\n• takeOrdered(n): RDD is returned with first ‘n’ elements.\n• saveAsSequenceFile(path): the elements in the dataset to be written to the local file \n\nsystem with given path.\n\nThe main Transformations of RDD are:\n\n• map(β): Elements from the input file is mapped and new dataset is returned through \nfunction β.\n\n• filter(β): New dataset is returned if the function β returns true.\n• groupBykey(): When called a dataset of (key, value) pairs, this function returns a \n\ndataset of (key, value) pairs.\n• ReduceBykey(β): A (key, value) pair dataset is returned, where the values of each key \n\nare combined using the given reduce function β.\n\nIn the proposed work an enhanced Distributed Memory-based Resilience Dataset \nFilter (DMRDF) is applied. DMRDF method have long Lineage and it is recomputed \nthemselves using prior information, thus it achieves fault-tolerance. ***** has been \nimplemented to remove the redundancy in the dataset for product pre-launch predic-\ntion. This enhanced method is simple and fast.\n\n• Let the list of n customers represented as C = {c1, c2, c3 . . . , cn}\n\n• Let the list of N reviews be represented as R = {r1, r2, r3 . . . , rN }\n\n• Let x significant features are identified from feature set (F  ) represented as Fx ⊂ F\n\n• An active customer consists of significant feature having information Gain value \ndenoted by �G\n\nIn the DMRDF method, a product is chosen and its ******** reviews are found out. \nEliminate customers with similar reviews on the selected product and also reviews \nwith insignificant features. Calculate the memory-based Resilient Dataset Filter score \nbetween each of the customer reviews with significant features.\n\nLet us consider a set C of ‘n’ number of customers, the set R of ‘N’ number of reviews and \na set of significant features ′F ′\n\nx are considered. The corresponding vectors are represented \nas KC , KR and KFx . Then KRi is represented using a row vector and KFj is represented using \nthe column vector. Each entry KCm denote the number of times the  mth review arrives in \n\n\n\nPage 8 of *********** et al. J Big Data            (****) **** \n\n*********. The similarities between ith review of mth customer is found out using  L1 norm \nof KRi and KCm . The Distributed Memory-based resilient filter score δ is calculated using the \nEq. (5).\n\nThe δ score is calculated for each ******** review whereas the score lies between [0,1]. \nThe significant features are found out using Eq. 4. For customer reviews without significant \nfeatures, �G value will be zero. The reviews with δ score value 0 are found to be insignificant \nwithout any significant feature or opinion and hence those reviews are eliminated and not \nconsidered for further processing in the work. More than one Distributed Memory-based \nresilient filter score value is identified then the second occurrence of the review is consid-\nered as duplicate.\n\nPrediction classifiers\n\nLogistic regression and Support Vector Machine classifiers are the supervised machine \nlearning approaches used in the proposed work for product pre-launch prediction.\n\nLogistic regression (LR)\n\nWe have implemented proposed model using logistic regression analysis for prediction. \nThis model predicts the failure or success of a new product in the market by analysing \nselected product features from ******** reviews. A case study has been conducted using \nthe dataset of ******** reviews of mobile phones. Success or failure is the predictor vari-\nable used for training and testing the dataset. For training the model 75% of the dataset is \nused and for testing the model, remaining 25% is used.\n\n• Let p be the prediction variable value, assigning 0 for failure and 1 for success.\n• p0 is the constant value.\n• b is the logarithmic base value.\n\nThen the logit function is,\n\nThen the Logistic regression value γ is shown in Eq. (7),\n\n(5)δ =\n\nN\nn\n�\n\ni = 1\n\nm = 1\n\n\n\n\n\n�\n\nKRi ∗\n\n�\n\n�x\nj=1 KFj\n\n��\n\n∗ KCm\n\n*** · KCm\n\n\n\n ∗ |�G|\n\n(6)\nL0 = b\n\np0+p\nx\n∑\n\ni=1\n\nfi\n\n(7.1)γ =\nL0\n\n(\n\nbp0+p\n∑x\n\ni=1 fi\n)\n\n+ 1\n\n(7.2)=\n1\n\n1+ b\n−\n\n(\n\nb\np0+p\n\n∑x\ni=1\n\nfi\n)\n\n\n\nPage 9 of *********** et al. J Big Data            (****) ****  \n\nThe probability value of γ lies between [0,1]. In this work, if this value is greater than 0.5 \nthe pre-launch prediction of the product is considered as success and for values less than \n0.5, it is considered as failure.\n\nSupport Vector Machine (***)\n\nSVM is the supervised machine learning method, used to learn from set of data to get new \nskills and knowledge. This classification method can learn from data features relationships \n( zi ) and its class \n\n(\n\nyi\n)\n\n that can be applied to predict the success or failure class the product \nbelongs to.\n\n• For a set T  of t training feature vectors, zi ∈ RD, where i = 1 to t.\n• Let yi ∈ {+1,−1} , where +1 belongs to product success class and -1 belongs to product \n\nfailure class.\n• The data separation occurs in the real numbers denoted as X in the D dimensional \n\ninput space.\n• Let w be the hyper plane normal vector element, where w ∈ XD.\n\nThe hyper plane is placed in such a way that distance between the nearest vectors of the \ntwo classes to the hyperplane should be maximum. Thus, the decision hyper plane is calcu-\nlated as,\n\nThe conditions for training dataset d ∈ X , is calculated as\n\nTo maximize the margin the value of w should be minimized.\nThe products in the positive one class (+1) are considered as successful products, [from \n\nEq. (9)] and those in the negative one class (−1) [from Eq. (10)] are in failure class.\n\nExperimental setup\n\nThe proposed system was implemented using Apache Spark 2.2.1 framework. Spark pro-\ngramming for python using PySpark version 2.1.2, which is the Spark python API has been \nused for the application development. An Ubuntu running Apache web server using Web \nServer Gateway Interface is used. Amazon Web Services is used to run some components \nof the software system large servers (nodes), having two Intel Xeon E5-2699V4 2.2 G Hz \nprocessors (VCPUs) with 4 cores and 16 GB of RAM on different Spark cluster configura-\ntions. According to the scalability requirements the software components can be config-\nured and can run on separate servers.\n\n(8)α(w) =\n2\n\n�w�\n\n(9)wtzi + d ≥ 1, where yi = +1.\n\n(10)wtzi + d ≤ −1, whereyi = yi − 1.\n\n\n\nPage 10 of *********** et al. J Big Data            (****) **** \n\nResults and discussions\nTo evaluate our prediction system several case studies have been conducted. Support \nVector Machine and Logistic regression classifiers are employed to perform the predic-\ntion. Most significant customer review features are used to analyse the system perfor-\nmance. The prediction accuracy evaluation is taken as one of the system design factors. \nThe system response time is another major concern for big data processing system. In \nthe customer review feature identification, we propose feature information gain and \nDMRDF approach to identify significant features and to eliminate redundant customer \nreviews from the input dataset.\n\nFigure  2 illustrates significant features required for the mobile phone sustainability. \nCustomer reviews and ratings of 7 brands of mobile phones are identified and evalu-\nated with DMRDF using SVM and LR. The graph shows the significant features identi-\nfied by the model against the percentage of ********* whose reviews are analysed. 88% \nof the ********* identified internal storage as a significant feature. Product price has \nbeen identified by 79% of ********* as significant feature. With this evaluation customer \nrequirements for a product can be analysed in a better manner, thus can optimize the \ndesign of the product for better product quality and for product sustainability in the \nindustry.\n\nFigure 3 shows the comparison of the processing time taken by the proposed model \nwith different dataset size against that of the state of art techniques. DMRDF method \ntakes less time for completion of the application compared to other gini-index and latent \nsemantic analysis methods. Hence the proposed model is fast and scalable. It provides a \nhigh-speed processing performance with large datasets. This shows the ***** applica-\nbility in big data analytics, whereas gini-index and LSA-based methods processing time \nis larger for large volume of dataset. From the Fig. 3 it can be seen that with 9 GB dataset \ntime taken for prediction using LSA-based model, Gini-index model and DMRDF model \nis 342 s, 495 s and 156 s respectively. With 18 GB dataset time taken for prediction using \nLSA-based model, Gini-index model and DMRDF model *****, ***** and 256 s respec-\ntively. Gini-index and LSA-based methods time taken for 18 GB dataset is twice that of \n9 GB dataset. But for DMRDF model time taken for 18 GB dataset is 1.6 times that of \n\n79%\n\n15%\n\n45%\n35%\n\n22%\n\n40%\n\n22%\n\n39%\n\n88%\n\n53%\n\n21%\n\n61%\n\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n\n100%\n\nPe\nrc\n\nen\nta\n\nge\n o\n\nf C\nus\n\nto\nm\n\ner\ns\n\nIden�fied Significant Features\nFig. 2 Identified Significant Features from ******** reviews and Ratings\n\n\n\nPage 11 of *********** et al. J Big Data            (****) ****  \n\n9 GB dataset and also it is 3 times lesser than Gini-index method. DMRDF model has \nmore advantage compared to the other state of art techniques in the case of application \nexecution and performance.\n\nThe reliability of the methods considered for the pre-launch prediction depends on \nprecision [44], recall and prediction accuracy measurement. Table 5 shows a comparison \nof precision, recall and accuracy measures of DMRDF, Gini-index and LSA-based meth-\nods with Support Vector Machine and Logistic Regression classifiers using customer \nreviews dataset over a period of *********. The results shown in Table 3 are best proved \nusing DMRDF with Support Vector Machine classification with prediction accuracy of \n95.4%. The DMRDF outperforms LSA-based and Gini-index methods in P@R, R@R and \nPA measures. Using proposed method, true positive (TP), false positive (FP), true nega-\ntive (TN) and false negative (FN) are found out. The prediction accuracy (PA), precision \n(P@R) and recall (R@R) are computed using Eqs. (10), (11), and (12) respectively.\n\n(10)PA =\nTP + TN\n\nTP + ** + ** + **\n\n0\n\n100\n\n200\n\n300\n\n400\n\n500\n\n600\n\n700\n\n800\n\n900\n\n1000\n\n1GB 5GB 9GB 13GB 18GB\n\nGini-index\n\n*****\n\nLSA-based\n\nTi\nm\n\ne \nTa\n\nke\nn \n\nin\n se\n\nc\n\nDataset size\nFig. 3 Dataset Size versus Processing Time Graph\n\nTable 3 Performance comparison of the proposed model with state of art techniques\n\nClassifier Support vector machine\n\nMethod used P@R (precision) PA % \n(prediction \naccuracy)\n\n***** 0.941 0.92 95.4\n\nLSA-based 0.894 0.79 87.5\n\nGini-index 0.66 0.567 83.2\n\nClassifier Logistic regression\n\nMethod used P@R R@R % PA %\n\n***** 0.915 0.849 93.5\n\nLSA-based 0.839 0.753 83\n\nGini-index 0.62 0.52 79.8\n\n\n\nPage 12 of *********** et al. J Big Data            (****) **** \n\nUsing DMRDF with SVM classifier and LR classifier, the prediction accuracy varia-\ntions are less compared to LSA-based and Gini-index methods. Hence ***** out-\nperforms the other two methods for customer review feature prediction.\n\nFurthermore Fig.  4, shows the DMRDF, LSA-based and Gini-index approaches as \napplied to the customer reviews and ratings datasets for 3, 6, 12, 18 and *********. \nIn DMRDF many features *** appear in different customer review aspects, hence \nperformance evaluation will not consider duplicate ******** reviews. In Gini- index, \nfeatures are extracted based on the polarity of the reviews and for large dataset P@R \nand R@R are less. The results show that DMRDF method outperforms the other two \nmethods in big data analysis. Gini-index approach does not perform well in customer \nreview feature prediction.\n\nConclusion and future work\nTechnological development in this era brings new challenges in artificial intelligence \nlike prediction, which is the next frontier for innovation and productivity. This work \nproposes the implementation of a scalable and reliable big data processing model \n\n(11)P@R =\nTP\n\nTP + FP\n\n(12)R@R =\nTP\n\nTP + **\n\na *** b *** \n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n************\n\nP@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n************\n\nR@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\nc Logistic Regression d Logistic Regression\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n************\n\nP@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n************\n\nR@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\nFig. 4 Precision and Recall of DMRDF, LSA-based and Gini-index methods using SVM and LR classifiers\n\n\n\nPage 13 of *********** et al. J Big Data            (****) ****  \n\nwhich identify significant features and eliminates redundant data using Feature Infor-\nmation Gain and Distributed Memory-based Resilient Dataset Filter method with \nLogistic Regression and Support Vector Machine prediction classifiers. A compari-\nson of the analysis has been conducted with state of art techniques like Gini-index \nand LSA-based approaches. The prediction accuracy, precision and recall of DMRDF \nmethod outperforms the other methods. Results show that the prediction accuracy \nof the proposed method increases by 10% using significant feature identification and \nelimination of redundancy from dataset compared to state of art techniques. Large \nfeature dimensionality reduces the prediction accuracy of the LSA-based method \nwhere as number of significant features plays an important role in prediction model-\nling. Results show that proposed DMRDF model is scalable and with huge volume of \ndataset model performance is good as well as time taken for processing the applica-\ntion is less compared to state of art techniques.\n\nResilience property of DMRDF method have long lineage, hence this can achieve \nfault-tolerance. DMRDF model is fast because of the in-memory computation \nmethod. Proposed design can be extended to other product feature identification big \ndata processing domains. As a future work, the model may be developed to make real \ntime streaming predictions through a unified API that searches ******** comments, \nratings and surveys from different reliable online websites concurrently to obtain syn-\nthesis of sentiments with an information fusion approach. Since the statistical prop-\nerties of customer reviews and ratings vary over time, the performance of machine \nlearning algorithms can also come down. To cope with the limitations of deep learn-\ning matrix factorization integrated with DMRDF can be adapted.\n\nAbbreviations\nDMRDF: Distributed Memory-based Resilient Dataset Filter; FIG: Feature information gain; RDD: Resilient distributed \ndataset; SVM: Support vector machine; LR: Logistic regression; LSA: Latent semantic analysis; PA: Prediction accuracy; \nP@R: Precision; R@R: Recall; MF: Matrix factorization.\n\nAcknowledgements\nNot applicable.\n\nAuthors’ contributions\n** designed and implemented the model for Pre-launch product prediction. SN analysed and interpreted the customer \nreviews and ratings dataset regarding the pre-launch product prediction. PS supervised the design, implementation \nand analysis of the model for pre-launch product prediction. ** was a major *********** in writing the manuscript. All \n******* read and approved the final manuscript.\n\nFunding\nNot applicable.\n\nAvailability of data and materials\nThe datasets generated and/or analysed during the current study are available in the ****** repository. [snap.stanford.\nedu/data/web-Amazon.html] [40] and [http://www.kaggl *********** tClou dHQ/flipk art-produ cts] [39].\n\nCompeting interests\nThe ******* declare that they have no competing interests.\n\nAuthor details\n1 Information Technology, School **************, Cochin University of Science & Technology, ************, India. \n2 Department of Computer Science, *****************************************, ************, India. 3 Department \nof Ship Technology, *****************************************, ************, India. \n\nReceived: ***************   Accepted: ****************\n\n Published online: **************** \n\n*****************************************************\n\n\nPage 14 of *********** et al. J Big Data            (****) **** \n\nReferences\n 1. ******, *******, *******, ****, *****, ****. Text mining and probabilistic modeling for online review spam detection. \n\nACM Trans Manag Inform Syst. ****;2(4):25.\n 2. *****, ****, ******. Social commerce research: definition, research themes and the trends. Int J Inform Manag. \n\n****;37:190–201.\n 3. *********, *********. Word-of-mouth communications in marketing: a meta-analytic review of the antecedents \n\nand **********. J Acad Market Sci. ****;36(4):578–96.\n 4. ******, et al. Redundant data removal technique for efficient big data search processing. Int J Softw Eng Appl. \n\n****;7.4:427–36.\n 5. ******, **********, and ********** Mining the peanut gallery: opinion extraction and semantic classification of \n\nproduct reviews. WWW’****.\n 6. ******, ***********, ***********, *****. Large-scale parallel collaborative filtering for the netflix prize. ****. p. \n\n337–48. https ://doi.org/10.1007/978-3-540-68880 -8_32.\n 7. *********, ***********. Consumer behavior in social commerce: a literature review. *** Support Syst. \n\n****;86:95–108.\n 8. ********, *************, ************. The effect of online consumer reviews on new product sales. ************** \n\nComm. ****;17(1):39–58.\n 9. ********, *********, ********, et al. Detection of fraudulent and malicious websites by analysing user reviews \n\nfor online shopping websites. **********************. ****;5(3):171–89. https ://doi.org/10.1007/s1128 0-015-0381-x.\n 10. *******, and *******. Big data analytics. In: Proceedings of the **** international conference on communication, \n\ninformation & computing ********** (******), ************************************************* (****). ****. p. 1–4. \n******************************* t.2012.63981 80.\n\n 11. ************** et al. Addressing big data challenges for scientific data infrastructure. In: **** 4th Int. conference \ncloud computing technology and science (********). ****.\n\n 12. **********, *********, *********** and ***********. Review spam detection via time-series pattern discovery. In: \n*** Proceedings of the 21st international conference companion on World Wide Web. ****. p. 635–6.\n\n 13. *******, ******, **********. matrix factorization technique for recommender systems. Computer. ****;******.\n 14. ***************, ******, & ********. Restricted boltzmann machines for collaborative filtering. In: Proc. of the 24th \n\nInt. conference on machine learning. ****. p. 791–8.\n 15. ******, ******, *** MR. Learning to recommend with explicit and implicit social relations. *** Trans Intell Syst \n\nTechnol. ****;2(3):29.\n 16. ***************, ********, ******** A survey on detection of reviews using sentiment classification of methods. \n\n*******. ****;2(2):310–4.\n 17. ****, and ****. Memory or time—performance evaluation for iterative operation on hadoop and spark. In: Proc. of \n\nthe **** **** 10th Int. Con. on high-performance computing and communications. ****. https ://doi.org/10.1109/\nhpcc.and.euc.2013.106.\n\n 18. *************, ************, *************, ***********. Product recommendation in online social net-\nworking communities—an empirical study of antecedents and a ********. J Inform Manag. ****;56(2):185–95.\n\n 19. *******, ************. Designing novel review ranking systems: predicting the usefulness and impact of reviews. In: \nInt Conference Electron Comm ACM. ****. p. 303–10.\n\n 20. ********, *******, ******, ****. Predicting consumer product demands via Big Data: the roles of online promotional \nmarketing and online reviews. Int J Prod Res. ****;55:1–15. https ://doi.org/10.1080/00207 543.2015.10665 19.\n\n 21. ******, **********, **********, & ****** Online Feature Selection. In: Proceedings of the 22nd ACM SIGKDD Int. \nConference on KDD ‘16, ****. https ://doi.org/10.1145/29396 72.29398 81.\n\n 22. *********, ***********, and *******. Empirical analysis of predictive algorithms for collaborative filtering. In: Proc. \nof the 14th Conf. on Uncertainty in Artifical Intelligence, ****.\n\n 23. ***********, *******, *****, ******, *****, *************, ******** Spotting opinion spammers using behavioral \nfootprints. In: Proc. of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining \nChicago, ***. ****. p. 632–40.\n\n 24. ************, ***********, ****************. Statistical and Machine Learning forecasting methods: concerns and \nways forward. PLoS ***. ****;13(3):e0194889. https ://doi.org/10.1371/journ al.pone.01948 89.\n\n 25. ******, *****, *******, **************** Prediction of rainfall using logistic regression. Pak J Stat Oper Res. ****. \nhttps ://doi.org/10.18187 /pjsor .v8i3.535.\n\n 26. ******, *******, ****, ******, *******, ***** SVD Feature: a toolkit for feature-based collaborative filtering. ****** \nLearn Res. ****;13(1):3619–22.\n\n 27. *****, ********, *********** Collaborative filtering beyond the user-item matrix—a survey of the state of art and \nfuture challenges. *** Comput Surv. ****;47(1):3.\n\n 28. ******, & *********** Generalized probabilistic matrix factorizations for collaborative filtering, In Data mining \n(****), **** 10th international conference. ****. p. 1025–30.\n\n 29. ***************, & ******* Bayesian probabilistic matrix factorization using Markov chain Monte Carlo. In: Proc. of \nthe 25th int. conference on machine learning. ****. p. 880–7.\n\n 30. **********, ***************, ********, **********, ***********. Survey of review spam detection using machine \nlearning techniques. J Big Data. ****;2(1):23.\n\n 31. **********, *******. Product reviews in mobile decision aid systems. *********: PERMID; ****. p. 15–8.\n 32. *********, et al. A disease diagnosis and treatment recommendation system based on big data mining and cloud \n\ncomputing. Inform Sci. ****;435:124–49.\n 33. ********, *********, ********, ************. Aspect term extraction for sentiment analysis in large movie \n\nreviews using Gini-index feature selection method and SVM classifier. World Wide Web. ****;20:135–54. https ://doi.\norg/10.1007/s1128 0-015-0381-x.\n\n 34. ******, *********, *********, ********, *******. LIBLINEAR: A library for large linear classification. J Mach Learn Res. \n****;9:****–4.\n\n********************************************\n*****************************************\n*********************************************\n************************hpcc.and.euc.2013.106\n*********************************************\n*********************************************\n***************************************\n*******************1371/journal.pone.0194889\n***************************************\n*****************************************\n*****************************************\n\n\nPage 15 of *********** et al. J Big Data            (****) ****  \n\n 35. **********, *******, and *********** Why should I trust you?: Explaining the predictions of any classifier. In: Proc. \n*************. ***********. Discov. Data Mining. ****. p. 1135–44.\n\n 36. *****, et al. An effective scheme for QoS estimation via alternating direction method-based matrix factorization. \n**** Trans Serv Comput. ****;12(4):503–18.\n\n 37. ******, ********, ******, ***** and *****. Movie rating and review summarization in mobile environment. In: **** \ntrans. systems, man and cybernetics, Part C: applications and reviews. ****. p. 397–407.\n\n 38. ******, VN. The nature of statistical learning theory, ********, 2nd ed, ****. Translated by ***********, **********\ngong. Beijing: *******************; ****.\n\n 39. [Dataset] Flipkart-products. http://www.kaggl *********** tClou dHQ/flipk art-produ cts.\n 40. [Dataset] https ://snap.stanf ********************** n.html.\n 41. [Dataset] ****, ********** Ups and downs: modeling the visual evolution of fashion trends with one-class collabora-\n\ntive filtering. ***; ****.\n 42. **********, *********. Extracting product features and opinions from reviews. ****; *****.\n 43. *********, ***********, *****, ******, ****, **********, **********, *********, ********* Resilient distributed \n\ndatasets: A fault-tolerant abstraction for in-memory cluster computing Technical Report UCB/EECS-2011-82. ** \nBerkeley: ***************; ****.\n\n 44. *******, *********** The relationship between precision-recall and ROC curves, In ICML. ****. p. 233–40.\n 45. ******, ******. Exploring the usefulness of predicting people’s locations. Procedia Soc Beh Sci. ****. https ://doi.\n\norg/10.1016/j.sbspr o.*******.451.\n\nPublisher’s Note\n*************** remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\n*****************************************************\n**********************************************\n********************************************\n*****************************************451\n\n\tImproving prediction with enhanced Distributed Memory-based Resilient Dataset Filter\n\tAbstract \n\tIntroduction\n\tRelated work\n\tMethodology\n\tData collection phase\n\tDataset pre-processing\n\tResilient Distributed Dataset\n\n\tPrediction classifiers\n\tLogistic regression (LR)\n\tSupport Vector Machine (***)\n\n\tExperimental setup\n\n\tResults and discussions\n\tConclusion and future work\n\tAcknowledgements\n\tReferences\n\n\n\n\n",
      "merged_content": "\nImproving prediction with enhanced \nDistributed Memory‑based Resilient Dataset \nFilter\nSandhya Narayanan1*, Philip Samuel2 and Mariamma Chacko3\n\nIntroduction\nAnalyzing and processing massive volumes of data in different applications like sensor \ndata, health care and e-Commerce require big data processing technologies. Extracting \nuseful information from the enormous size of unstructured data is a crucial thing. As the \namount of data becomes more extensive, sophisticated pre-processing techniques are \nrequired to analyze the data. In social networking sites and other online shopping sites, \na massive volume of online product reviews from a large size of customers are available \n[1]. The impact of online product reviews affects 90% of the current e-Commerce mar-\nket [2]. Customer reviews contribute the product sale to an extent and product life in the \nmarket depends on online product recommendations.\n\nOnline feedback is one of the communication methods which gives direct suggestions \nfrom the customers [3, 4]. Online reviews and ratings from customers are another infor-\nmation source about product quality [5, 6]. Customer reviews can help to decide on a new \nsuccessful product launch. Online shopping has several advantages over retail shopping. In \nretail shopping, the customers visit the shop and receive price information but less product \n\nAbstract \n\nLaunching new products in the consumer electronics market is challenging. Develop-\ning and marketing the same in limited time affect the sustainability of such companies. \nThis research work introduces a model that can predict the success of a product. A \nFeature Information Gain (FIG) measure is used for significant feature identification \nand Distributed Memory-based Resilient Dataset Filter (DMRDF) is used to eliminate \nduplicate reviews, which in turn improves the reliability of the product reviews. The \npre-processed dataset is used for prediction of product pre-launch in the market using \nclassifiers such as Logistic regression and Support vector machine. DMRDF method is \nfault-tolerant because of its resilience property and also reduces the dataset redun-\ndancy; hence, it increases the prediction accuracy of the model. The proposed model \nworks in a distributed environment to handle a massive volume of the dataset and \ntherefore, it is scalable. The output of this feature modelling and prediction allows the \nmanufacturer to optimize the design of his new product.\n\nKeywords: Distributed Memory-based, Resilient Distribution Dataset, Redundancy\n\nOpen Access\n\n© The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and \nthe source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material \nin this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material \nis not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the \npermitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creat iveco \nmmons .org/licen ses/by/4.0/.\n\nRESEARCH\n\nNarayanan et al. J Big Data            (2020) 7:13  \nhttps://doi.org/10.1186/s40537‑020‑00292‑y\n\n*Correspondence:   \nnairsands@gmail.com \n1 Information Technology, \nSchool of Engineering, \nCochin University of Science \n& Technology, Kochi 682022, \nIndia\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-020-00292-y&domain=pdf\n\n\nPage 2 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\ninformation from shop owners. On the other hand, online shopping sites give product \nreviews and previous customer feedbacks without extra cost and effort for the customers \n[7–10].\n\nInvesting in poor quality products potentially affects an industry’s brand loyalty and this \nstrategy should be changed by the eCommerce firms [5, 11]. Consumer product success \ndepends on different criteria, such as the quality of the product and marketing strategies. \nThe users should provide their valuable and accurate reviews about the products [12]. Cus-\ntomers bother to give reviews about products, whether they liked it or not. If the users \nprovide reviews, then other retailers can create some duplicated reviews [13, 14]. In online \nmarketing, the volume and value of product reviews are examined [15, 16]. The number \nof the product reviews on the shopping sites, blogs and forums has increased awareness \namong the users. This large volume of the reviews leads to the need for significant data \nprocessing methods [17, 18]. The value is the rating on the products. The ratio of positive to \nnegative reviews about the product leads to the quality of the product [19, 20].\n\nFeature selection is a crucial phase in data pre-processing [21]. Selecting features from \nan un-structured massive volume of data reduce the model complexity and improves the \nprediction accuracy. Different feature selection methods existing are the filter, wrapper and \nembedded. The wrapper feature selection method evaluates the usefulness of the feature \nand it depends on the performance of the classifier [22]. The filter method calculates the \nrelevance of the features and analyzes data in a univariate manner. The embedded process \nis similar to the wrapper method. Embedded and wrapper methods are more expensive \ncompared to the filter method. The state-of-art methods in customer review analysis gener-\nally discuss on categorizing positive and negative reviews using different natural language \nprocessing techniques and spam reviews recognition [23]. Feature selection of customer \nreviews increases prediction accuracy, thereby improves the model performance.\n\nAn enhanced method, which is a combination of filter and wrapper method is proposed \nin this work, which focuses on product pre-launch prediction with enhanced distributive \nfeature selection method. Since many redundant reviews are available on the web in large \nvolumes, a big data processing model has been implemented to filter out duplicated and \nunreliable data from customer reviews in-order to increase prediction accuracy. A scalable \nbig data processing model has been applied to predict the success or failure of a new prod-\nuct. The realization of the model has been done by Distributed Memory-based Resilient \nDataset Filter with prediction classifiers.\n\nThis paper is organized as follows. “Related work” section discusses related work. “Meth-\nodology” section contains the proposed methodology with System design, Resilient Distrib-\nuted Dataset and Prediction using classifiers. “Results and discussions” section summarizes \nresults and discussion. The conclusion of the paper is shown in “Conclusion and future \nwork” section.\n\nRelated work\nMakridakis et al. [24] illustrate that machine learning methods are alternative methods \nfor statistical analysis of multiple forecasting field. Author claims that statistical methods \nare more accurate than machine learning [25] methods. The reason for less accuracy is \nthe unknown values of data i.e., improper knowledge and pre-processing of data.\n\n\n\nPage 3 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nDifferent works have been implemented using the Matrix factorization (MF) [14] \nmethod with collaborative filtering [26]. Hao et al. [15] focused on a work based on the \nfactorization of the user rating matrix into two vectors, i.e., user latent and item latent \nwith low dimensionality. The sum of squared distance can be minimized by training a \nmodel that can find a solution using Stochastic Gradient Decent [27] or by least squares \n[28]. Salakhutdinov et al. [29] proposed a method that can be scaled linearly by probabil-\nity related matrix factorization on a big volume of datasets and then comparing it with \nthe single value decomposition method. This matrix factorization outperforms other \nprobability factorization methods like Bayesian-based probabilistic analysis [29] and \nstandard probability-based matrix factorization methods. A conventional approach, like \ntraditional collaborative Filtering [13, 30] method depends on customers and items. The \nuser item matrix factorization technique has been used for implementation purpose. \nIn the recommender system, there is a limitation in the sparsity problem and cold start \nproblem. In addition to the user item matrix factorization method, various analyses and \napproaches have been implemented to solve these recommendation issues.\n\nWietsma et al. [31] proposed a recommender system that gives information about the \nmobile decision aid and filtering function. This has been implemented with a study of \n29 features of student user behavior. The result shows the correlation among the user \nreviews and product reviews from different websites. Jianguo Chen et al. [32] proposed \na recommendation system for the treatment and diagnosis of the diseases. For cluster \nanalysis of disease symptoms, a density-peaked method is adopted. A rule-based apriori \nalgorithm is used for the diagnosis of disease and treatment. Asha et al. [33] proposed \nthe Gini-index feature method using movie review dataset. The sentimental analysis \nof the reviews are performed and opinion extraction of the sentences are done. Gini-\nindex impurity measure improves the accuracy of the polarity prediction by sentimental \nanalysis using Support vector machine [34, 35]. Depending on the frequency of occur-\nrence of a word in the document, the term frequency is calculated and opinion words \nare extracted using the Gini-index method. In this method, high term frequency words \nare not included, as it decreases the precision. The disadvantage of this method is that \nfor the huge volume of data, the prediction accuracy decreases.\n\nLuo et al. [36] proposed a method based on historical data to analyze the quality of \nservice for automatic service selection. Liu et al. [37] proposed a system in a mobile envi-\nronment for movie rating and review summarization. The authors used Latent Semantic \nAnalysis (LSA-based) method for product feature identification and feature-based sum-\nmarization. Statistical methods [38] have been used for identifying opinion words. The \ndisadvantage of this method is that LSA-based method cannot be represented efficiently; \nhence, it is difficult to index based on individual dimensions. This reduces the prediction \naccuracy in large datasets.\n\nLack of appropriate computing models for handling huge volume and redundancy in \ncustomer review datasets is a major challenge. Another major challenge handled in the \nproposed work is the existence of a pre-launch product in the industry based on the \nproduct features, which can be predicted based on the customer feedback in the form \nof reviews and ratings of the existing products. This prediction helps to optimize the \ndesign of the product to improve its quality with the required product features. Many \nof the relational database management systems are handling structured data, which is \n\n\n\nPage 4 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nnot scalable for big data that handles a large volume of unstructured data. This proposed \nmodel solves the problem of redundancy in a huge volume of the dataset for better pre-\ndiction accuracy.\n\nMethodology\nA pre-launch product prediction using different classifiers has been analysed by huge \ncustomer review and rating dataset. The product prediction is done through the phases \nconsisting of data collection phase, feature selection and duplicate data removal, build-\ning prediction classifier, training as well as testing.\n\nFigure 1 describes the various stages in system design of the model. The input dataset \nconsists of multivariate data which includes categorical, real and text data. Input dataset \nis fed for data pre-processing. Data pre-processing consists of feature selection, redun-\ndancy elimination and data integration which is done using Feature Information Gain \nand Distributed Memory-based Resilient Dataset Filter approach. The cleaned dataset \nis trained using classification algorithms. The classifiers considered for training are Sup-\nport Vector Machine (SVM) and Logistic Regression (LR). Further the dataset is tested \nfor pre-launch prediction using LR and SVM.\n\nData collection phase\n\nThis methodology can be applied for different products. Several datasets like Ama-\nzon and flip cart customer reviews are available as public datasets [39–41]. The data-\nset of customer reviews and ratings of seven brands of mobile phones for a period of \n24 months are considered in this work. The mobile phones product reviews are chosen \nbecause of two reasons. New mobile phones are launched into the market industry day \nby day which is one of the unavoidable items in everyone’s life. Market sustainability for \nthe mobile phones is very low.\n\nTable  1 shows a sample set of product reviews in which input dataset consists of \nuser features and product features. User features consists of Author, ReviewID and \nTitle depending on the user. Product feature consists of Product categories, Overall \nratings and Review Content. Since mobile phone is taken as the product, the catego-\nrization is done according to the features such as Battery life, price, camera, RAM, \n\nData collection \n\nCategorical\n\nText\n\nReal\n\nData Pre-\nprocessing\n\nFeature \nIdentification\n\nRedundancy\nRemoval\n\nData \nIntegration\n\nTraining \nDataset Using \nclassification \nalgorithms\n\nSupport \nVector \n\nLogistic \nRegression\n\nTesting Dataset \nUsing \n\nclassification \nalgorithms\n\nLogistic \nRegression\n\nSupport \nVector \n\nFig. 1 Product prelaunch prediction System Design\n\n  \n\n\n\nPage 5 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nprocessor, weight etc. Some features are given a priority weightage depending on the \nproduct and user requirements. Input dataset with JSON file format is taken.\n\nDataset pre‑processing\n\nIn data pre-processing, feature selection plays a major role. In the product review \ndataset of a mobile phone, a large number of features exist. Identifying a feature from \ncustomer reviews is important for this model to improve the prediction accuracy. \nEnhanced Feature Information Gain measure has been implemented to identify sig-\nnificant feature.\n\nFeatures are identified based on the content of the product reviews, ratings of the \nproduct reviews and opinion identification of the reviews. Ratings of the product \nreviews can be further categorized based on a rating scale of 5 (1—Bad, 2—Average, \n3—Good, 4—very good, 5—Excellent). For opinion identification of the product, the \npolarity of extracted opinions for each review is classified using Senti-WordNet [42].\n\nFeature Information Gain measures the amount of information of a feature \nretrieved from a particular review. Impurity which is the measure of reliability of fea-\ntures in the input dataset should be reduced to get significant features. To measure \nfeature impurity, the best information of a feature obtained from each review is calcu-\nlated as follows\n\n• Let Pi be the probability of any feature instance \n(\n\nf\n)\n\n of k feature set F =\n{\n\nf1, f2, . . . fk\n}\n\n \nbelonging to  ith customer review Ri , where i varies from 1 to N.\n\n• Let N denotes the total number of customer reviews.\n• Let OR denotes the polarity of extracted opinions of the Review.\n• Let SR denotes product rating scale of review (R).\n\nTable 1 Sample set of Product Reviews\n\n\n\nPage 6 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nThe information of a feature with respect to review rating and opinion is denoted by \nIf\n\nExpected information gain of the feature denoted as Ef\n\nReview Feature Impurity R(I) is calculated as\n\nThen Feature Information Gain (�G) to find out significant features are calculated \nas\n\nFeatures are selected based on the �G value and those with an Information gain \ngreater than 0.5 is selected as a significant feature. Table 2 shows the significant fea-\nture from customer reviews and ratings.\n\nNext step is to eliminate the redundant reviews and to replace null values of an \nactive customer from the customer review dataset using an enhanced big data pro-\ncessing approach. Reviews with significant features obtained from feature identifica-\ntion are considered for further processing.\n\n(1)If = log2\n\n(\n\n1\n\nP(R = F)\n\n)\n\n∗ OR ∗ SR.\n\n(2)Ef =\n\nN\n∑\n\ni=1\n\n−Pi(R = F).\n∥\n\n∥If\n∥\n\n∥\n\n1\n.\n\n(3)R(I) = −\n\nN\n∑\n\ni=1\n\nPi.log2Ef .\n\n(4)�G = R(I)−\n\nN\n∑\n\ni=1\n\n[(\n\nOR\n\nN\n∗ Ef\n\n)\n\n−\n\n(\n\nSR\n\nN\n∗ Ef\n\n)]\n\n.\n\nTable 2 Significant Features from Customer Reviews and Ratings\n\nNo Customer reviewed features No Customer reviewed features\n\n1 Author 17 RAM\n\n2 Title 18 Sim type\n\n3 ReviewID 19 Product category\n\n4 Content 20 Thickness\n\n5 Product brand 21 Weight of mobile phone\n\n6 Ratings 22 Height\n\n7 Battery life 23 Product type\n\n8 Price 24 Product rating\n\n9 Feature information gain 25 Front camera\n\n10 Review type 26 Back camera\n\n11 Product display 27 Opinion of review\n\n12 Processor 28 Multi-band\n\n13 Operating system 29 Network support\n\n14 Water proof 30 Quick charging\n\n15 Rear camera 31 Finger sensor\n\n16 Applications inbuilt 32 Internal storage\n\n\n\nPage 7 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nResilient Distributed Dataset\n\nResilient Distributed Dataset (RDD) [43] is a big data processing approach, which allows \nto store cache chunks of data on memory and persevere it as per the requirements. \nThe in-memory data caching is supported by RDD. Variety of jobs at a point of time \nis another challenge which is handled by RDD. This method deals with chunks of data \nduring processing and analysis. RDD can also be used for machine learning supported \nsystems as well as in big data processing and analysis, which happens to be an almost \npervasive requirement in the industry.\n\nIn the proposed method the main actions of RDD are:\n\n• Reduce (β): Combine all the elements of the dataset using the function β.\n• First (): This function will return the first element\n• takeOrdered(n): RDD is returned with first ‘n’ elements.\n• saveAsSequenceFile(path): the elements in the dataset to be written to the local file \n\nsystem with given path.\n\nThe main Transformations of RDD are:\n\n• map(β): Elements from the input file is mapped and new dataset is returned through \nfunction β.\n\n• filter(β): New dataset is returned if the function β returns true.\n• groupBykey(): When called a dataset of (key, value) pairs, this function returns a \n\ndataset of (key, value) pairs.\n• ReduceBykey(β): A (key, value) pair dataset is returned, where the values of each key \n\nare combined using the given reduce function β.\n\nIn the proposed work an enhanced Distributed Memory-based Resilience Dataset \nFilter (DMRDF) is applied. DMRDF method have long Lineage and it is recomputed \nthemselves using prior information, thus it achieves fault-tolerance. DMRDF has been \nimplemented to remove the redundancy in the dataset for product pre-launch predic-\ntion. This enhanced method is simple and fast.\n\n• Let the list of n customers represented as C = {c1, c2, c3 . . . , cn}\n\n• Let the list of N reviews be represented as R = {r1, r2, r3 . . . , rN }\n\n• Let x significant features are identified from feature set (F  ) represented as Fx ⊂ F\n\n• An active customer consists of significant feature having information Gain value \ndenoted by �G\n\nIn the DMRDF method, a product is chosen and its customer reviews are found out. \nEliminate customers with similar reviews on the selected product and also reviews \nwith insignificant features. Calculate the memory-based Resilient Dataset Filter score \nbetween each of the customer reviews with significant features.\n\nLet us consider a set C of ‘n’ number of customers, the set R of ‘N’ number of reviews and \na set of significant features ′F ′\n\nx are considered. The corresponding vectors are represented \nas KC , KR and KFx . Then KRi is represented using a row vector and KFj is represented using \nthe column vector. Each entry KCm denote the number of times the  mth review arrives in \n\n\n\nPage 8 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\ncustomers. The similarities between ith review of mth customer is found out using  L1 norm \nof KRi and KCm . The Distributed Memory-based resilient filter score δ is calculated using the \nEq. (5).\n\nThe δ score is calculated for each customer review whereas the score lies between [0,1]. \nThe significant features are found out using Eq. 4. For customer reviews without significant \nfeatures, �G value will be zero. The reviews with δ score value 0 are found to be insignificant \nwithout any significant feature or opinion and hence those reviews are eliminated and not \nconsidered for further processing in the work. More than one Distributed Memory-based \nresilient filter score value is identified then the second occurrence of the review is consid-\nered as duplicate.\n\nPrediction classifiers\n\nLogistic regression and Support Vector Machine classifiers are the supervised machine \nlearning approaches used in the proposed work for product pre-launch prediction.\n\nLogistic regression (LR)\n\nWe have implemented proposed model using logistic regression analysis for prediction. \nThis model predicts the failure or success of a new product in the market by analysing \nselected product features from customer reviews. A case study has been conducted using \nthe dataset of customer reviews of mobile phones. Success or failure is the predictor vari-\nable used for training and testing the dataset. For training the model 75% of the dataset is \nused and for testing the model, remaining 25% is used.\n\n• Let p be the prediction variable value, assigning 0 for failure and 1 for success.\n• p0 is the constant value.\n• b is the logarithmic base value.\n\nThen the logit function is,\n\nThen the Logistic regression value γ is shown in Eq. (7),\n\n(5)δ =\n\nN\nn\n�\n\ni = 1\n\nm = 1\n\n\n\n\n\n�\n\nKRi ∗\n\n�\n\n�x\nj=1 KFj\n\n��\n\n∗ KCm\n\nKRi · KCm\n\n\n\n ∗ |�G|\n\n(6)\nL0 = b\n\np0+p\nx\n∑\n\ni=1\n\nfi\n\n(7.1)γ =\nL0\n\n(\n\nbp0+p\n∑x\n\ni=1 fi\n)\n\n+ 1\n\n(7.2)=\n1\n\n1+ b\n−\n\n(\n\nb\np0+p\n\n∑x\ni=1\n\nfi\n)\n\n\n\nPage 9 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nThe probability value of γ lies between [0,1]. In this work, if this value is greater than 0.5 \nthe pre-launch prediction of the product is considered as success and for values less than \n0.5, it is considered as failure.\n\nSupport Vector Machine (SVM)\n\nSVM is the supervised machine learning method, used to learn from set of data to get new \nskills and knowledge. This classification method can learn from data features relationships \n( zi ) and its class \n\n(\n\nyi\n)\n\n that can be applied to predict the success or failure class the product \nbelongs to.\n\n• For a set T  of t training feature vectors, zi ∈ RD, where i = 1 to t.\n• Let yi ∈ {+1,−1} , where +1 belongs to product success class and -1 belongs to product \n\nfailure class.\n• The data separation occurs in the real numbers denoted as X in the D dimensional \n\ninput space.\n• Let w be the hyper plane normal vector element, where w ∈ XD.\n\nThe hyper plane is placed in such a way that distance between the nearest vectors of the \ntwo classes to the hyperplane should be maximum. Thus, the decision hyper plane is calcu-\nlated as,\n\nThe conditions for training dataset d ∈ X , is calculated as\n\nTo maximize the margin the value of w should be minimized.\nThe products in the positive one class (+1) are considered as successful products, [from \n\nEq. (9)] and those in the negative one class (−1) [from Eq. (10)] are in failure class.\n\nExperimental setup\n\nThe proposed system was implemented using Apache Spark 2.2.1 framework. Spark pro-\ngramming for python using PySpark version 2.1.2, which is the Spark python API has been \nused for the application development. An Ubuntu running Apache web server using Web \nServer Gateway Interface is used. Amazon Web Services is used to run some components \nof the software system large servers (nodes), having two Intel Xeon E5-2699V4 2.2 G Hz \nprocessors (VCPUs) with 4 cores and 16 GB of RAM on different Spark cluster configura-\ntions. According to the scalability requirements the software components can be config-\nured and can run on separate servers.\n\n(8)α(w) =\n2\n\n�w�\n\n(9)wtzi + d ≥ 1, where yi = +1.\n\n(10)wtzi + d ≤ −1, whereyi = yi − 1.\n\n\n\nPage 10 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nResults and discussions\nTo evaluate our prediction system several case studies have been conducted. Support \nVector Machine and Logistic regression classifiers are employed to perform the predic-\ntion. Most significant customer review features are used to analyse the system perfor-\nmance. The prediction accuracy evaluation is taken as one of the system design factors. \nThe system response time is another major concern for big data processing system. In \nthe customer review feature identification, we propose feature information gain and \nDMRDF approach to identify significant features and to eliminate redundant customer \nreviews from the input dataset.\n\nFigure  2 illustrates significant features required for the mobile phone sustainability. \nCustomer reviews and ratings of 7 brands of mobile phones are identified and evalu-\nated with DMRDF using SVM and LR. The graph shows the significant features identi-\nfied by the model against the percentage of customers whose reviews are analysed. 88% \nof the customers identified internal storage as a significant feature. Product price has \nbeen identified by 79% of customers as significant feature. With this evaluation customer \nrequirements for a product can be analysed in a better manner, thus can optimize the \ndesign of the product for better product quality and for product sustainability in the \nindustry.\n\nFigure 3 shows the comparison of the processing time taken by the proposed model \nwith different dataset size against that of the state of art techniques. DMRDF method \ntakes less time for completion of the application compared to other gini-index and latent \nsemantic analysis methods. Hence the proposed model is fast and scalable. It provides a \nhigh-speed processing performance with large datasets. This shows the DMRDF applica-\nbility in big data analytics, whereas gini-index and LSA-based methods processing time \nis larger for large volume of dataset. From the Fig. 3 it can be seen that with 9 GB dataset \ntime taken for prediction using LSA-based model, Gini-index model and DMRDF model \nis 342 s, 495 s and 156 s respectively. With 18 GB dataset time taken for prediction using \nLSA-based model, Gini-index model and DMRDF model 740 s, 910 s and 256 s respec-\ntively. Gini-index and LSA-based methods time taken for 18 GB dataset is twice that of \n9 GB dataset. But for DMRDF model time taken for 18 GB dataset is 1.6 times that of \n\n79%\n\n15%\n\n45%\n35%\n\n22%\n\n40%\n\n22%\n\n39%\n\n88%\n\n53%\n\n21%\n\n61%\n\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n\n100%\n\nPe\nrc\n\nen\nta\n\nge\n o\n\nf C\nus\n\nto\nm\n\ner\ns\n\nIden�fied Significant Features\nFig. 2 Identified Significant Features from Customer reviews and Ratings\n\n\n\nPage 11 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\n9 GB dataset and also it is 3 times lesser than Gini-index method. DMRDF model has \nmore advantage compared to the other state of art techniques in the case of application \nexecution and performance.\n\nThe reliability of the methods considered for the pre-launch prediction depends on \nprecision [44], recall and prediction accuracy measurement. Table 5 shows a comparison \nof precision, recall and accuracy measures of DMRDF, Gini-index and LSA-based meth-\nods with Support Vector Machine and Logistic Regression classifiers using customer \nreviews dataset over a period of 24 months. The results shown in Table 3 are best proved \nusing DMRDF with Support Vector Machine classification with prediction accuracy of \n95.4%. The DMRDF outperforms LSA-based and Gini-index methods in P@R, R@R and \nPA measures. Using proposed method, true positive (TP), false positive (FP), true nega-\ntive (TN) and false negative (FN) are found out. The prediction accuracy (PA), precision \n(P@R) and recall (R@R) are computed using Eqs. (10), (11), and (12) respectively.\n\n(10)PA =\nTP + TN\n\nTP + TN + FP + FN\n\n0\n\n100\n\n200\n\n300\n\n400\n\n500\n\n600\n\n700\n\n800\n\n900\n\n1000\n\n1GB 5GB 9GB 13GB 18GB\n\nGini-index\n\nDMRDF\n\nLSA-based\n\nTi\nm\n\ne \nTa\n\nke\nn \n\nin\n se\n\nc\n\nDataset size\nFig. 3 Dataset Size versus Processing Time Graph\n\nTable 3 Performance comparison of the proposed model with state of art techniques\n\nClassifier Support vector machine\n\nMethod used P@R (precision) PA % \n(prediction \naccuracy)\n\nDMRDF 0.941 0.92 95.4\n\nLSA-based 0.894 0.79 87.5\n\nGini-index 0.66 0.567 83.2\n\nClassifier Logistic regression\n\nMethod used P@R R@R % PA %\n\nDMRDF 0.915 0.849 93.5\n\nLSA-based 0.839 0.753 83\n\nGini-index 0.62 0.52 79.8\n\n\n\nPage 12 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nUsing DMRDF with SVM classifier and LR classifier, the prediction accuracy varia-\ntions are less compared to LSA-based and Gini-index methods. Hence DMRDF out-\nperforms the other two methods for customer review feature prediction.\n\nFurthermore Fig.  4, shows the DMRDF, LSA-based and Gini-index approaches as \napplied to the customer reviews and ratings datasets for 3, 6, 12, 18 and 24 months. \nIn DMRDF many features may appear in different customer review aspects, hence \nperformance evaluation will not consider duplicate customer reviews. In Gini- index, \nfeatures are extracted based on the polarity of the reviews and for large dataset P@R \nand R@R are less. The results show that DMRDF method outperforms the other two \nmethods in big data analysis. Gini-index approach does not perform well in customer \nreview feature prediction.\n\nConclusion and future work\nTechnological development in this era brings new challenges in artificial intelligence \nlike prediction, which is the next frontier for innovation and productivity. This work \nproposes the implementation of a scalable and reliable big data processing model \n\n(11)P@R =\nTP\n\nTP + FP\n\n(12)R@R =\nTP\n\nTP + FN\n\na SVM b SVM \n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nP@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nR@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\nc Logistic Regression d Logistic Regression\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nP@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nR@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\nFig. 4 Precision and Recall of DMRDF, LSA-based and Gini-index methods using SVM and LR classifiers\n\n\n\nPage 13 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nwhich identify significant features and eliminates redundant data using Feature Infor-\nmation Gain and Distributed Memory-based Resilient Dataset Filter method with \nLogistic Regression and Support Vector Machine prediction classifiers. A compari-\nson of the analysis has been conducted with state of art techniques like Gini-index \nand LSA-based approaches. The prediction accuracy, precision and recall of DMRDF \nmethod outperforms the other methods. Results show that the prediction accuracy \nof the proposed method increases by 10% using significant feature identification and \nelimination of redundancy from dataset compared to state of art techniques. Large \nfeature dimensionality reduces the prediction accuracy of the LSA-based method \nwhere as number of significant features plays an important role in prediction model-\nling. Results show that proposed DMRDF model is scalable and with huge volume of \ndataset model performance is good as well as time taken for processing the applica-\ntion is less compared to state of art techniques.\n\nResilience property of DMRDF method have long lineage, hence this can achieve \nfault-tolerance. DMRDF model is fast because of the in-memory computation \nmethod. Proposed design can be extended to other product feature identification big \ndata processing domains. As a future work, the model may be developed to make real \ntime streaming predictions through a unified API that searches customer comments, \nratings and surveys from different reliable online websites concurrently to obtain syn-\nthesis of sentiments with an information fusion approach. Since the statistical prop-\nerties of customer reviews and ratings vary over time, the performance of machine \nlearning algorithms can also come down. To cope with the limitations of deep learn-\ning matrix factorization integrated with DMRDF can be adapted.\n\nAbbreviations\nDMRDF: Distributed Memory-based Resilient Dataset Filter; FIG: Feature information gain; RDD: Resilient distributed \ndataset; SVM: Support vector machine; LR: Logistic regression; LSA: Latent semantic analysis; PA: Prediction accuracy; \nP@R: Precision; R@R: Recall; MF: Matrix factorization.\n\nAcknowledgements\nNot applicable.\n\nAuthors’ contributions\nSN designed and implemented the model for Pre-launch product prediction. SN analysed and interpreted the customer \nreviews and ratings dataset regarding the pre-launch product prediction. PS supervised the design, implementation \nand analysis of the model for pre-launch product prediction. MC was a major contributor in writing the manuscript. All \nauthors read and approved the final manuscript.\n\nFunding\nNot applicable.\n\nAvailability of data and materials\nThe datasets generated and/or analysed during the current study are available in the Kaggle repository. [snap.stanford.\nedu/data/web-Amazon.html] [40] and [http://www.kaggl e.com/Promp tClou dHQ/flipk art-produ cts] [39].\n\nCompeting interests\nThe authors declare that they have no competing interests.\n\nAuthor details\n1 Information Technology, School of Engineering, Cochin University of Science & Technology, Kochi 682022, India. \n2 Department of Computer Science, Cochin University of Science & Technology, Kochi 682022, India. 3 Department \nof Ship Technology, Cochin University of Science & Technology, Kochi 682022, India. \n\nReceived: 25 October 2019   Accepted: 17 February 2020\n\n Published online: 28 February 2020 \n\nhttp://www.kaggle.com/PromptCloudHQ/flipkart-products\n\n\nPage 14 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nReferences\n 1. Lau RY, Liao SY, Kwok RC, Xu K, Xia Y, Li Y. Text mining and probabilistic modeling for online review spam detection. \n\nACM Trans Manag Inform Syst. 2011;2(4):25.\n 2. Lin X, Li Y, Wang X. Social commerce research: definition, research themes and the trends. Int J Inform Manag. \n\n2017;37:190–201.\n 3. Matos CAD, Rossi CAV. Word-of-mouth communications in marketing: a meta-analytic review of the antecedents \n\nand moderators. J Acad Market Sci. 2008;36(4):578–96.\n 4. Jeon S, et al. Redundant data removal technique for efficient big data search processing. Int J Softw Eng Appl. \n\n2013;7.4:427–36.\n 5. Dave K, Lawrence S, and Pennock D. Mining the peanut gallery: opinion extraction and semantic classification of \n\nproduct reviews. WWW’2003.\n 6. Zhou Y, Wilkinson D, Schreiber R, Pan R. Large-scale parallel collaborative filtering for the netflix prize. 2008. p. \n\n337–48. https ://doi.org/10.1007/978-3-540-68880 -8_32.\n 7. Zhang KZK, Benyoucef M. Consumer behavior in social commerce: a literature review. Dec Support Syst. \n\n2016;86:95–108.\n 8. Cui Geng, Lui Hon-Kwong, Guo Xiaoning. The effect of online consumer reviews on new product sales. Int J Electron \n\nComm. 2012;17(1):39–58.\n 9. Manek AS, Shenoy PD, Mohan MC, et al. Detection of fraudulent and malicious websites by analysing user reviews \n\nfor online shopping websites. Int J Knowl Web Intell. 2016;5(3):171–89. https ://doi.org/10.1007/s1128 0-015-0381-x.\n 10. Singh S, and Singh N. Big data analytics. In: Proceedings of the 2012 international conference on communication, \n\ninformation & computing technology (ICCICT), institute of electrical and electronics engineers (IEEE). 2012. p. 1–4. \nhttp://dx.doi.org/10.1109/iccic t.2012.63981 80.\n\n 11. Demchenko Yuri et al. Addressing big data challenges for scientific data infrastructure. In: IEEE 4th Int. conference \ncloud computing technology and science (CloudCom). 2012.\n\n 12. Sihong Xie, Guan Wang, Shuyang Lin and Yu Philip S. Review spam detection via time-series pattern discovery. In: \nACM Proceedings of the 21st international conference companion on World Wide Web. 2012. p. 635–6.\n\n 13. Koren Y, Bell R, Volinsky C. matrix factorization technique for recommender systems. Computer. 2009;8:30–7.\n 14. Salakhutdinov R, Mnih A, & Hinton G. Restricted boltzmann machines for collaborative filtering. In: Proc. of the 24th \n\nInt. conference on machine learning. 2007. p. 791–8.\n 15. Hao MA, King I, Lyu MR. Learning to recommend with explicit and implicit social relations. ACM Trans Intell Syst \n\nTechnol. 2011;2(3):29.\n 16. Bandakkanavar V, Ramesh M, Geeta V. A survey on detection of reviews using sentiment classification of methods. \n\nIJRITCC. 2014;2(2):310–4.\n 17. Gu V, and Li H. Memory or time—performance evaluation for iterative operation on hadoop and spark. In: Proc. of \n\nthe 2013 IEEE 10th Int. Con. on high-performance computing and communications. 2013. https ://doi.org/10.1109/\nhpcc.and.euc.2013.106.\n\n 18. Zhang Hanpeng, Wang Zhaohua, Chen Shengjun, Guo Chengqi. Product recommendation in online social net-\nworking communities—an empirical study of antecedents and a mediator. J Inform Manag. 2019;56(2):185–95.\n\n 19. Ghose A, Ipeirotis PG. Designing novel review ranking systems: predicting the usefulness and impact of reviews. In: \nInt Conference Electron Comm ACM. 2007. p. 303–10.\n\n 20. Chong AY, Ch’ng E, Liu MJ, Li B. Predicting consumer product demands via Big Data: the roles of online promotional \nmarketing and online reviews. Int J Prod Res. 2015;55:1–15. https ://doi.org/10.1080/00207 543.2015.10665 19.\n\n 21. Yang H, Fujimaki R, Kusumura Y, & Liu J. Online Feature Selection. In: Proceedings of the 22nd ACM SIGKDD Int. \nConference on KDD ‘16, 2016. https ://doi.org/10.1145/29396 72.29398 81.\n\n 22. Breese JS, Heckerman D, and Kadie C. Empirical analysis of predictive algorithms for collaborative filtering. In: Proc. \nof the 14th Conf. on Uncertainty in Artifical Intelligence, 1998.\n\n 23. Mukherjee A, Kumar A, Liu B, Wang J, Hsu M, Castellanos M, Ghosh R. Spotting opinion spammers using behavioral \nfootprints. In: Proc. of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining \nChicago, ACM. 2013. p. 632–40.\n\n 24. Makridakis S, Spiliotis E, Assimakopoulos V. Statistical and Machine Learning forecasting methods: concerns and \nways forward. PLoS ONE. 2018;13(3):e0194889. https ://doi.org/10.1371/journ al.pone.01948 89.\n\n 25. Imon A, Roy C, Manos C, Bhattacharjee S. Prediction of rainfall using logistic regression. Pak J Stat Oper Res. 2012. \nhttps ://doi.org/10.18187 /pjsor .v8i3.535.\n\n 26. Chen T, Zhang W, Lu Q, Chen K, Zheng Z, Yu Y. SVD Feature: a toolkit for feature-based collaborative filtering. J Mach \nLearn Res. 2012;13(1):3619–22.\n\n 27. Shi Y, Larson M, Hanjalic A. Collaborative filtering beyond the user-item matrix—a survey of the state of art and \nfuture challenges. ACM Comput Surv. 2014;47(1):3.\n\n 28. Shan H, & Banerjee A. Generalized probabilistic matrix factorizations for collaborative filtering, In Data mining \n(ICDM), IEEE 10th international conference. 2010. p. 1025–30.\n\n 29. Salakhutdinov R, & Mnih A. Bayesian probabilistic matrix factorization using Markov chain Monte Carlo. In: Proc. of \nthe 25th int. conference on machine learning. 2008. p. 880–7.\n\n 30. Crawford M, Khoshgoftaar TM, Prusa JD, Richter AN, Al Najada H. Survey of review spam detection using machine \nlearning techniques. J Big Data. 2015;2(1):23.\n\n 31. Wietsma TA, Ricci F. Product reviews in mobile decision aid systems. Francesco: PERMID; 2005. p. 15–8.\n 32. Jianguo C, et al. A disease diagnosis and treatment recommendation system based on big data mining and cloud \n\ncomputing. Inform Sci. 2018;435:124–49.\n 33. Manek AS, Shenoy PD, Mohan MC, Venugopal KR. Aspect term extraction for sentiment analysis in large movie \n\nreviews using Gini-index feature selection method and SVM classifier. World Wide Web. 2017;20:135–54. https ://doi.\norg/10.1007/s1128 0-015-0381-x.\n\n 34. Fan RE, Chang K-W, Hsieh C-J, Wang X-R, Lin C-J. LIBLINEAR: A library for large linear classification. J Mach Learn Res. \n2008;9:1871–4.\n\nhttps://doi.org/10.1007/978-3-540-68880-8_32\nhttps://doi.org/10.1007/s11280-015-0381-x\nhttp://dx.doi.org/10.1109/iccict.2012.6398180\nhttps://doi.org/10.1109/hpcc.and.euc.2013.106\nhttps://doi.org/10.1109/hpcc.and.euc.2013.106\nhttps://doi.org/10.1080/00207543.2015.1066519\nhttps://doi.org/10.1145/2939672.2939881\nhttps://doi.org/10.1371/journal.pone.0194889\nhttps://doi.org/10.18187/pjsor.v8i3.535\nhttps://doi.org/10.1007/s11280-015-0381-x\nhttps://doi.org/10.1007/s11280-015-0381-x\n\n\nPage 15 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\n 35. Ribeiro MT, Singh S, and Guestrin C. Why should I trust you?: Explaining the predictions of any classifier. In: Proc. \nACMSIGKDD Int. Conf. Knowl. Discov. Data Mining. 2016. p. 1135–44.\n\n 36. Luo X, et al. An effective scheme for QoS estimation via alternating direction method-based matrix factorization. \nIEEE Trans Serv Comput. 2019;12(4):503–18.\n\n 37. Liu CL, Hsaio WH, Lee CH, Lu GC and Jou E. Movie rating and review summarization in mobile environment. In: IEEE \ntrans. systems, man and cybernetics, Part C: applications and reviews. 2012. p. 397–407.\n\n 38. Vapnik, VN. The nature of statistical learning theory, Springer, 2nd ed, 1999. Translated by Xu Jianghua, Zhang Xue-\ngong. Beijing: China Machine Press; 2000.\n\n 39. [Dataset] Flipkart-products. http://www.kaggl e.com/Promp tClou dHQ/flipk art-produ cts.\n 40. [Dataset] https ://snap.stanf ord.edu/data/web-Amazo n.html.\n 41. [Dataset] He R, McAuley J. Ups and downs: modeling the visual evolution of fashion trends with one-class collabora-\n\ntive filtering. WWW; 2016.\n 42. Popescu AM, Etzioni O. Extracting product features and opinions from reviews. 2005; EMNLP.\n 43. Zaharia M, Chowdhury M, Das T, Dave A, Ma J, McCauley M, Franklin M, Shenker S, Stoica I. Resilient distributed \n\ndatasets: A fault-tolerant abstraction for in-memory cluster computing Technical Report UCB/EECS-2011-82. UC \nBerkeley: EECS Department; 2011.\n\n 44. Davis J, Goadrich M. The relationship between precision-recall and ROC curves, In ICML. 2006. p. 233–40.\n 45. Lee JS, Lee ES. Exploring the usefulness of predicting people’s locations. Procedia Soc Beh Sci. 2014. https ://doi.\n\norg/10.1016/j.sbspr o.2014.04.451.\n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nhttp://www.kaggle.com/PromptCloudHQ/flipkart-products\nhttps://snap.stanford.edu/data/web-Amazon.html\nhttps://doi.org/10.1016/j.sbspro.2014.04.451\nhttps://doi.org/10.1016/j.sbspro.2014.04.451\n\n\tImproving prediction with enhanced Distributed Memory-based Resilient Dataset Filter\n\tAbstract \n\tIntroduction\n\tRelated work\n\tMethodology\n\tData collection phase\n\tDataset pre-processing\n\tResilient Distributed Dataset\n\n\tPrediction classifiers\n\tLogistic regression (LR)\n\tSupport Vector Machine (SVM)\n\n\tExperimental setup\n\n\tResults and discussions\n\tConclusion and future work\n\tAcknowledgements\n\tReferences\n\n\n\n\n",
      "text": [
        "",
        "Published online: 28 February 2020"
      ],
      "layoutText": [
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"Published online: 28 February 2020\",\"lines\":[{\"boundingBox\":[{\"x\":4,\"y\":16},{\"x\":1018,\"y\":18},{\"x\":1018,\"y\":73},{\"x\":4,\"y\":70}],\"text\":\"Published online: 28 February 2020\"}],\"words\":[{\"boundingBox\":[{\"x\":5,\"y\":16},{\"x\":269,\"y\":17},{\"x\":270,\"y\":70},{\"x\":5,\"y\":70}],\"text\":\"Published\"},{\"boundingBox\":[{\"x\":291,\"y\":17},{\"x\":498,\"y\":17},{\"x\":498,\"y\":71},{\"x\":292,\"y\":70}],\"text\":\"online:\"},{\"boundingBox\":[{\"x\":508,\"y\":17},{\"x\":575,\"y\":17},{\"x\":575,\"y\":71},{\"x\":508,\"y\":71}],\"text\":\"28\"},{\"boundingBox\":[{\"x\":593,\"y\":17},{\"x\":852,\"y\":18},{\"x\":852,\"y\":73},{\"x\":594,\"y\":71}],\"text\":\"February\"},{\"boundingBox\":[{\"x\":871,\"y\":18},{\"x\":1008,\"y\":18},{\"x\":1008,\"y\":74},{\"x\":871,\"y\":73}],\"text\":\"2020\"}]}"
      ],
      "pii_entities": [
        {
          "text": "Sandhya Narayanan1",
          "type": "Person",
          "subtype": null,
          "offset": 88,
          "length": 18,
          "score": 0.99
        },
        {
          "text": "Philip Samuel2",
          "type": "Person",
          "subtype": null,
          "offset": 109,
          "length": 14,
          "score": 0.99
        },
        {
          "text": "Mariamma Chacko3",
          "type": "Person",
          "subtype": null,
          "offset": 128,
          "length": 16,
          "score": 0.99
        },
        {
          "text": "mar",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 769,
          "length": 3,
          "score": 0.8
        },
        {
          "text": "customers",
          "type": "PersonType",
          "subtype": null,
          "offset": 1010,
          "length": 9,
          "score": 0.95
        },
        {
          "text": "customers",
          "type": "PersonType",
          "subtype": null,
          "offset": 1060,
          "length": 9,
          "score": 0.67
        },
        {
          "text": "dancy",
          "type": "Person",
          "subtype": null,
          "offset": 2150,
          "length": 5,
          "score": 0.57
        },
        {
          "text": "manufacturer",
          "type": "PersonType",
          "subtype": null,
          "offset": 2408,
          "length": 12,
          "score": 0.88
        },
        {
          "text": "The Author",
          "type": "Organization",
          "subtype": null,
          "offset": 2560,
          "length": 10,
          "score": 0.66
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 2574,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "author",
          "type": "PersonType",
          "subtype": null,
          "offset": 2820,
          "length": 6,
          "score": 0.92
        },
        {
          "text": "copyright holder",
          "type": "PersonType",
          "subtype": null,
          "offset": 3323,
          "length": 16,
          "score": 0.66
        },
        {
          "text": "Narayanan",
          "type": "Person",
          "subtype": null,
          "offset": 3440,
          "length": 9,
          "score": 0.98
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 3480,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "7:13",
          "type": "DateTime",
          "subtype": "Time",
          "offset": 3486,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "https://doi.org/10.1186/s40537",
          "type": "URL",
          "subtype": null,
          "offset": 3493,
          "length": 30,
          "score": 0.8
        },
        {
          "text": "nairsands@gmail.com",
          "type": "Email",
          "subtype": null,
          "offset": 3557,
          "length": 19,
          "score": 0.8
        },
        {
          "text": "School of Engineering",
          "type": "Organization",
          "subtype": null,
          "offset": 3605,
          "length": 21,
          "score": 0.65
        },
        {
          "text": "Cochin University of Science",
          "type": "Organization",
          "subtype": null,
          "offset": 3629,
          "length": 28,
          "score": 0.91
        },
        {
          "text": "http://creativecommons.org/licenses/by/4.0/",
          "type": "URL",
          "subtype": null,
          "offset": 3768,
          "length": 43,
          "score": 0.8
        },
        {
          "text": "http://creativecommons.org/licenses/by/4.0/",
          "type": "URL",
          "subtype": null,
          "offset": 3812,
          "length": 43,
          "score": 0.8
        },
        {
          "text": "http://crossmark.crossref.org/dialog/?doi=10.1186/s40537-020-00292-y&domain=pdf",
          "type": "URL",
          "subtype": null,
          "offset": 3856,
          "length": 79,
          "score": 0.8
        },
        {
          "text": "15Narayanan",
          "type": "Person",
          "subtype": null,
          "offset": 3948,
          "length": 11,
          "score": 0.88
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 3990,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "7:13",
          "type": "DateTime",
          "subtype": "Time",
          "offset": 3996,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "owners",
          "type": "PersonType",
          "subtype": null,
          "offset": 4025,
          "length": 6,
          "score": 0.53
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 4461,
          "length": 5,
          "score": 0.89
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 4916,
          "length": 5,
          "score": 0.93
        },
        {
          "text": "customer",
          "type": "PersonType",
          "subtype": null,
          "offset": 6524,
          "length": 8,
          "score": 0.5
        },
        {
          "text": "Resilient",
          "type": "Organization",
          "subtype": null,
          "offset": 6763,
          "length": 9,
          "score": 0.53
        },
        {
          "text": "Makridakis",
          "type": "Person",
          "subtype": null,
          "offset": 7209,
          "length": 10,
          "score": 0.96
        },
        {
          "text": "15Narayanan",
          "type": "Person",
          "subtype": null,
          "offset": 7574,
          "length": 11,
          "score": 0.87
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 7616,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "7:13",
          "type": "DateTime",
          "subtype": "Time",
          "offset": 7622,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Hao",
          "type": "Person",
          "subtype": null,
          "offset": 7752,
          "length": 3,
          "score": 0.92
        },
        {
          "text": "Salakhutdinov",
          "type": "Person",
          "subtype": null,
          "offset": 8074,
          "length": 13,
          "score": 0.97
        },
        {
          "text": "al",
          "type": "Person",
          "subtype": null,
          "offset": 8091,
          "length": 2,
          "score": 0.52
        },
        {
          "text": "customers",
          "type": "PersonType",
          "subtype": null,
          "offset": 8570,
          "length": 9,
          "score": 0.92
        },
        {
          "text": "Wietsma",
          "type": "Person",
          "subtype": null,
          "offset": 8930,
          "length": 7,
          "score": 0.95
        },
        {
          "text": "student",
          "type": "PersonType",
          "subtype": null,
          "offset": 9116,
          "length": 7,
          "score": 0.9
        },
        {
          "text": "user",
          "type": "PersonType",
          "subtype": null,
          "offset": 9124,
          "length": 4,
          "score": 0.55
        },
        {
          "text": "Jianguo Chen",
          "type": "Person",
          "subtype": null,
          "offset": 9241,
          "length": 12,
          "score": 0.98
        },
        {
          "text": "Asha",
          "type": "Person",
          "subtype": null,
          "offset": 9512,
          "length": 4,
          "score": 0.95
        },
        {
          "text": "Luo",
          "type": "Person",
          "subtype": null,
          "offset": 10207,
          "length": 3,
          "score": 0.96
        },
        {
          "text": "Liu",
          "type": "Person",
          "subtype": null,
          "offset": 10334,
          "length": 3,
          "score": 0.96
        },
        {
          "text": "15Narayanan",
          "type": "Person",
          "subtype": null,
          "offset": 11468,
          "length": 11,
          "score": 0.89
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 11510,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "7:13",
          "type": "DateTime",
          "subtype": "Time",
          "offset": 11516,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "customer",
          "type": "PersonType",
          "subtype": null,
          "offset": 11819,
          "length": 8,
          "score": 0.53
        },
        {
          "text": "24 months",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 12998,
          "length": 9,
          "score": 0.8
        },
        {
          "text": "user",
          "type": "PersonType",
          "subtype": null,
          "offset": 13482,
          "length": 4,
          "score": 0.91
        },
        {
          "text": "15Narayanan",
          "type": "Person",
          "subtype": null,
          "offset": 14098,
          "length": 11,
          "score": 0.9
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 14140,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "7:13",
          "type": "DateTime",
          "subtype": "Time",
          "offset": 14146,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "user",
          "type": "PersonType",
          "subtype": null,
          "offset": 14252,
          "length": 4,
          "score": 0.5
        },
        {
          "text": "customer",
          "type": "PersonType",
          "subtype": null,
          "offset": 15687,
          "length": 8,
          "score": 0.75
        },
        {
          "text": "15Narayanan",
          "type": "Person",
          "subtype": null,
          "offset": 15877,
          "length": 11,
          "score": 0.91
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 15919,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "7:13",
          "type": "DateTime",
          "subtype": "Time",
          "offset": 15925,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "15Narayanan",
          "type": "Person",
          "subtype": null,
          "offset": 17567,
          "length": 11,
          "score": 0.9
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 17609,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "7:13",
          "type": "DateTime",
          "subtype": "Time",
          "offset": 17615,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "RDD",
          "type": "Organization",
          "subtype": null,
          "offset": 17952,
          "length": 3,
          "score": 0.85
        },
        {
          "text": "DMRDF",
          "type": "Organization",
          "subtype": null,
          "offset": 19258,
          "length": 5,
          "score": 0.74
        },
        {
          "text": "customer",
          "type": "PersonType",
          "subtype": null,
          "offset": 19793,
          "length": 8,
          "score": 0.63
        },
        {
          "text": "15Narayanan",
          "type": "Person",
          "subtype": null,
          "offset": 20447,
          "length": 11,
          "score": 0.91
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 20489,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "7:13",
          "type": "DateTime",
          "subtype": "Time",
          "offset": 20495,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "customers",
          "type": "PersonType",
          "subtype": null,
          "offset": 20502,
          "length": 9,
          "score": 0.81
        },
        {
          "text": "customer",
          "type": "PersonType",
          "subtype": null,
          "offset": 20735,
          "length": 8,
          "score": 0.68
        },
        {
          "text": "customer",
          "type": "PersonType",
          "subtype": null,
          "offset": 21699,
          "length": 8,
          "score": 0.54
        },
        {
          "text": "customer",
          "type": "PersonType",
          "subtype": null,
          "offset": 21771,
          "length": 8,
          "score": 0.65
        },
        {
          "text": "KRi",
          "type": "Organization",
          "subtype": null,
          "offset": 22306,
          "length": 3,
          "score": 0.5
        },
        {
          "text": "15Narayanan",
          "type": "Person",
          "subtype": null,
          "offset": 22457,
          "length": 11,
          "score": 0.91
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 22499,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "7:13",
          "type": "DateTime",
          "subtype": "Time",
          "offset": 22505,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "SVM",
          "type": "Organization",
          "subtype": "Sports",
          "offset": 22760,
          "length": 3,
          "score": 0.69
        },
        {
          "text": "15Narayanan",
          "type": "Person",
          "subtype": null,
          "offset": 24669,
          "length": 11,
          "score": 0.87
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 24711,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "7:13",
          "type": "DateTime",
          "subtype": "Time",
          "offset": 24717,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "customers",
          "type": "PersonType",
          "subtype": null,
          "offset": 25686,
          "length": 9,
          "score": 0.97
        },
        {
          "text": "customers",
          "type": "PersonType",
          "subtype": null,
          "offset": 25736,
          "length": 9,
          "score": 0.96
        },
        {
          "text": "customers",
          "type": "PersonType",
          "subtype": null,
          "offset": 25845,
          "length": 9,
          "score": 0.97
        },
        {
          "text": "DMRDF",
          "type": "Organization",
          "subtype": null,
          "offset": 26514,
          "length": 5,
          "score": 0.56
        },
        {
          "text": "740 s",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 26936,
          "length": 5,
          "score": 0.8
        },
        {
          "text": "910 s",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 26943,
          "length": 5,
          "score": 0.8
        },
        {
          "text": "Customer",
          "type": "PersonType",
          "subtype": null,
          "offset": 27362,
          "length": 8,
          "score": 0.66
        },
        {
          "text": "15Narayanan",
          "type": "Person",
          "subtype": null,
          "offset": 27405,
          "length": 11,
          "score": 0.9
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 27447,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "7:13",
          "type": "DateTime",
          "subtype": "Time",
          "offset": 27453,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "24 months",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 28035,
          "length": 9,
          "score": 0.8
        },
        {
          "text": "TN",
          "type": "Organization",
          "subtype": null,
          "offset": 28539,
          "length": 2,
          "score": 0.76
        },
        {
          "text": "FP",
          "type": "Organization",
          "subtype": null,
          "offset": 28544,
          "length": 2,
          "score": 0.71
        },
        {
          "text": "FN",
          "type": "Organization",
          "subtype": null,
          "offset": 28549,
          "length": 2,
          "score": 0.73
        },
        {
          "text": "DMRDF",
          "type": "Organization",
          "subtype": null,
          "offset": 28642,
          "length": 5,
          "score": 0.73
        },
        {
          "text": "DMRDF",
          "type": "Organization",
          "subtype": null,
          "offset": 28930,
          "length": 5,
          "score": 0.9
        },
        {
          "text": "DMRDF",
          "type": "Organization",
          "subtype": null,
          "offset": 29068,
          "length": 5,
          "score": 0.83
        },
        {
          "text": "15Narayanan",
          "type": "Person",
          "subtype": null,
          "offset": 29158,
          "length": 11,
          "score": 0.9
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 29200,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "7:13",
          "type": "DateTime",
          "subtype": "Time",
          "offset": 29206,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "DMRDF",
          "type": "Organization",
          "subtype": null,
          "offset": 29362,
          "length": 5,
          "score": 0.74
        },
        {
          "text": "24 months",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 29597,
          "length": 9,
          "score": 0.8
        },
        {
          "text": "may",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 29632,
          "length": 3,
          "score": 0.8
        },
        {
          "text": "customer",
          "type": "PersonType",
          "subtype": null,
          "offset": 29739,
          "length": 8,
          "score": 0.72
        },
        {
          "text": "FN",
          "type": "Organization",
          "subtype": null,
          "offset": 30382,
          "length": 2,
          "score": 0.65
        },
        {
          "text": "SVM",
          "type": "Organization",
          "subtype": "Sports",
          "offset": 30388,
          "length": 3,
          "score": 0.62
        },
        {
          "text": "SVM",
          "type": "Organization",
          "subtype": "Sports",
          "offset": 30394,
          "length": 3,
          "score": 0.67
        },
        {
          "text": "3 6 12 18 24",
          "type": "PhoneNumber",
          "subtype": null,
          "offset": 30433,
          "length": 12,
          "score": 0.8
        },
        {
          "text": "3 6 12 18 24",
          "type": "PhoneNumber",
          "subtype": null,
          "offset": 30532,
          "length": 12,
          "score": 0.8
        },
        {
          "text": "3 6 12 18 24",
          "type": "PhoneNumber",
          "subtype": null,
          "offset": 30676,
          "length": 12,
          "score": 0.8
        },
        {
          "text": "3 6 12 18 24",
          "type": "PhoneNumber",
          "subtype": null,
          "offset": 30775,
          "length": 12,
          "score": 0.8
        },
        {
          "text": "15Narayanan",
          "type": "Person",
          "subtype": null,
          "offset": 30955,
          "length": 11,
          "score": 0.87
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 30997,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "7:13",
          "type": "DateTime",
          "subtype": "Time",
          "offset": 31003,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "customer",
          "type": "PersonType",
          "subtype": null,
          "offset": 32444,
          "length": 8,
          "score": 0.71
        },
        {
          "text": "SN",
          "type": "Organization",
          "subtype": null,
          "offset": 33233,
          "length": 2,
          "score": 0.9
        },
        {
          "text": "MC",
          "type": "Person",
          "subtype": null,
          "offset": 33524,
          "length": 2,
          "score": 0.89
        },
        {
          "text": "contributor",
          "type": "PersonType",
          "subtype": null,
          "offset": 33539,
          "length": 11,
          "score": 0.6
        },
        {
          "text": "authors",
          "type": "PersonType",
          "subtype": null,
          "offset": 33583,
          "length": 7,
          "score": 0.97
        },
        {
          "text": "Kaggle",
          "type": "Organization",
          "subtype": null,
          "offset": 33777,
          "length": 6,
          "score": 0.55
        },
        {
          "text": "e.com/Promp",
          "type": "URL",
          "subtype": null,
          "offset": 33865,
          "length": 11,
          "score": 0.8
        },
        {
          "text": "authors",
          "type": "PersonType",
          "subtype": null,
          "offset": 33939,
          "length": 7,
          "score": 0.96
        },
        {
          "text": "of Engineering",
          "type": "Organization",
          "subtype": null,
          "offset": 34043,
          "length": 14,
          "score": 0.56
        },
        {
          "text": "Kochi 682022",
          "type": "Address",
          "subtype": null,
          "offset": 34102,
          "length": 12,
          "score": 0.79
        },
        {
          "text": "Cochin University of Science & Technology",
          "type": "Organization",
          "subtype": null,
          "offset": 34158,
          "length": 41,
          "score": 0.51
        },
        {
          "text": "Kochi 682022",
          "type": "Address",
          "subtype": null,
          "offset": 34201,
          "length": 12,
          "score": 0.84
        },
        {
          "text": "Cochin University of Science & Technology",
          "type": "Organization",
          "subtype": null,
          "offset": 34256,
          "length": 41,
          "score": 0.62
        },
        {
          "text": "Kochi 682022",
          "type": "Address",
          "subtype": null,
          "offset": 34299,
          "length": 12,
          "score": 0.82
        },
        {
          "text": "25 October 2019",
          "type": "DateTime",
          "subtype": "Date",
          "offset": 34332,
          "length": 15,
          "score": 0.8
        },
        {
          "text": "17 February 2020",
          "type": "DateTime",
          "subtype": "Date",
          "offset": 34360,
          "length": 16,
          "score": 0.8
        },
        {
          "text": "28 February 2020",
          "type": "DateTime",
          "subtype": "Date",
          "offset": 34397,
          "length": 16,
          "score": 0.8
        },
        {
          "text": "http://www.kaggle.com/PromptCloudHQ/flipkart-products",
          "type": "URL",
          "subtype": null,
          "offset": 34416,
          "length": 53,
          "score": 0.8
        },
        {
          "text": "15Narayanan",
          "type": "Person",
          "subtype": null,
          "offset": 34483,
          "length": 11,
          "score": 0.89
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 34525,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "7:13",
          "type": "DateTime",
          "subtype": "Time",
          "offset": 34531,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Lau RY",
          "type": "Person",
          "subtype": null,
          "offset": 34553,
          "length": 6,
          "score": 0.84
        },
        {
          "text": "Liao SY",
          "type": "Person",
          "subtype": null,
          "offset": 34561,
          "length": 7,
          "score": 0.94
        },
        {
          "text": "Kwok RC",
          "type": "Person",
          "subtype": null,
          "offset": 34570,
          "length": 7,
          "score": 0.93
        },
        {
          "text": "Xu K",
          "type": "Person",
          "subtype": null,
          "offset": 34579,
          "length": 4,
          "score": 0.94
        },
        {
          "text": "Xia Y",
          "type": "Person",
          "subtype": null,
          "offset": 34585,
          "length": 5,
          "score": 0.92
        },
        {
          "text": "Li Y",
          "type": "Person",
          "subtype": null,
          "offset": 34592,
          "length": 4,
          "score": 0.88
        },
        {
          "text": "2011",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 34702,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Lin X",
          "type": "Person",
          "subtype": null,
          "offset": 34720,
          "length": 5,
          "score": 0.98
        },
        {
          "text": "Li Y",
          "type": "Person",
          "subtype": null,
          "offset": 34727,
          "length": 4,
          "score": 0.98
        },
        {
          "text": "Wang X",
          "type": "Person",
          "subtype": null,
          "offset": 34733,
          "length": 6,
          "score": 0.98
        },
        {
          "text": "2017",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 34833,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Matos CAD",
          "type": "Person",
          "subtype": null,
          "offset": 34854,
          "length": 9,
          "score": 0.85
        },
        {
          "text": "Rossi CAV",
          "type": "Person",
          "subtype": null,
          "offset": 34865,
          "length": 9,
          "score": 0.71
        },
        {
          "text": "moderators",
          "type": "PersonType",
          "subtype": null,
          "offset": 34967,
          "length": 10,
          "score": 0.94
        },
        {
          "text": "2008",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 34998,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Jeon S",
          "type": "Person",
          "subtype": null,
          "offset": 35021,
          "length": 6,
          "score": 0.95
        },
        {
          "text": "2013",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 35135,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Dave K",
          "type": "Person",
          "subtype": null,
          "offset": 35156,
          "length": 6,
          "score": 0.98
        },
        {
          "text": "Lawrence S",
          "type": "Person",
          "subtype": null,
          "offset": 35164,
          "length": 10,
          "score": 0.96
        },
        {
          "text": "Pennock D.",
          "type": "Person",
          "subtype": null,
          "offset": 35180,
          "length": 10,
          "score": 0.83
        },
        {
          "text": "2003",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 35291,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Zhou Y",
          "type": "Person",
          "subtype": null,
          "offset": 35301,
          "length": 6,
          "score": 0.97
        },
        {
          "text": "Wilkinson D",
          "type": "Person",
          "subtype": null,
          "offset": 35309,
          "length": 11,
          "score": 0.95
        },
        {
          "text": "Schreiber R",
          "type": "Person",
          "subtype": null,
          "offset": 35322,
          "length": 11,
          "score": 0.86
        },
        {
          "text": "Pan R",
          "type": "Person",
          "subtype": null,
          "offset": 35335,
          "length": 5,
          "score": 0.89
        },
        {
          "text": "2008",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 35410,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Zhang KZK",
          "type": "Person",
          "subtype": null,
          "offset": 35481,
          "length": 9,
          "score": 0.97
        },
        {
          "text": "Benyoucef M",
          "type": "Person",
          "subtype": null,
          "offset": 35492,
          "length": 11,
          "score": 0.94
        },
        {
          "text": "Dec",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 35564,
          "length": 3,
          "score": 0.8
        },
        {
          "text": "2016",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 35584,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Cui Geng",
          "type": "Person",
          "subtype": null,
          "offset": 35604,
          "length": 8,
          "score": 0.99
        },
        {
          "text": "Lui Hon-Kwong",
          "type": "Person",
          "subtype": null,
          "offset": 35614,
          "length": 13,
          "score": 0.99
        },
        {
          "text": "Guo Xiaoning",
          "type": "Person",
          "subtype": null,
          "offset": 35629,
          "length": 12,
          "score": 0.99
        },
        {
          "text": "Int J Electron",
          "type": "Organization",
          "subtype": null,
          "offset": 35703,
          "length": 14,
          "score": 0.67
        },
        {
          "text": "2012",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 35726,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Manek AS",
          "type": "Person",
          "subtype": null,
          "offset": 35748,
          "length": 8,
          "score": 0.94
        },
        {
          "text": "Shenoy PD",
          "type": "Person",
          "subtype": null,
          "offset": 35758,
          "length": 9,
          "score": 0.95
        },
        {
          "text": "Mohan MC",
          "type": "Person",
          "subtype": null,
          "offset": 35769,
          "length": 8,
          "score": 0.92
        },
        {
          "text": "Int J Knowl Web Intell",
          "type": "Organization",
          "subtype": null,
          "offset": 35891,
          "length": 22,
          "score": 0.72
        },
        {
          "text": "2016",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 35915,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Singh S",
          "type": "Person",
          "subtype": null,
          "offset": 35983,
          "length": 7,
          "score": 0.95
        },
        {
          "text": "Singh N",
          "type": "Person",
          "subtype": null,
          "offset": 35996,
          "length": 7,
          "score": 0.9
        },
        {
          "text": "2012",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 36048,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "technology",
          "type": "Organization",
          "subtype": null,
          "offset": 36122,
          "length": 10,
          "score": 0.6
        },
        {
          "text": "ICCICT",
          "type": "Organization",
          "subtype": null,
          "offset": 36134,
          "length": 6,
          "score": 0.93
        },
        {
          "text": "institute of electrical and electronics engineers",
          "type": "Organization",
          "subtype": null,
          "offset": 36143,
          "length": 49,
          "score": 0.73
        },
        {
          "text": "IEEE",
          "type": "Organization",
          "subtype": null,
          "offset": 36194,
          "length": 4,
          "score": 0.96
        },
        {
          "text": "2012",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 36201,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "http://dx.doi.org/10.1109/iccic",
          "type": "URL",
          "subtype": null,
          "offset": 36216,
          "length": 31,
          "score": 0.8
        },
        {
          "text": "Demchenko Yuri",
          "type": "Person",
          "subtype": null,
          "offset": 36271,
          "length": 14,
          "score": 0.98
        },
        {
          "text": "IEEE",
          "type": "Organization",
          "subtype": null,
          "offset": 36364,
          "length": 4,
          "score": 0.86
        },
        {
          "text": "CloudCom",
          "type": "Organization",
          "subtype": null,
          "offset": 36430,
          "length": 8,
          "score": 0.58
        },
        {
          "text": "2012",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 36441,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Sihong Xie",
          "type": "Person",
          "subtype": null,
          "offset": 36453,
          "length": 10,
          "score": 0.99
        },
        {
          "text": "Guan Wang",
          "type": "Person",
          "subtype": null,
          "offset": 36465,
          "length": 9,
          "score": 0.99
        },
        {
          "text": "Shuyang Lin",
          "type": "Person",
          "subtype": null,
          "offset": 36476,
          "length": 11,
          "score": 0.99
        },
        {
          "text": "Yu Philip S",
          "type": "Person",
          "subtype": null,
          "offset": 36492,
          "length": 11,
          "score": 0.98
        },
        {
          "text": "ACM",
          "type": "Organization",
          "subtype": null,
          "offset": 36567,
          "length": 3,
          "score": 0.63
        },
        {
          "text": "2012",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 36649,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Koren Y",
          "type": "Person",
          "subtype": null,
          "offset": 36671,
          "length": 7,
          "score": 0.97
        },
        {
          "text": "Bell R",
          "type": "Person",
          "subtype": null,
          "offset": 36680,
          "length": 6,
          "score": 0.95
        },
        {
          "text": "Volinsky C",
          "type": "Person",
          "subtype": null,
          "offset": 36688,
          "length": 10,
          "score": 0.93
        },
        {
          "text": "2009",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 36766,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "8:30–7",
          "type": "DateTime",
          "subtype": "TimeRange",
          "offset": 36771,
          "length": 6,
          "score": 0.8
        },
        {
          "text": "Salakhutdinov R",
          "type": "Person",
          "subtype": null,
          "offset": 36784,
          "length": 15,
          "score": 0.95
        },
        {
          "text": "Mnih A",
          "type": "Person",
          "subtype": null,
          "offset": 36801,
          "length": 6,
          "score": 0.95
        },
        {
          "text": "Hinton G",
          "type": "Person",
          "subtype": null,
          "offset": 36811,
          "length": 8,
          "score": 0.95
        },
        {
          "text": "2007",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 36941,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Hao MA",
          "type": "Person",
          "subtype": null,
          "offset": 36962,
          "length": 6,
          "score": 0.87
        },
        {
          "text": "King I",
          "type": "Person",
          "subtype": null,
          "offset": 36970,
          "length": 6,
          "score": 0.85
        },
        {
          "text": "Lyu",
          "type": "Person",
          "subtype": null,
          "offset": 36978,
          "length": 3,
          "score": 0.94
        },
        {
          "text": "ACM",
          "type": "Organization",
          "subtype": null,
          "offset": 37053,
          "length": 3,
          "score": 0.56
        },
        {
          "text": "2011",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 37086,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Bandakkanavar V",
          "type": "Person",
          "subtype": null,
          "offset": 37105,
          "length": 15,
          "score": 0.98
        },
        {
          "text": "Ramesh M",
          "type": "Person",
          "subtype": null,
          "offset": 37122,
          "length": 8,
          "score": 0.97
        },
        {
          "text": "Geeta V.",
          "type": "Person",
          "subtype": null,
          "offset": 37132,
          "length": 8,
          "score": 0.83
        },
        {
          "text": "IJRITCC",
          "type": "Organization",
          "subtype": null,
          "offset": 37219,
          "length": 7,
          "score": 0.77
        },
        {
          "text": "2014",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 37228,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Gu V",
          "type": "Person",
          "subtype": null,
          "offset": 37250,
          "length": 4,
          "score": 0.97
        },
        {
          "text": "Li H",
          "type": "Person",
          "subtype": null,
          "offset": 37260,
          "length": 4,
          "score": 0.97
        },
        {
          "text": "2013",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 37368,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "IEEE",
          "type": "Organization",
          "subtype": null,
          "offset": 37373,
          "length": 4,
          "score": 0.64
        },
        {
          "text": "2013",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 37443,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Zhang Hanpeng",
          "type": "Person",
          "subtype": null,
          "offset": 37504,
          "length": 13,
          "score": 0.99
        },
        {
          "text": "Wang Zhaohua",
          "type": "Person",
          "subtype": null,
          "offset": 37519,
          "length": 12,
          "score": 0.99
        },
        {
          "text": "Chen Shengjun",
          "type": "Person",
          "subtype": null,
          "offset": 37533,
          "length": 13,
          "score": 0.99
        },
        {
          "text": "Guo Chengqi",
          "type": "Person",
          "subtype": null,
          "offset": 37548,
          "length": 11,
          "score": 0.99
        },
        {
          "text": "mediator",
          "type": "PersonType",
          "subtype": null,
          "offset": 37666,
          "length": 8,
          "score": 0.73
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 37692,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Ghose A",
          "type": "Person",
          "subtype": null,
          "offset": 37717,
          "length": 7,
          "score": 0.95
        },
        {
          "text": "Ipeirotis PG",
          "type": "Person",
          "subtype": null,
          "offset": 37726,
          "length": 12,
          "score": 0.89
        },
        {
          "text": "2007",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 37868,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Chong AY",
          "type": "Person",
          "subtype": null,
          "offset": 37891,
          "length": 8,
          "score": 0.97
        },
        {
          "text": "Ch’ng E",
          "type": "Person",
          "subtype": null,
          "offset": 37901,
          "length": 7,
          "score": 0.98
        },
        {
          "text": "Liu MJ",
          "type": "Person",
          "subtype": null,
          "offset": 37910,
          "length": 6,
          "score": 0.95
        },
        {
          "text": "Li B",
          "type": "Person",
          "subtype": null,
          "offset": 37918,
          "length": 4,
          "score": 0.95
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 38053,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Yang H",
          "type": "Person",
          "subtype": null,
          "offset": 38123,
          "length": 6,
          "score": 0.97
        },
        {
          "text": "Fujimaki R",
          "type": "Person",
          "subtype": null,
          "offset": 38131,
          "length": 10,
          "score": 0.97
        },
        {
          "text": "Kusumura Y",
          "type": "Person",
          "subtype": null,
          "offset": 38143,
          "length": 10,
          "score": 0.97
        },
        {
          "text": "Liu J.",
          "type": "Person",
          "subtype": null,
          "offset": 38157,
          "length": 6,
          "score": 0.88
        },
        {
          "text": "2016",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 38258,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Breese JS",
          "type": "Person",
          "subtype": null,
          "offset": 38314,
          "length": 9,
          "score": 0.95
        },
        {
          "text": "Heckerman D",
          "type": "Person",
          "subtype": null,
          "offset": 38325,
          "length": 11,
          "score": 0.94
        },
        {
          "text": "Kadie C",
          "type": "Person",
          "subtype": null,
          "offset": 38342,
          "length": 7,
          "score": 0.96
        },
        {
          "text": "1998",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 38495,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Mukherjee A",
          "type": "Person",
          "subtype": null,
          "offset": 38507,
          "length": 11,
          "score": 0.97
        },
        {
          "text": "Kumar A",
          "type": "Person",
          "subtype": null,
          "offset": 38520,
          "length": 7,
          "score": 0.95
        },
        {
          "text": "Liu B",
          "type": "Person",
          "subtype": null,
          "offset": 38529,
          "length": 5,
          "score": 0.95
        },
        {
          "text": "Wang J",
          "type": "Person",
          "subtype": null,
          "offset": 38536,
          "length": 6,
          "score": 0.95
        },
        {
          "text": "Hsu M",
          "type": "Person",
          "subtype": null,
          "offset": 38544,
          "length": 5,
          "score": 0.95
        },
        {
          "text": "Castellanos M",
          "type": "Person",
          "subtype": null,
          "offset": 38551,
          "length": 13,
          "score": 0.92
        },
        {
          "text": "Ghosh R.",
          "type": "Person",
          "subtype": null,
          "offset": 38566,
          "length": 8,
          "score": 0.81
        },
        {
          "text": "ACM",
          "type": "Organization",
          "subtype": null,
          "offset": 38738,
          "length": 3,
          "score": 0.8
        },
        {
          "text": "2013",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 38743,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Makridakis S",
          "type": "Person",
          "subtype": null,
          "offset": 38766,
          "length": 12,
          "score": 0.96
        },
        {
          "text": "Spiliotis E",
          "type": "Person",
          "subtype": null,
          "offset": 38780,
          "length": 11,
          "score": 0.94
        },
        {
          "text": "Assimakopoulos V",
          "type": "Person",
          "subtype": null,
          "offset": 38793,
          "length": 16,
          "score": 0.93
        },
        {
          "text": "ONE",
          "type": "Organization",
          "subtype": "Sports",
          "offset": 38898,
          "length": 3,
          "score": 0.56
        },
        {
          "text": "2018",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 38903,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Imon A",
          "type": "Person",
          "subtype": null,
          "offset": 38979,
          "length": 6,
          "score": 0.97
        },
        {
          "text": "Roy C",
          "type": "Person",
          "subtype": null,
          "offset": 38987,
          "length": 5,
          "score": 0.96
        },
        {
          "text": "Manos C",
          "type": "Person",
          "subtype": null,
          "offset": 38994,
          "length": 7,
          "score": 0.96
        },
        {
          "text": "Bhattacharjee S.",
          "type": "Person",
          "subtype": null,
          "offset": 39003,
          "length": 16,
          "score": 0.85
        },
        {
          "text": "2012",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 39091,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Chen T",
          "type": "Person",
          "subtype": null,
          "offset": 39148,
          "length": 6,
          "score": 0.97
        },
        {
          "text": "Zhang W",
          "type": "Person",
          "subtype": null,
          "offset": 39156,
          "length": 7,
          "score": 0.96
        },
        {
          "text": "Lu Q",
          "type": "Person",
          "subtype": null,
          "offset": 39165,
          "length": 4,
          "score": 0.96
        },
        {
          "text": "Chen K",
          "type": "Person",
          "subtype": null,
          "offset": 39171,
          "length": 6,
          "score": 0.97
        },
        {
          "text": "Zheng Z",
          "type": "Person",
          "subtype": null,
          "offset": 39179,
          "length": 7,
          "score": 0.97
        },
        {
          "text": "Yu Y.",
          "type": "Person",
          "subtype": null,
          "offset": 39188,
          "length": 5,
          "score": 0.87
        },
        {
          "text": "J Mach",
          "type": "Person",
          "subtype": null,
          "offset": 39260,
          "length": 6,
          "score": 0.93
        },
        {
          "text": "2012",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 39279,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Shi Y",
          "type": "Person",
          "subtype": null,
          "offset": 39305,
          "length": 5,
          "score": 0.96
        },
        {
          "text": "Larson M",
          "type": "Person",
          "subtype": null,
          "offset": 39312,
          "length": 8,
          "score": 0.95
        },
        {
          "text": "Hanjalic A.",
          "type": "Person",
          "subtype": null,
          "offset": 39322,
          "length": 11,
          "score": 0.83
        },
        {
          "text": "ACM",
          "type": "Organization",
          "subtype": null,
          "offset": 39439,
          "length": 3,
          "score": 0.69
        },
        {
          "text": "2014",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 39456,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Shan H",
          "type": "Person",
          "subtype": null,
          "offset": 39476,
          "length": 6,
          "score": 0.96
        },
        {
          "text": "Banerjee A.",
          "type": "Person",
          "subtype": null,
          "offset": 39486,
          "length": 11,
          "score": 0.81
        },
        {
          "text": "ICDM",
          "type": "Organization",
          "subtype": null,
          "offset": 39592,
          "length": 4,
          "score": 0.53
        },
        {
          "text": "IEEE",
          "type": "Organization",
          "subtype": null,
          "offset": 39599,
          "length": 4,
          "score": 0.7
        },
        {
          "text": "2010",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 39635,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Salakhutdinov R",
          "type": "Person",
          "subtype": null,
          "offset": 39659,
          "length": 15,
          "score": 0.96
        },
        {
          "text": "Mnih A.",
          "type": "Person",
          "subtype": null,
          "offset": 39678,
          "length": 7,
          "score": 0.9
        },
        {
          "text": "2008",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 39822,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Crawford M",
          "type": "Person",
          "subtype": null,
          "offset": 39844,
          "length": 10,
          "score": 0.94
        },
        {
          "text": "Khoshgoftaar TM",
          "type": "Person",
          "subtype": null,
          "offset": 39856,
          "length": 15,
          "score": 0.87
        },
        {
          "text": "Prusa JD",
          "type": "Person",
          "subtype": null,
          "offset": 39873,
          "length": 8,
          "score": 0.93
        },
        {
          "text": "Richter AN",
          "type": "Person",
          "subtype": null,
          "offset": 39883,
          "length": 10,
          "score": 0.84
        },
        {
          "text": "Al Najada H",
          "type": "Person",
          "subtype": null,
          "offset": 39895,
          "length": 11,
          "score": 0.9
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 39988,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Wietsma TA",
          "type": "Person",
          "subtype": null,
          "offset": 40008,
          "length": 10,
          "score": 0.86
        },
        {
          "text": "Ricci F",
          "type": "Person",
          "subtype": null,
          "offset": 40020,
          "length": 7,
          "score": 0.87
        },
        {
          "text": "Francesco",
          "type": "Person",
          "subtype": null,
          "offset": 40077,
          "length": 9,
          "score": 0.99
        },
        {
          "text": "2005",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 40096,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Jianguo C",
          "type": "Person",
          "subtype": null,
          "offset": 40116,
          "length": 9,
          "score": 0.94
        },
        {
          "text": "2018",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 40250,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Manek AS",
          "type": "Person",
          "subtype": null,
          "offset": 40272,
          "length": 8,
          "score": 0.89
        },
        {
          "text": "Shenoy PD",
          "type": "Person",
          "subtype": null,
          "offset": 40282,
          "length": 9,
          "score": 0.92
        },
        {
          "text": "Mohan MC",
          "type": "Person",
          "subtype": null,
          "offset": 40293,
          "length": 8,
          "score": 0.9
        },
        {
          "text": "Venugopal KR",
          "type": "Person",
          "subtype": null,
          "offset": 40303,
          "length": 12,
          "score": 0.9
        },
        {
          "text": "2017",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 40466,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Fan RE",
          "type": "Person",
          "subtype": null,
          "offset": 40534,
          "length": 6,
          "score": 0.96
        },
        {
          "text": "Chang K-W",
          "type": "Person",
          "subtype": null,
          "offset": 40542,
          "length": 9,
          "score": 0.97
        },
        {
          "text": "Hsieh C-J",
          "type": "Person",
          "subtype": null,
          "offset": 40553,
          "length": 9,
          "score": 0.97
        },
        {
          "text": "Wang X-R",
          "type": "Person",
          "subtype": null,
          "offset": 40564,
          "length": 8,
          "score": 0.96
        },
        {
          "text": "Lin C-J",
          "type": "Person",
          "subtype": null,
          "offset": 40574,
          "length": 7,
          "score": 0.95
        },
        {
          "text": "2008",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 40656,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "1871",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 40663,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "https://doi.org/10.1007/978-3-540-68880-8_32",
          "type": "URL",
          "subtype": null,
          "offset": 40672,
          "length": 44,
          "score": 0.8
        },
        {
          "text": "https://doi.org/10.1007/s11280-015-0381-x",
          "type": "URL",
          "subtype": null,
          "offset": 40717,
          "length": 41,
          "score": 0.8
        },
        {
          "text": "http://dx.doi.org/10.1109/iccict.2012.6398180",
          "type": "URL",
          "subtype": null,
          "offset": 40759,
          "length": 45,
          "score": 0.8
        },
        {
          "text": "https://doi.org/10.1109/",
          "type": "URL",
          "subtype": null,
          "offset": 40805,
          "length": 24,
          "score": 0.8
        },
        {
          "text": "https://doi.org/10.1109/hpcc.and.euc.2013.106",
          "type": "URL",
          "subtype": null,
          "offset": 40851,
          "length": 45,
          "score": 0.8
        },
        {
          "text": "https://doi.org/10.1080/00207543.2015.1066519",
          "type": "URL",
          "subtype": null,
          "offset": 40897,
          "length": 45,
          "score": 0.8
        },
        {
          "text": "1066519",
          "type": "DateTime",
          "subtype": "Date",
          "offset": 40935,
          "length": 7,
          "score": 0.8
        },
        {
          "text": "https://doi.org/10.1145/2939672.2939881",
          "type": "URL",
          "subtype": null,
          "offset": 40943,
          "length": 39,
          "score": 0.8
        },
        {
          "text": "https://doi.org/10.",
          "type": "URL",
          "subtype": null,
          "offset": 40983,
          "length": 19,
          "score": 0.8
        },
        {
          "text": "https://doi.org/10.18187/pjsor.v8i3.535",
          "type": "URL",
          "subtype": null,
          "offset": 41028,
          "length": 39,
          "score": 0.8
        },
        {
          "text": "https://doi.org/10.1007/s11280-015-0381-x",
          "type": "URL",
          "subtype": null,
          "offset": 41068,
          "length": 41,
          "score": 0.8
        },
        {
          "text": "https://doi.org/10.1007/s11280-015-0381-x",
          "type": "URL",
          "subtype": null,
          "offset": 41110,
          "length": 41,
          "score": 0.8
        },
        {
          "text": "15Narayanan",
          "type": "Person",
          "subtype": null,
          "offset": 41165,
          "length": 11,
          "score": 0.86
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 41207,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "7:13",
          "type": "DateTime",
          "subtype": "Time",
          "offset": 41213,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Ribeiro MT",
          "type": "Person",
          "subtype": null,
          "offset": 41226,
          "length": 10,
          "score": 0.89
        },
        {
          "text": "Singh S",
          "type": "Person",
          "subtype": null,
          "offset": 41238,
          "length": 7,
          "score": 0.92
        },
        {
          "text": "Guestrin C.",
          "type": "Person",
          "subtype": null,
          "offset": 41251,
          "length": 11,
          "score": 0.82
        },
        {
          "text": "ACMSIGKDD Int",
          "type": "Organization",
          "subtype": null,
          "offset": 41345,
          "length": 13,
          "score": 0.6
        },
        {
          "text": "Conf. Knowl",
          "type": "Organization",
          "subtype": null,
          "offset": 41360,
          "length": 11,
          "score": 0.62
        },
        {
          "text": "2016",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 41394,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Luo X",
          "type": "Person",
          "subtype": null,
          "offset": 41418,
          "length": 5,
          "score": 0.97
        },
        {
          "text": "IEEE",
          "type": "Organization",
          "subtype": null,
          "offset": 41533,
          "length": 4,
          "score": 0.5
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 41557,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Liu CL",
          "type": "Person",
          "subtype": null,
          "offset": 41582,
          "length": 6,
          "score": 0.93
        },
        {
          "text": "Hsaio WH",
          "type": "Person",
          "subtype": null,
          "offset": 41590,
          "length": 8,
          "score": 0.93
        },
        {
          "text": "Lee CH",
          "type": "Person",
          "subtype": null,
          "offset": 41600,
          "length": 6,
          "score": 0.91
        },
        {
          "text": "Lu GC",
          "type": "Person",
          "subtype": null,
          "offset": 41608,
          "length": 5,
          "score": 0.94
        },
        {
          "text": "Jou E",
          "type": "Person",
          "subtype": null,
          "offset": 41618,
          "length": 5,
          "score": 0.94
        },
        {
          "text": "IEEE",
          "type": "Organization",
          "subtype": null,
          "offset": 41690,
          "length": 4,
          "score": 0.95
        },
        {
          "text": "2012",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 41767,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Vapnik",
          "type": "Person",
          "subtype": null,
          "offset": 41791,
          "length": 6,
          "score": 0.81
        },
        {
          "text": "Springer",
          "type": "Organization",
          "subtype": null,
          "offset": 41846,
          "length": 8,
          "score": 0.89
        },
        {
          "text": "1999",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 41864,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Xu Jianghua",
          "type": "Person",
          "subtype": null,
          "offset": 41884,
          "length": 11,
          "score": 0.99
        },
        {
          "text": "Zhang Xue-",
          "type": "Person",
          "subtype": null,
          "offset": 41897,
          "length": 10,
          "score": 0.98
        },
        {
          "text": "China Machine Press",
          "type": "Organization",
          "subtype": null,
          "offset": 41923,
          "length": 19,
          "score": 0.93
        },
        {
          "text": "2000",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 41944,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "e.com/Promp",
          "type": "URL",
          "subtype": null,
          "offset": 42002,
          "length": 11,
          "score": 0.8
        },
        {
          "text": "ord.edu/data/web-Amazo",
          "type": "URL",
          "subtype": null,
          "offset": 42080,
          "length": 22,
          "score": 0.8
        },
        {
          "text": "He R",
          "type": "Person",
          "subtype": null,
          "offset": 42126,
          "length": 4,
          "score": 0.72
        },
        {
          "text": "McAuley J.",
          "type": "Person",
          "subtype": null,
          "offset": 42132,
          "length": 10,
          "score": 0.79
        },
        {
          "text": "WWW",
          "type": "Organization",
          "subtype": null,
          "offset": 42249,
          "length": 3,
          "score": 0.51
        },
        {
          "text": "2016",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 42254,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Popescu AM",
          "type": "Person",
          "subtype": null,
          "offset": 42265,
          "length": 10,
          "score": 0.89
        },
        {
          "text": "Etzioni O",
          "type": "Person",
          "subtype": null,
          "offset": 42277,
          "length": 9,
          "score": 0.83
        },
        {
          "text": "2005",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 42343,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "EMNLP",
          "type": "Organization",
          "subtype": null,
          "offset": 42349,
          "length": 5,
          "score": 0.71
        },
        {
          "text": "Zaharia M",
          "type": "Person",
          "subtype": null,
          "offset": 42361,
          "length": 9,
          "score": 0.98
        },
        {
          "text": "Chowdhury M",
          "type": "Person",
          "subtype": null,
          "offset": 42372,
          "length": 11,
          "score": 0.96
        },
        {
          "text": "Das T",
          "type": "Person",
          "subtype": null,
          "offset": 42385,
          "length": 5,
          "score": 0.89
        },
        {
          "text": "Dave A",
          "type": "Person",
          "subtype": null,
          "offset": 42392,
          "length": 6,
          "score": 0.97
        },
        {
          "text": "Ma J",
          "type": "Person",
          "subtype": null,
          "offset": 42400,
          "length": 4,
          "score": 0.96
        },
        {
          "text": "McCauley M",
          "type": "Person",
          "subtype": null,
          "offset": 42406,
          "length": 10,
          "score": 0.96
        },
        {
          "text": "Franklin M",
          "type": "Person",
          "subtype": null,
          "offset": 42418,
          "length": 10,
          "score": 0.95
        },
        {
          "text": "Shenker S",
          "type": "Person",
          "subtype": null,
          "offset": 42430,
          "length": 9,
          "score": 0.93
        },
        {
          "text": "Stoica I.",
          "type": "Person",
          "subtype": null,
          "offset": 42441,
          "length": 9,
          "score": 0.79
        },
        {
          "text": "UC",
          "type": "Organization",
          "subtype": "Sports",
          "offset": 42581,
          "length": 2,
          "score": 0.68
        },
        {
          "text": "EECS Department",
          "type": "Organization",
          "subtype": null,
          "offset": 42595,
          "length": 15,
          "score": 0.7
        },
        {
          "text": "2011",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 42612,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Davis J",
          "type": "Person",
          "subtype": null,
          "offset": 42624,
          "length": 7,
          "score": 0.94
        },
        {
          "text": "Goadrich M.",
          "type": "Person",
          "subtype": null,
          "offset": 42633,
          "length": 11,
          "score": 0.84
        },
        {
          "text": "2006",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 42712,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Lee JS",
          "type": "Person",
          "subtype": null,
          "offset": 42734,
          "length": 6,
          "score": 0.94
        },
        {
          "text": "Lee ES",
          "type": "Person",
          "subtype": null,
          "offset": 42742,
          "length": 6,
          "score": 0.87
        },
        {
          "text": "2014",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 42831,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "2014.04",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 42874,
          "length": 7,
          "score": 0.8
        },
        {
          "text": "Springer Nature",
          "type": "Organization",
          "subtype": null,
          "offset": 42905,
          "length": 15,
          "score": 0.94
        },
        {
          "text": "http://www.kaggle.com/PromptCloudHQ/flipkart-products",
          "type": "URL",
          "subtype": null,
          "offset": 43025,
          "length": 53,
          "score": 0.8
        },
        {
          "text": "https://snap.stanford.edu/data/web-Amazon.html",
          "type": "URL",
          "subtype": null,
          "offset": 43079,
          "length": 46,
          "score": 0.8
        },
        {
          "text": "https://doi.org/10.1016/j.sbspro.2014.04.451",
          "type": "URL",
          "subtype": null,
          "offset": 43126,
          "length": 44,
          "score": 0.8
        },
        {
          "text": "2014.04",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 43159,
          "length": 7,
          "score": 0.8
        },
        {
          "text": "https://doi.org/10.1016/j.sbspro.2014.04.",
          "type": "URL",
          "subtype": null,
          "offset": 43171,
          "length": 41,
          "score": 0.8
        },
        {
          "text": "2014.04",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 43204,
          "length": 7,
          "score": 0.8
        },
        {
          "text": "SVM",
          "type": "Organization",
          "subtype": "Sports",
          "offset": 43509,
          "length": 3,
          "score": 0.69
        }
      ]
    },
    {
      "@search.score": 6.6324844,
      "content": "\nDetecting problematic transactions \nin a consumer‑to‑consumer e‑commerce \nnetwork\nShun Kodate1,2, Ryusuke Chiba3, Shunya Kimura3 and Naoki Masuda2,4,5* \n\nIntroduction\nIn tandem with the rapid growth of online and electronic transactions and communi-\ncations, fraud is expanding at a dramatic speed and penetrates our daily lives. Fraud \nincluding cybercrimes costs billions of dollars per year and threatens the security of our \nsociety (UK Parliament 2017; McAfee 2019). In particular, in the recent era where online \nactivity dominates, attacking a system is not too costly, whereas defending the system \nagainst fraud is costly (Anderson et al. 2013). The dimension of fraud is vast and ranges \nfrom credit card fraud, money laundering, computer intrusion, to plagiarism, to name a \nfew.\n\nAbstract \n\nProviders of online marketplaces are constantly combatting against problematic \ntransactions, such as selling illegal items and posting fictive items, exercised by some \nof their users. A typical approach to detect fraud activity has been to analyze registered \nuser profiles, user’s behavior, and texts attached to individual transactions and the user. \nHowever, this traditional approach may be limited because malicious users can easily \nconceal their information. Given this background, network indices have been exploited \nfor detecting frauds in various online transaction platforms. In the present study, we \nanalyzed networks of users of an online consumer-to-consumer marketplace in which \na seller and the corresponding buyer of a transaction are connected by a directed \nedge. We constructed egocentric networks of each of several hundreds of fraudulent \nusers and those of a similar number of normal users. We calculated eight local network \nindices based on up to connectivity between the neighbors of the focal node. Based \non the present descriptive analysis of these network indices, we fed twelve features \nthat we constructed from the eight network indices to random forest classifiers with \nthe aim of distinguishing between normal users and fraudulent users engaged in each \none of the four types of problematic transactions. We found that the classifier accu-\nrately distinguished the fraudulent users from normal users and that the classification \nperformance did not depend on the type of problematic transaction.\n\nKeywords: Network analysis, Machine learning, Fraud detection, Computational social \nscience\n\nOpen Access\n\n© The Author(s) 2020. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://\ncreat iveco mmons .org/licen ses/by/4.0/.\n\nRESEARCH\n\nKodate et al. Appl Netw Sci            (2020) 5:90  \nhttps://doi.org/10.1007/s41109‑020‑00330‑x Applied Network Science\n\n*Correspondence:   \nnaokimas@buffalo.edu \n4 Department \nof Mathematics, University \nat Buffalo, Buffalo, NY \n14260-2900, USA\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0003-1567-801X\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1007/s41109-020-00330-x&domain=pdf\n\n\nPage 2 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nComputational and statistical methods for detecting and preventing fraud have been \ndeveloped and implemented for decades (Bolton and Hand 2002; Phua et  al. 2010; \nAbdallah et al. 2016; West and Bhattacharya 2016). Standard practice for fraud detec-\ntion is to employ statistical methods including the case of machine learning algorithms. \nIn particular, when both fraudulent and non-fraudulent samples are available, one can \nconstruct a classifier via supervised learning (Bolton and Hand 2002; Phua et al. 2010; \nAbdallah et al. 2016; West and Bhattacharya 2016). Exemplar features to be fed to such a \nstatistical classifier include the transaction amount, day of the week, item category, and \nuser’s address for detecting frauds in credit card systems, number of calls, call duration, \ncall type, and user’s age, gender, and geographical region in the case of telecommunica-\ntion, and user profiles and transaction history in the case of online auctions (Abdallah \net al. 2016).\n\nHowever, many of these features can be easily faked by advanced fraudsters (Akoglu \net al. 2015; Google LLC 2018). Furthermore, fraudulent users are adept at escaping the \neyes of the administrators or authorities that would detect the usage of particular words \nas a signature of anomalous behavior (Pu and Webb 2006; Hayes 2007; Bhowmick and \nHazarika 2016). For example, if the authority discovers that one jargon means a drug, \nthen fraudulent users may easily switch to another jargon to confuse the authority.\n\nNetwork analysis is an alternative way to construct features and is not new to fraud \ndetection techniques (Savage et al. 2014; Akoglu et al. 2015). The idea is to use connec-\ntivity between nodes, which are usually users or goods, in the given data and calculate \ngraph-theoretic quantities or scores that characterize nodes. These methods stand on \nthe expectation that anomalous users show connectivity patterns that are distinct from \nthose of normal users (Akoglu et al. 2015). Network analysis has been deployed for fraud \ndetection in insurance (Šubelj et  al. 2011), money laundering (Dreżewski et  al. 2015; \nColladon and Remondi 2017; Savage et al. 2017), health-care data (Liu et al. 2016), car-\nbooking (Shchur et al. 2018), a social security system (Van Vlasselaer et al. 2016), mobile \nadvertising (Hu et al. 2017), a mobile phone network (Ferrara et al. 2014), online social \nnetworks (Bhat and Abulaish 2013; Jiang et  al. 2014; Hooi et  al. 2016; Rasheed et  al. \n2018), online review forums (Akoglu et al. 2013; Liu et al. 2017; Wang et al. 2018), online \nauction or marketplaces (Chau et  al. 2006; Pandit et  al. 2007; Wang and Chiu 2008; \nBangcharoensap et  al. 2015; Yanchun et  al. 2011), credit card transactions (Van Vlas-\nselaer et al. 2015; Li et al. 2017), cryptocurrency transaction (Monamo et al. 2016), and \nvarious other fields (Akoglu et al. 2010). For example, fraudulent users and their accom-\nplices were shown to form approximately bipartite cores in a network of users to inflate \ntheir reputations in an online auction system (Chau et al. 2006). Then, the authors pro-\nposed an algorithm based on a belief propagation to detect such suspicious connectivity \npatterns. This method has been proven to be also effective on empirical data obtained \nfrom eBay (Pandit et al. 2007).\n\nIn the present study, we analyze a data set obtained from a large online consumer-to-\nconsumer (C2C) marketplace, Mercari, operating in Japan and the US. They are the larg-\nest C2C marketplace in Japan, in which, as of 2019, there are 13 million monthly active \nusers and 133 billion yen (approximately 1.2 billion USD) transactions per quarter year \n(Mercari 2019). Note that we analyze transaction frauds based on transaction networks \nof users, which contrasts with previous studies of online C2C marketplaces that looked \n\n\n\nPage 3 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nat reputation frauds (Chau et al. 2006; Pandit et al. 2007; Wang and Chiu 2008; Yanchun \net  al. 2011). Many prior network-based fraud detection algorithms used global infor-\nmation about networks, such as connected components, communities, betweenness, \nk-cores, and that determined by belief propagation (Chau et al. 2006; Pandit et al. 2007; \nWang and Chiu 2008; Šubelj et  al. 2011; Akoglu et  al. 2013; Bhat and Abulaish 2013; \nFerrara et al. 2014; Jiang et al. 2014; Bangcharoensap et al. 2015; Dreżewski et al. 2015; \nVan Vlasselaer et al. 2015; Hooi et al. 2016; Liu et al. 2016; Van Vlasselaer et al. 2016; \nColladon and Remondi 2017; Hu et al. 2017; Li et al. 2017; Liu et al. 2017; Savage et al. \n2017; Shchur et al. 2018; Rasheed et al. 2018; Wang et al. 2018). Others used local infor-\nmation about the users’ network, such as the degree, the number of triangles, and the \nlocal clustering coefficient (Chau et al. 2006; Akoglu et al. 2010; Šubelj et al. 2011; Yan-\nchun et al. 2011; Bhat and Abulaish 2013; Bangcharoensap et al. 2015; Dreżewski et al. \n2015; Monamo et al. 2016; Van Vlasselaer et al. 2016; Colladon and Remondi 2017). We \nwill focus on local features of users, i.e., features of a node that can be calculated from \nthe connectivity of the user and the connectivity between neighbors of the user. This is \nbecause local features are easier and faster to calculate and thus practical for commercial \nimplementations.\n\nMaterials and methods\nData\n\nMercari is an online C2C marketplace service, where users trade various items among \nthemselves. The service is operating in Japan and the United States. In the present study, \nwe used the data obtained from the Japanese market between July 2013 and January \n2019. In addition to normal transactions, we focused on the following types of prob-\nlematic transactions: fictive, underwear, medicine, and weapon. Fictive transactions are \ndefined as selling non-existing items. Underwear refers to transactions of used under-\nwear; they are prohibited by the service from the perspective of morality and hygiene. \nMedicine refers to transactions of medicinal supplies, which are prohibited by the law. \nWeapon refers to transactions of weapons, which are prohibited by the service because \nthey may lead to crime. The number of sampled users of each type is shown in Table 1.\n\nNetwork analysis\n\nWe examine a directed and weighted network of users in which a user corresponds to a \nnode and a transaction between two users represents a directed edge. The weight of the \nedge is equal to the number of transactions between the seller and the buyer. We con-\nstructed egocentric networks of each of several hundreds of normal users and those of \nfraudulent users, i.e., those engaged in at least one problematic sell. Figure 1 shows the \negocentric networks of two normal users (Fig. 1a, b) and those of two fraudulent users \ninvolved in selling a fictive item (Fig. 1c, d). The egocentric network of either a normal or \nfraudulent user contained the nodes neighboring the focal user, edges between the focal \nuser and these neighbors, and edges between the pairs of these neighbors.\n\nWe calculated eight indices for each focal node. They are local indices in the mean-\ning that they require the information up to the connectivity among the neighbors of the \nfocal node.\n\n\n\nPage 4 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nFive out of the eight indices use only the information about the connectivity of the focal \nnode. The degree ki of node vi is the number of its neighbors. The node strength  (Barrat \net al. 2004) (i.e., weighted degree) of node vi , denoted by si , is the number of transactions in \nwhich vi is involved. Using these two indices, we also considered the mean number of trans-\nactions per neighbor, i.e., si/ki , as a separate index. These three indices do not use informa-\ntion about the direction of edges.\n\nThe sell probability of node vi , denoted by SPi , uses the information about the direction of \nedges and defined as the proportion of the vi ’s neighbors for which vi acts as seller. Precisely, \nthe sell probability is given by\n\n(1)SPi =\nkouti\n\nk ini + kouti\n\n,\n\nFig. 1 Examples of egocentric networks. a, b Egocentric networks of arbitrarily selected two normal users. c, \nd Egocentric networks of arbitrarily selected two fraudulent users involved in selling a fictive item\n\n\n\n\n\nPage 5 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nwhere k ini  is vi ’s in-degree (i.e., the number of neighbors from whom vi bought at least \none item) and kouti  is vi ’s out-degree (i.e., the number of neighbors to whom vi sold at \nleast one item). It should be noted that, if vi acted as both seller and buyer towards vj , the \ncontribution of vj to both in- and out-degree of vi is equal to one. Therefore, k ini + kouti  is \nnot equal to ki in general.\n\nThe weighted version of the sell probability, denoted by WSPi , is defined as\n\nwhere sini  is node vi ’s weighted in-degree (i.e., the number of buys) and souti  is vi ’s weighted \nout-degree (i.e., the number of sells).\n\nThe other three indices are based on triangles that involve the focal node. The local \nclustering coefficient Ci quantifies the abundance of undirected and unweighted triangles \naround vi (Newman 2010). It is defined as the number of undirected and unweighted trian-\ngles including vi divided by ki(ki − 1)/2 . The local clustering coefficient Ci ranges between \n0 and 1.\n\nWe hypothesized that triangles contributing to an increase in the local clustering coef-\nficient are localized around particular neighbors of node vi . Such neighbors together with vi \nmay form an overlapping set of triangles, which may be regarded as a community (Radicchi \net al. 2004; Palla et al. 2005). Therefore, our hypothesis implies that the extent to which the \nfocal node is involved in communities should be different between normal and fraudulent \nusers. To quantify this concept, we introduce the so-called triangle congregation, denoted \nby mi . It is defined as the extent to which two triangles involving vi share another node and \nis given by\n\nwhere Tri = Ciki(ki − 1)/2 is the number of triangles involving vi . Note that mi ranges \nbetween 0 and 1.\n\nFrequencies of different directed three-node subnetworks, conventionally known as net-\nwork motifs (Milo et al. 2002), may distinguish between normal and fraudulent users. In \nparticular, among triangles composed of directed edges, we hypothesized that feedforward \ntriangles (Fig. 2a) should be natural and that cyclic triangles (Fig. 2b) are not. We hypoth-\nesized so because a natural interpretation of a feedforward triangle is that a node with out-\ndegree two tends to serve as seller while that with out-degree zero tends to serve as buyer \nand there are many such nodes that use the marketplace mostly as buyer or seller but not \nboth. In contrast, an abundance of cyclic triangles may imply that relatively many users use \nthe marketplace as both buyer and seller. We used the index called the cycle probability, \ndenoted by CYPi , which is defined by\n\nwhere FFi and CYi are the numbers of feedforward triangles and cyclic triangles to which \nnode vi belongs. The definition of FFi and CYi , and hence CYPi , is valid even when the \n\n(2)WSPi =\nsouti\n\nsini + souti\n\n,\n\n(3)mi =\n(Number of pairs of triangles involving vi that share another node)\n\nTri(Tri − 1)/2\n,\n\n(4)CYPi =\nCYi\n\nFFi + CYi\n,\n\n\n\nPage 6 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\ntriangles involving vi have bidirectional edges. In the case of Fig. 2c, for example, any of \nthe three nodes contains one feedforward triangle and one cyclic triangle. The other four \ncases in which bidirectional edges are involved in triangles are shown in Fig. 2d–g. In the \ncalculation of CYPi , we ignored the weights of edges.\n\nRandom forest classifier\n\nTo classify users into normal and fraudulent users based on their local network proper-\nties, we employed a random forest classifier (Breiman 2001; Breiman et al. 1984; Hastie \net  al. 2009) implemented in scikit-learn (Pedregosa et  al. 2011). It uses an ensemble \nlearning method that combines multiple classifiers, each of which is a decision tree, \nbuilt from training data and classifies test data avoiding overfitting. We combined 300 \ndecision-tree classifiers to construct a random forest classifier. Each decision tree is con-\nstructed on the basis of training samples that are randomly subsampled with replace-\nment from the set of all the training samples. To compute the best split of each node \nin a tree, one randomly samples the candidate features from the set of all the features. \nThe probability that a test sample is positive in a tree is estimated as follows. Consider \nthe terminal node in the tree that a test sample eventually reaches. The fraction of posi-\ntive training samples at the terminal node gives the probability that the test sample is \nclassified as positive. One minus the positive probability gives the negative probability \nestimated for the same test sample. The positive or negative probability for the random \nforest classifier is obtained as the average of single-tree positive or negative probability \nover all the 300 trees. A sample is classified as positive by the random forest classifier if \nthe positive probability is larger than 0.5, otherwise classified as negative.\n\nWe split samples of each type into two sets such that 75% and 25% of the samples of \neach type are assigned to the training and test samples, respectively. There were more \n\ncyclicfeedforward feedforward: 1\ncyclic: 1\n\nfeedforward: 2\ncyclic: 0\n\nfeedforward: 3\ncyclic: 1\n\nfeedforward: 6\ncyclic: 2\n\na b c d\n\nf g\n\nfeedforward: 2\ncyclic: 0\n\ne\n\nFig. 2 Directed triangle patterns and their count. a Feedforward triangle. b Cyclic triangle. c– g Five \nthree-node patterns that contain directed triangles and reciprocal edges. The numbers shown in the figure \nrepresent the number of feedforward or cyclic triangles to which each three-node pattern contributes\n\n\n\nPage 7 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nnormal users than any type of fraudulent user. Therefore, to balance the number of \nthe negative (i.e., normal) and positive (i.e., fraudulent) samples, we uniformly ran-\ndomly subsampled the negative samples (i.e., under-sampling) such that the number \nof the samples is the same between the normal and fraudulent types in the training \nset. Based on the training sample constructed in this manner, we built each of the 300 \ndecision trees and hence a random forest classifier. Then, we examined the classifica-\ntion performance of the random forest classifier on the set of test samples.\n\nThe true positive rate, also called the recall, is defined as the proportion of the posi-\ntive samples (i.e., fraudulent users) that the random forest classifier correctly classifies \nas positive. The false positive rate is defined as the proportion of the negative samples \n(i.e., normal users) that are incorrectly classified as positive. The precision is defined \nas the proportion of the truly positive samples among those that are classified as posi-\ntive. The true positive rate, false positive rate, and precision range between 0 and 1.\n\nWe used the following two performance measures for the random forest classifier. \nTo draw the receiver operating characteristic (ROC) curve for a random forest clas-\nsifier, one first arranges the test samples in descending order of the estimated prob-\nability that they are positive. Then, one plots each test sample, with its false positive \nrate on the horizontal axis and the true positive rate on the vertical axis. By connect-\ning the test samples in a piecewise linear manner, one obtains the ROC curve. The \nprecision–recall (PR) curve is generated by plotting the samples in the same order in \n[0, 1]2 , with the recall on the horizontal axis and the precision on the vertical axis. For \nan accurate binary classifier, both ROC and PR curves visit near (x, y) = (0, 1) . There-\nfore, we quantify the performance of the classifier by the area under the curve (AUC) \nof each curve. The AUC ranges between 0 and 1, and a large value indicates a good \nperformance of the random forest classifier.\n\nTo calculate the importance of each feature in the random forest classifier, we \nused the permutation importance (Strobl et al. 2007; Altmann et al. 2010). With this \nmethod, the importance of a feature is given by the decrease in the performance of \nthe trained classifier when the feature is randomly permuted among the test samples. \nA large value indicates that the feature considerably contributes to the performance \nof the classifier. To calculate the permutation importance, we used the AUC value of \nthe ROC curve as the performance measure of a random forest classifier. We com-\nputed the permutation importance of each feature with ten different permutations \nand adopted the average over the ten permutations as the importance of the feature.\n\nWe optimized the parameters of the random forest classifier by a grid search with \n10-fold cross-validation on the training set. For the maximum depth of each tree (i.e., \nthe max_depth parameter in scikit-learn), we explored the integers between 3 and 10. \nFor the number of candidate features for each split (i.e., max_features), we explored \nthe integers between 3 and 6. For the minimum number of samples required at termi-\nnal nodes (i.e., min_samples_leaf ), we explored 1, 3, and 5. As mentioned above, the \nnumber of trees (i.e., n_estimators) was set to 300. The seed number for the random \nnumber generator (i.e., random_state) was set to 0. For the other hyperparameters, \nwe used the default values in scikit-learn version 0.22. In the parameter optimization, \nwe evaluated the performance of the random forest classifier with the AUC value of \nthe ROC curve measured on a single set of training and test samples.\n\n\n\nPage 8 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nTo avoid sampling bias, we built 100 random forest classifiers, trained each classifier, \nand tested its performance on a randomly drawn set of train and test samples, whose \nsampling scheme was described above.\n\nResults\nDescriptive statistics\n\nThe survival probability of the degree (i.e., a fraction of nodes whose degree is larger \nthan a specified value) is shown in Fig. 3a for each user type. Approximately 60% of the \nnormal users have degree ki = 1 , whereas the fraction of the users with ki = 1 is approxi-\nmately equal to 2% or less for any type of fraudulent user (Table 1). Therefore, we expect \nthat whether ki = 1 or ki ≥ 2 gives useful information for distinguishing between normal \nand fraudulent users. The degree distribution at ki ≥ 2 may provide further information \nuseful for the classification. The survival probability of the degree distribution condi-\ntioned on ki ≥ 2 for the different types of users is shown in Fig. 3b. The figure suggests \nthat the degree distribution is systematically different between the normal and fraudu-\nlent users. However, we consider that the difference is not as clear-cut as that in the frac-\ntion of users having ki = 1 (Table 1).\n\nThe survival probability of the node strength (i.e., weighted degree) is shown in Fig. 3c \nfor each user type. As in the case for the unweighted degree, we found that many nor-\nmal users, but not fraudulent users, have si = 1 . In fact, the number of the normal users \nwith si = 1 is equal to those with ki = 1 (Table 1), implying that all normal users with \nki = 1 participated in just one transaction. In contrast, no user had si = 1 for any type \nof fraudulent user. The survival probability of the node strength conditioned on si ≥ 2 \napparently does not show a clear distinction between the normal and fraudulent users \n(Fig. 3d, Table 1).\n\na b\n\nc d\n\nFig. 3 Survival probability of the degree for each user type. a Degree (i.e., ki ) for all nodes. b Degree for the \nnodes with ki ≥ 2 . c Strength (i.e., si ) for all nodes. d Strength for the nodes with si ≥ 2\n\n\n\nPage 9 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nThe distribution of the average number of transactions per edge, i.e., si/ki , is shown \nin Fig. 4a. We found that a majority of normal users have si/ki = 1 . This result indicates \nthat a large fraction of normal users is engaged in just one transaction per neighbor \n(Table 1). This result is consistent with the fact that approximately 60% of the normal \nusers have ki = si = 1 . In contrast, many of any type of fraudulent users have si/ki > 1 . \nHowever, they tend to have a smaller value of si/ki than the normal users. This differ-\nence is more noticeable when we discraded the users with si/ki = 1 (Fig. 4b, Table 1). \nTherefore, less frequent transactions with a specific neighbor seem to be a characteristic \nbehavior of fraudulent users.\n\nThe distribution of the unweighted sell probability for the different user types is \nshown in Fig.  5a. The distribution for the normal users is peaked around 0 and 1, \n\nTable 1 Properties of different types of users\n\nIn the first column, Mean ( A | B ), for example, represents the mean of A conditioned on B. Unless the first column mentions \nthe conditional mean, median, or the number of transactions, the numbers reported in the table represent the number of \nusers\n\nSeed user type Normal Fictive Underwear Medicine Weapon\n\nNumber of seed users 999 440 468 469 416\n\nNumber of transactions \ninvolving the seed user\n\n151,021 66,215 151,278 92,497 81,970\n\nTotal number of transactions 27,683,860 850,739 2,325,898 925,361 533,963\n\nki = 1 587 (58.8%) 8 (1.8%) 3 (0.6%) 2 (0.4%) 5 (1.2%)\n\nMean ( ki | ki ≥ 2) 195.0 138.3 297.8 184.2 179.7\n\nMedian ( ki | ki ≥ 2) 77.5 61.0 170.0 97.0 86.0\n\nsi = 1 587 (58.8%) 8 (1.8%) 3 (0.6%) 2 (0.4%) 5 (1.2%)\n\nMean ( si | si ≥ 2) 365.1 153.3 325.3 198.1 199.4\n\nMedian ( si | si ≥ 2) 89.0 66.5 175.0 100.0 90.0\n\nsi ≥ 2 412 432 465 467 411\n\nsi/ki = 1 97 (23.5%) 97 (22.5%) 86 (18.5%) 156 (33.4%) 121 (29.4%)\n\nMean ( si/ki | si/ki > 1) 1.413 1.135 1.055 1.066 1.092\n\nMedian ( si/ki | si/ki > 1) 1.124 1.059 1.03 1.031 1.055\n\nki ≥ 2 412 432 465 467 411\n\nSPi = 1 157 (38.1%) 15 (3.5%) 21 (4.5%) 16 (3.4%) 17 (4.1%)\n\nk\nout\ni\n\n= 1 118 (28.6%) 21 (4.9%) 2 (0.4%) 2 (0.4%) 9 (2.2%)\n\nsi ≥ 2 412 432 465 467 411\n\nWSPi = 1 157 (38.1%) 15 (3.5%) 21 (4.5%) 16 (3.4%) 17 (4.1%)\n\ns\nout\ni\n\n= 1 118 (28.6%) 14 (3.2%) 2 (0.4%) 2 (0.4%) 9 (2.2%)\n\nki ≥ 2 412 432 465 467 411\n\nCi = 0 118 (28.6%) 152 (35.2%) 108 (23.2%) 154 (33.0%) 128 (31.1%)\n\nMean ( Ci | Ci > 0) 8.554× 10\n−3\n\n8.348× 10\n−3 9.500× 10\n\n−4\n2.231× 10\n\n−3\n3.810× 10\n\n−3\n\nMedian ( Ci | Ci > 0) 2.411× 10\n−3\n\n2.039× 10\n−3 5.288× 10\n\n−4\n6.494× 10\n\n−4\n1.337× 10\n\n−3\n\nTri ≥ 2 262 241 317 251 244\n\nmi = 0 17 (6.5%) 27 (11.2%) 54 (17.0%) 44 (17.5%) 32 (13.1%)\n\nmi = 1 12 (4.6%) 9 (3.7%) 4 (1.3%) 6 (2.4%) 11 (4.5%)\n\nMean ( mi | mi > 0) 8.554× 10\n−3\n\n8.348× 10\n−3 9.500× 10\n\n−4\n2.231× 10\n\n−3\n3.810× 10\n\n−3\n\nMedian ( mi | mi > 0) 2.411× 10\n−3\n\n2.039× 10\n−3 5.288× 10\n\n−4\n6.494× 10\n\n−4\n1.337× 10\n\n−3\n\nFFi + CYi ≥ 1 294 280 357 313 283\n\nCYPi = 0 234 (79.6%) 188 (67.1%) 222 (62.2%) 227 (72.5%) 202 (71.4%)\n\nMean ( CYPi | CYPi > 0) 1.987× 10\n−2\n\n7.367× 10\n−2\n\n6.739× 10\n−2\n\n8.551× 10\n−2\n\n5.544× 10\n−2\n\nMedian ( CYPi | CYPi > 0) 1.521× 10\n−2\n\n4.481× 10\n−2\n\n3.396× 10\n−2\n\n3.822× 10\n−2\n\n3.618× 10\n−2\n\n\n\nPage 10 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nindicating that a relatively large fraction of normal users is almost exclusive buyer or \nseller. Note that, by definition, the sell probability is at least 1/(k ini + kouti ) because our \nsamples are sellers. Therefore, a peak around the sell probability of zero implies that \nthe users probably have no or few sell transactions apart from the one sell transaction \nbased on which the users have been sampled as seller. In contrast, the distribution \nfor any fraudulent type is relatively flat. Figure  5b shows the relationships between \nthe unweighted sell probability and the degree. On the dashed line in Fig. 5b, the sell \nprobability is equal to 1/(k ini + kouti ) , indicating that the node has kouti = 1 , which is \nthe smallest possible out-degree. The users on this line were buyers in all but one \n\na b\n\nFig. 4 Survival probability of the average number of transactions per neighbor. a si/ki for all nodes. b si/ki for \nthe nodes with si/ki > 1\n\na b\n\nc d\n\nFig. 5 Sell probability for each user type. a Distribution of the unweighted sell probability. b Relationship \nbetween the degree and the unweighted sell probability. c Distribution of the weighted sell probability. d \nRelationship between the node strength and the weighted sell probability. The dashed lines in b, d indicate \n1/(k in\n\ni\n+ k\n\nout\ni\n\n) and 1/(sin\ni\n+ s\n\nout\ni\n\n) , respectively\n\n\n\nPage 11 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\ntransaction. Figure 5b indicates that a majority of such users are normal as opposed \nto fraudulent users, which is quantitatively confirmed in Table 1. We also found that \nmost of the normal users were either on the horizontal line with the sell probability \nof one (38.1% of the normal users with ki ≥ 2 ; see Table 1 for the corresponding frac-\ntions of normal users with ki = 1 ) or on the dashed line (28.6%). This is not the case \nfor any type of fraudulent user (Table 1).\n\nThe distribution of the weighted sell probability for the different user types and the \nrelationships between the weighted sell probability and the node strength are shown \nin Fig.  5c, d, respectively. The results are similar to the case of the unweighted sell \nprobability in two aspects. First, the normal users and the fraudulent users form dis-\ntinct frequency distributions (Fig. 5c). Second, most of the normal users are either on \nthe horizontal line with the weighted sell probability of one or on the dashed line with \nthe smallest possible weighted sell probability, i.e., 1/si (Fig. 5d, Table 1).\n\nThe survival probability of the local clustering coefficient is shown in Fig.  6a. It \nshould be noted that, in this analysis, we confined ourselves to the users with ki ≥ 2 \nbecause Ci is undefined when ki = 1 . We found that the number of users with Ci = 0 is \nnot considerably different between the normal and fraudulent users (also see Table 1). \nFigure  6b shows the survival probability of Ci conditioned on Ci > 0 . The normal \nusers tend to have a larger value of Ci than fraudulent users, whereas this tendency is \nnot strong (Table 1).\n\nThe survival probability of the triangle congregation is shown in Fig. 7a. Contrary to \nour hypothesis, there is no clear difference between the distribution of the normal and \nfraudulent users. The triangle congregation tends to be large when the node strength \nis small (Fig. 7b) and the local clustering coefficient is large (Fig. 7d). It depends little \non the weighted sell probability (Fig. 7c). However, we did not find clear differences in \nthe triangle congregation between the normal and fraudulent users (also see Table 1).\n\nThe survival probability of the cycle probability is shown in Fig. 8a. A large fraction \nof any type of users has CYPi = 0 (Table 1). When the users with CYPi = 0 are dis-\ncarded, the normal users tend to have a smaller value of CYPi than any type of fraudu-\nlent users (Fig. 8b, Table 1).\n\na b\n\nFig. 6 Local clustering coefficient for each user type. a Survival probability. b Survival probability conditioned \non Ci > 0\n\n\n\nPage 12 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nClassification of users\n\nBased on the eight indices whose descriptive statistics were analyzed in the previ-\nous section, we defined 12 features and fed them to the random forest classifier. The \naim of the classifier is to distinguish between normal and fraudulent users. The first \nfeature is binary and whether the degree ki = 1 or ki ≥ 2 . The second feature is also \nbinary and whether the node strength si = 1 or si ≥ 2 . The third feature is si/ki , which \nis a real number greater than or equal to 1. The fourth feature is binary and whether the \nunweighted sell probability SPi = 1 or SPi < 1 . The fifth feature is binary and whether \n\na b\n\nc d\n\nFig. 7 Triangle congregation for each user type. a Survival probability. b Relationship between the triangle \ncongregation, mi , and the node strength. c Relationship between mi and the weighted sell probability. d \nRelationship between mi and the local clustering coefficient\n\na b\n\nFig. 8 Cycle probability for each user type. a Survival probability. b Survival probability conditioned on \nCYPi > 0\n\n\n\nPage 13 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nSPi = 1/(k ini + kouti ) or SPi > 1/(k ini + kouti ) , i.e., whether kouti = 1 or kouti > 1 . The sixth \nfeature is SPi , which ranges between 0 and 1. The seventh feature is binary and whether \nthe weighted sell probability WSPi = 1 or WSPi < 1 . The eighth feature is binary and \nwhether WSPi = 1/(sini + souti ) or WSPi > 1/(sini + souti ) , i.e., whether souti = 1 or \nsouti > 1 . The ninth feature is WSPi , which ranges between 0 and 1. The tenth feature is \nthe local clustering coefficient Ci , which ranges between 0 and 1. When ki = 1 , the local \nclustering coefficient is undefined. In this case, we set Ci = − 1 . The eleventh feature is \nthe triangle congregation mi , which ranges between 0 and 1. When there is no triangle \nor only one triangle involving vi , one cannot calculate mi . In this case, we set mi = − 1 . \nFinally, the twelfth feature is the cycle probability CYPi , which ranges between 0 and 1. \nWhen there is neither feedforward nor cyclic triangle involving vi , CYPi is undefined. In \nthis case, we set CYPi = − 1.\n\nThe ROC and PR curves when all the 12 features of users are used and the fraudu-\nlent type is fictive transactions are shown in Fig. 9a, b, respectively. Each thin line cor-\nresponds to one of the 100 classifiers. The thick lines correspond to the average of the \n100 lines. The dashed lines correspond to the uniformly random classification. Figure 9 \nindicates that the classification performance seems to be high. Quantitatively, for this \nand the other types of fraudulent users, the AUC values always exceeded 0.91 (Table 2).\n\nThe importance of each feature in the classifier is shown in Fig.  10a, separately for \nthe different fraud types. The importance of each feature is similar across the different \ntypes of fraud. Figure 10a indicates that the average number of transactions per neighbor \n(i.e., si/ki ), whether or not kouti = 1 (i.e., SPi = 1/(k ini + kouti ) ), whether or not souti = 1 \n(i.e., WSPi = 1/(sini + souti ) ), and the weighted sell probability (i.e., WSPi ) are the four \nfeatures of the highest importance. Given the results of the descriptive statistics in the \nprevious section, a small value of si/ki , kouti  = 1 , souti  = 1 , and a moderate WSPi value \nstrongly suggest that the user may be fraudulent.\n\nFigure 10a also suggests that the features based on the triangles, i.e., Ci , mi , and CYPi , \nare not strong contributors to the classifier’s performance. Because these features are the \nonly ones that require the information about the connectivity between pairs of neigh-\nbors of the focal node, it is practically beneficial if one can realize a similar classification \n\na b\n\nFig. 9 ROC and PR curves when the normal users and those involved in fictive transactions are classified. a \nROC curves. b PR curves. Each thin line corresponds to one of the 100 classifiers. The thick lines correspond to \nthe average of the 100 lines. The dashed lines correspond to the uniformly random classification\n\n\n\nPage 14 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nperformance without using these features; then only the information on the connectivity \nof the focal users is required. To explore this possibility, we constructed the random for-\nest classifier using the nine out of the twelve features that do not require the connectivity \nbetween neighbors of the focal node. The mean AUC values for the ROC and PR curves \nare shown in Table 2. We find that, despite some reduction in the performance scores \nrelative to the case of the classifier using all the 12 features, the AUC values with the \nnine features are still large, all exceeding 0.88. The permutation importance of the nine \nfeatures is shown in Fig. 10b. The results are similar to those when all the 12 features are \nused, although the importance of WSPi considerably increased in the case of the nine \nfeatures (Fig. 10a).\n\nMore than half of the normal users have ki = 1 , and there are few fraudulent users \nwith ki = 1 in each fraud category (Table 1). The classification between the normal and \nfraudulent users may be an easy problem for this reason, leading to the large AUC val-\nues. To exclude this possibility, we carried out a classification test for the subdata in \nwhich the normal and fraudulent users with ki = 1 were excluded, leaving 412 normal \nusers and a similar number of fraudulent users in each category (Table 1). We did not \n\na b\n\nFig. 10 Permutation importance of the features in the random forest classifier. a 12 features. b 9 features. The \nbars indicate the average over the 100 classifiers. The error bars indicate standard deviation\n\nTable 2 AUC values for the random forest classifiers\n\nThe average and standard deviation were calculated based on the 100 classifiers\n\nFictive Underwear Medicine Weapon\n\n12 features\n\nROC 0.962 ± 0.003 0.981 ± 0.001 0.979 ± 0.003 0.969 ± 0.004\n\nPR 0.916 ± 0.009 0.948 ± 0.006 0.947 ± 0.005 0.916 ± 0.015\n\n9 features\n\nROC 0.951 ± 0.003 0.973 ± 0.003 0.971 ± 0.003 0.961 ± 0.004\n\nPR 0.889 ± 0.009 0.923 ± 0.010 0.930 ± 0.009 0.888 ± 0.025\n\n\n\nPage 15 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\ncarry out subsampling because the number of the negative and positive samples were \nsimilar. Instead, we generated 100 different sets of train and test samples and built a clas-\nsifier based on each set of train and test samples. The AUC values when either 10 or 7 \nfeatures (i.e., the features excluding whether or not ki = 1 and whether or not si = 1 ) are \nused are shown in Table 3. The table indicates that the AUC values are still competitively \nlarge while they are smaller than those when whether or not ki = 1 and whether or not \nsi = 1 are used as features (Table 2).\n\nDiscussion\nWe showed that a random forest classifier using network features of users distinguished \ndifferent types of fraudulent users from normal users with approximately 0.91–0.98 in \nterms of the AUC. We only used the information about local transaction networks cen-\ntered around focal users to synthesize their features. We did so because it is better in \npractice not to demand the information about global transaction networks due to the \nlarge number of users. It should be noted that AUC values of ≈ 0.88–0.97 was also real-\nized when we only used the information about the connectivity of the focal user, not the \nconnectivity between the neighbors of the focal user. This result has a practical advan-\ntage when the present fraud-detection method is implemented online because it allows \none to classify users with a smaller amount of data per user.\n\nThe random forest classifier is an arbitrary choice. One can alternatively use a different \nlinear or nonlinear classifier to pursue a higher classification performance. This is left as \nfuture work. Other future tasks include the generalizability of the present results to dif-\nferent types of fraudulent transactions, such as resale tickets, pornography, and stolen \nitems, and to different platforms. In particular, if a classifier trained with test samples \nfrom fraudulent users of a particular type and normal users is effective at detecting dif-\nferent types of fraud, the classifier will also be potentially useful for detecting unknown \ntypes of fraudulent transactions. It is also a potentially relevant question to assess the \nclassification performance when one pools different types of fraud as a single positive \ncategory to train a classifier.\n\nPrior network-based fraud detection has employed either global or local network \nproperties to characterize nodes. Global network properties refer to those that require \nthe structure of the entire network for calculating a quantity for individual nodes, such \nas the connected component (Šubelj et al. 2011; Savage et al. 2017; Wang et al. 2018), \nbetweenness centrality (Šubelj et al. 2011; Dreżewski et al. 2015; Colladon and Remondi \n2017), user’s suspiciousness determined by belief propagation (Chau et al. 2006; Pandit \n\nTable 3 AUC values for the random forest classifiers excluding users with ki = 1\n\nThe average and standard deviation were calculated based on the 100 classifiers\n\nFictive Underwear Medicine Weapon\n\n10 features\n\nROC 0.925 ± 0.016 0.950 ± 0.013 0.954 ± 0.012 0.916 ± 0.019\n\nPR 0.923 ± 0.019 0.950 ± 0.018 0.954 ± 0.016 0.911 ± 0.023\n\n7 features\n\nROC 0.886 ± 0.020 0.921 ± 0.015 0.933 ± 0.014 0.899 ± 0.020\n\nPR 0.874 ± 0.027 0.901 ± 0.021 0.928 ± 0.019 0.880 ± 0.028\n\n\n\nPage 16 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\net al. 2007; Akoglu et al. 2013; Bangcharoensap et al. 2015; Van Vlasselaer et al. 2015, \n2016; Li et al. 2017; Hu et al. 2017), dense subgraphs including the case of communities \n(Šubelj et al. 2011; Bhat and Abulaish 2013; Ferrara et al. 2014; Jiang et al. 2014; Hooi \net al. 2016; Liu et al. 2016; Shchur et al. 2018), and k-core (Wang and Chiu 2008; Rasheed \net  al. 2018). Although many of these methods have accrued a high classification per-\nformance, they require the information about the entire network. Obtaining such data \nmay be difficult when the network is large or rapidly evolving over time, thus potentially \ncompromising the computation speed, memory requirement, and the accuracy of the \ninformation on the nodes and edges. Alternatively, other methods employed local net-\nwork properties such as the degree including the case of directed and/or weighted net-\nworks (Chau et al. 2006; Akoglu et al. 2010; Šubelj et al. 2011; Yanchun et al. 2011; Bhat \nand Abulaish 2013; Bangcharoensap et al. 2015; Dreżewski et  al. 2015; Monamo et al. \n2016; Van Vlasselaer et al. 2016; Colladon and Remondi 2017) and the abundance of tri-\nangles and quadrangles (Monamo et al. 2016; Van Vlasselaer et al. 2016). The use of local \nnetwork properties may be advantageous in industrial contexts, particularly to test sam-\npled users, because local quantities can be rapidly calculated given a seed node. Another \nreason for which we focused on local properties was that we could not obtain the global \nnetwork structure for computational reasons. It should be noted that, while the use of \nglobal network properties in addition to local ones may improve the classification accu-\nracy (Bhat and Abulaish 2013), the present local method attained a similar classification \nperformance to those based on global network properties, i.e., 0.880–0.986 in terms of \nthe ROC AUC (Šubelj et al. 2011; Van Vlasselaer et al. 2015; Van Vlasselaer et al. 2016; \nHu et al. 2017; Li et al. 2017; Savage et al. 2017).\n\nA prior study using data from the same marketplace, Mercari, aimed to distinguish \nbetween desirable non-professional frequent sellers and undesirable professional sellers \n(Yamamoto et al. 2019). The authors used information about user profiles, item descrip-\ntions, and other behavioral data such as the number of purchases per day. In contrast, \nwe focused on local network features of the users (while a quantity similar to WSPi was \nused as a feature in Yamamoto et al. (2019)). In addition, we used specific types of fraud-\nulent transactions, whereas Yamamoto et al. (2019) focused on problematic transactions \nas a single broad category. How the present results generalize to different categoriza-\ntions of fraudulent transactions, the platform’s different data such as their US market \ndata, and similar data obtained from other online marketplaces is unknown. Combining \nnetwork and non-network features may realize a better classification performance . Fur-\nthermore, using the information about the time of the transactions may also yield better \nclassification. Using the time information allows us to ask new questions such as predic-\ntion of users’ behavior. These topics warrant future work.\n\nAbbreviations\nC2C: Consumer-to-consumer; ROC: Receiver operating characteristic; PR: Precision–recall; AUC : Area under the curve.\n\nAcknowledgements\nThis work was carried out using the computational facilities of the Advanced Computing Research Centre, University of \nBristol.\n\nAuthors’ contributions\nShun Kodate analyzed data, developed methodology, visualized the results, and drafted the manuscript; RC curated data \nand critically revised the manuscript; Shunya Kimura coordinated the study, acquired funding, and critically revised the \nmanuscript; NM coordinated the study, acquired funding, developed methodology, drafted the manuscript. All authors \n\n\n\nPage 17 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\ngave final approval for publication and agreed to be held accountable for the work performed therein. All authors read \nand approved the final manuscript.\n\nFunding\nThe authors acknowledge financial support by Mercari, Inc. S. Kodate was supported in part by the Top Global University \nProject from the Ministry of Education, Culture, Sports, Science and Technology (MEXT) of Japan.\n\nAvailability of data and materials\nMercari, Inc. approved the use of the data for the present study under the condition that the data were hashed and \nonly released to the collaborators of the project (i.e., the first and last authors, because the second and third authors \nare employees of the company). The figures and tables of the present paper are summary statistics of the data and not \nsufficient on their own for others to replicate the results of the present study. Although the data have been hashed, the \ncompany cannot share the data with the public. This is because, if anybody traces the transaction data on the Mercari’s \nweb platform and checks them against the hashed data, that person would be able to identify individual users including \ntheir private information. Therefore, hashing/anonymizing does not help to guarantee the users’ privacy. Any bona fide \nresearcher could approach the company (Shunya Kimura: kimuras@mercari.com and Ryusuke Chiba: metalunk@mercari.\ncom) to seek access to the complete dataset. However, for the aforementioned reasons, such an attempt is unlikely to be \nsuccessful. The users were made aware that their data may be used for the present research because the Mercari’s terms \nof use (in Japanese only: https ://www.merca ri.com/jp/tos/), Article 20, Term 2, states that their data can be used for \nresearch by the company and by those who the company permits.\n\nEthics approval and consent to participate\nMercari, Inc. approved the use of the data for the present study under the condition that the data were hashed and \nonly released to the collaborators of the project (i.e., the first and last authors, because the second and third authors are \nemployees of the company).\n\nCompeting interests\nThe second and third authors are employees of the company that provided the data analysed in the present manuscript. \nHowever, this fact does not cause any conflict of interest because the analyses, results and their interpretation are free of \nany bias towards the merit of the company.\n\nAuthor details\n1 Graduate School of Information Sciences, Tohoku University, Sendai 980-8579, Japan. 2 Department of Engineering \nMathematics, University of Bristol, Bristol BS8 1UB, UK. 3 Mercari, Inc., Tokyo 106-6118, Japan. 4 Department of Mathemat-\nics, University at Buffalo, Buffalo, NY 14260-2900, USA. 5 Computational and Data-Enabled Science and Engineering \nProgram, University at Buffalo, Buffalo, NY 14260-5030, USA. \n\nReceived: 12 August 2020   Accepted: 23 October 2020\n\nReferences\nAbdallah A, Maarof MA, Zainal A (2016) Fraud detection system: a survey. J Netw Comput Appl 68:90–113\nAkoglu L, McGlohon M, Faloutsos C (2010) Oddball: spotting anomalies in weighted graphs. In: Pacific-Asia conference on \n\nknowledge discovery and data mining, pp 410–421\nAkoglu L, Chandy R, Faloutsos C (2013) Opinion fraud detection in online reviews by network effects. In: 7th international \n\nAAAI conference on weblogs and social media, pp 2–11\nAkoglu L, Tong H, Koutra D (2015) Graph based anomaly detection and description: a survey. Data Min Knowl Discov \n\n29:626–688\nAltmann A, Toloşi L, Sander O, Lengauer T (2010) Permutation importance: a corrected feature importance measure. Bioinfo \n\n26:1340–1347\nAnderson R, Barton C, Böhme R, Clayton R, Van Eeten MJ, Levi M, Moore T, Savage S (2013) Measuring the cost of cybercrime. \n\nIn: The economics of information security and privacy. Springer, Berlin, pp 265–300\nBangcharoensap P, Kobayashi H, Shimizu N, Yamauchi S, Murata T (2015) Two step graph-based semi-supervised learning \n\nfor online auction fraud detection. In: Joint European conference on machine learning and knowledge discovery in \ndatabases, pp 165–179\n\nBarrat A, Barthelemy M, Pastor-Satorras R, Vespignani A (2004) The architecture of complex weighted networks. Proc Natl \nAcad Sci USA 101:3747–3752\n\nBhat SY, Abulaish M (2013) Community-based features for identifying spammers in online social networks. In: 2013 IEEE/ACM \ninternational conference on advances in social networks analysis and mining (ASONAM 2013), pp 100–107\n\nBhowmick A, Hazarika SM (2016) Machine learning for e-mail spam filtering: review, techniques and trends. Preprint arXiv \n:1606.01042 \n\nBolton RJ, Hand DJ (2002) Statistical fraud detection: a review. Stat Sci 17:235–249\nBreiman L (2001) Random forests. Mach Learn 45:5–32\nBreiman L, Friedman JH, Olshen RA, Stone CJ (1984) Classification and regression trees. Chapman & Hall, Boca Raton\nChau DH, Pandit S, Faloutsos C (2006) Detecting fraudulent personalities in networks of online auctioneers. In: European \n\nconference on principles of data mining and knowledge discovery, pp 103–114\nColladon AF, Remondi E (2017) Using social network analysis to prevent money laundering. Expert Syst Appl 67:49–58\nDreżewski R, Sepielak J, Filipkowski W (2015) The application of social network analysis algorithms in a system supporting \n\nmoney laundering detection. Inf Sci 295:18–32\nFerrara E, De Meo P, Catanese S, Fiumara G (2014) Detecting criminal organizations in mobile phone networks. Expert Syst \n\nAppl 41:5733–5750\nGoogle LLC and White Ops, Inc (2018) The Hunt for 3ve. https ://servi ces.googl e.com/fh/files /blogs /3ve_googl e_white \n\nops_white paper _final _nov_2018.pdf. Accessed: 10 May 2019\n\n\n\nhttps://www.mercari.com/jp/tos/\nhttp://arxiv.org/abs/1606.01042\nhttp://arxiv.org/abs/1606.01042\nhttps://services.google.com/fh/files/blogs/3ve_google_whiteops_whitepaper_final_nov_2018.pdf\nhttps://services.google.com/fh/files/blogs/3ve_google_whiteops_whitepaper_final_nov_2018.pdf\n\n\nPage 18 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nHastie T, Tibshirani R, Friedman J (2009) The elements of statistical learning: data mining, inference, and prediction. Springer, \nNew York\n\nHayes B (2007) How many ways can you spell v1@gra? Am Sci 95:298–302\nHooi B, Song HA, Beutel A, Shah N, Shin K, Faloutsos C (2016) Fraudar: bounding graph fraud in the face of camouflage. In: \n\nProceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pp 895–904\nHu J, Liang J, Dong S (2017) ibgp: a bipartite graph propagation approach for mobile advertising fraud detection. Mobile Inf \n\nSyst 2017:1–12\nJiang M, Cui P, Beutel A, Faloutsos C, Yang S (2014) Inferring strange behavior from connectivity pattern in social networks. In: \n\nPacific-Asia conference on knowledge discovery and data mining, pp 126–138\nLi Y, Sun Y, Contractor N (2017) Graph mining assisted semi-supervised learning for fraudulent cash-out detection. In: Pro-\n\nceedings of the 2017 IEEE/ACM international conference on advances in social networks analysis and mining 2017, pp \n546–553\n\nLiu J, Bier E, Wilson A, Guerra-Gomez JA, Honda T, Sricharan K, Gilpin L, Davies D (2016) Graph analysis for detecting fraud, \nwaste, and abuse in healthcare data. AI Mag 37:33–46\n\nLiu S, Hooi B, Faloutsos C (2017) Holoscope: topology-and-spike aware fraud detection. In: Proceedings of the 2017 ACM on \nconference on information and knowledge management, pp 1539–1548\n\nMcAfee LLC (2019) Economic impact of cybercrime report. https ://www.mcafe e.com/enter prise /en-us/solut ions/lp/econo \nmics-cyber crime .html. Accessed: 25 Apr 2018\n\nMercari Inc (2019) FY2019.6 Q3 Presentation Material. https ://about .merca ri.com/en/ir/libra ry/resul ts/. Accessed 1 Nov 2020\nMilo R, Shen-Orr S, Itzkovitz S, Kashtan N, Chklovskii D, Alon U (2002) Network motifs: simple building blocks of complex \n\nnetworks. Science 298:824–827\nMonamo P, Marivate V, Twala B (2016) Unsupervised learning for robust Bitcoin fraud detection. In: 2016 information security \n\nfor South Africa (ISSA), pp 129–134\nNewman M (2010) Networks: an introduction. Oxford University Press, Oxford\nPalla G, Derényi I, Farkas I, Vicsek T (2005) Uncovering the overlapping community structure of complex networks in nature \n\nand society. Nature 435:814–818\nPandit S, Chau DH, Wang S, Faloutsos C (2007) Netprobe: a fast and scalable system for fraud detection in online auction \n\nnetworks. In: Proceedings of the 16th international conference on world wide web, pp 201–210\nPedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O, Blondel M, Prettenhofer P, Weiss R, Dubourg V et al (2011) \n\nScikit-learn: machine learning in Python. J Mach Learn Res 12:2825–2830\nPhua C, Lee V, Smith K, Gayler R (2010) A comprehensive survey of data mining-based fraud detection research. Preprint arXiv \n\n:1009.6119\nPu C, Webb S (2006) Observed trends in spam construction techniques: a case study of spam evolution. In: CEAS, pp 104–112\nRadicchi F, Castellano C, Cecconi F, Loreto V, Parisi D (2004) Defining and identifying communities in networks. Proc Natl Acad \n\nSci USA 101:2658–2663\nRasheed J, Akram U, Malik AK (2018) Terrorist network analysis and identification of main actors using machine learning tech-\n\nniques. In: Proceedings of the 6th international conference on information technology: IoT and smart city, pp 7–12\nSavage D, Zhang X, Yu X, Chou P, Wang Q (2014) Anomaly detection in online social networks. Soc Netw 39:62–70\nSavage D, Wang Q, Zhang X, Chou P, Yu X (2017) Detection of money laundering groups: supervised learning on small net-\n\nworks. In: Workshops at the 31st AAAI conference on artificial intelligence, pp 43–49\nShchur O, Bojchevski A, Farghal M, Günnemann S, Saber Y (2018) Anomaly detection in car-booking graphs. In: 2018 IEEE \n\ninternational conference on data mining workshops (ICDMW), pp 604–607\nStrobl C, Boulesteix A-L, Zeileis A, Hothorn T (2007) Bias in random forest variable importance measures: illustrations, sources \n\nand a solution. BMC Bioinform 8:25\nŠubelj L, Furlan Š, Bajec M (2011) An expert system for detecting automobile insurance fraud using social network analysis. \n\nExpert Syst Appl 38:1039–1052\nUK Parliament: The Growing Threat of Online Fraud (2017). https ://old.parli ament .uk/busin ess/commi ttees /commi ttees -a-z/\n\ncommo ns-selec t/publi c-accou nts-commi ttee/inqui ries/parli ament -2017/growi ng-threa t-onlin e-fraud -17-19/publi catio \nns/. Accessed 1 Nov 2020\n\nVan Vlasselaer V, Bravo C, Caelen O, Eliassi-Rad T, Akoglu L, Snoeck M, Baesens B (2015) Apate: a novel approach for automated \ncredit card transaction fraud detection using network-based extensions. Decis Support Syst 75:38–48\n\nVan Vlasselaer V, Eliassi-Rad T, Akoglu L, Snoeck M, Baesens B (2016) Gotcha! network-based fraud detection for social security \nfraud. Manag Sci 63:3090–3110\n\nWang J-C, Chiu C-C (2008) Recommending trusted online auction sellers using social network analysis. Expert Syst Appl \n34:1666–1679\n\nWang Z, Gu S, Zhao X, Xu X (2018) Graph-based review spammer group detection. Knowl Inf Syst 55:571–597\nWest J, Bhattacharya M (2016) Intelligent financial fraud detection: a comprehensive review. Comput Secur 57:47–66\nYamamoto H, Sugiyama N, Toriumi F, Kashida H, Yamaguchi T (2019) Angels or demons? Classifying desirable heavy users and \n\nundesirable power sellers in online C2C marketplace. J Comput Soc Sci 2:315–329\nYanchun Z, Wei Z, Changhai Y (2011) Detection of feedback reputation fraud in Taobao using social network theory. In: 2011 \n\ninternational joint conference on service sciences, pp 188–192\n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nhttps://www.mcafee.com/enterprise/en-us/solutions/lp/economics-cybercrime.html\nhttps://www.mcafee.com/enterprise/en-us/solutions/lp/economics-cybercrime.html\nhttps://about.mercari.com/en/ir/library/results/\nhttp://arxiv.org/abs/1009.6119\nhttp://arxiv.org/abs/1009.6119\nhttps://old.parliament.uk/business/committees/committees-a-z/commons-select/public-accounts-committee/inquiries/parliament-2017/growing-threat-online-fraud-17-19/publications/\nhttps://old.parliament.uk/business/committees/committees-a-z/commons-select/public-accounts-committee/inquiries/parliament-2017/growing-threat-online-fraud-17-19/publications/\nhttps://old.parliament.uk/business/committees/committees-a-z/commons-select/public-accounts-committee/inquiries/parliament-2017/growing-threat-online-fraud-17-19/publications/\n\n\tDetecting problematic transactions in a consumer-to-consumer e-commerce network\n\tAbstract \n\tIntroduction\n\tMaterials and methods\n\tData\n\tNetwork analysis\n\tRandom forest classifier\n\n\tResults\n\tDescriptive statistics\n\tClassification of users\n\n\tDiscussion\n\tAcknowledgements\n\tReferences\n\n\n",
      "metadata_storage_path": "aHR0cHM6Ly90cmFpbmluZ2NhdGFsb2dzdG9yYWdlLmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlci9zNDExMDktMDIwLTAwMzMwLXgucGRm0",
      "metadata_content_type": "application/pdf",
      "metadata_author": " Shun Kodate ",
      "metadata_title": "Detecting problematic transactions in a consumer-to-consumer e-commerce network",
      "people": [
        "Shun Kodate",
        "Ryusuke Chiba",
        "Shunya Kimura3",
        "Naoki Masuda",
        "Anderson",
        "Kodate",
        "Bolton",
        "Hand",
        "Phua",
        "Abdallah",
        "West",
        "Bhattacharya",
        "Akoglu",
        "Pu",
        "Webb",
        "Hayes",
        "Bhowmick",
        "Hazarika",
        "Savage",
        "Šubelj",
        "Dreżewski",
        "Colladon",
        "Remondi",
        "Liu",
        "Shchur",
        "Van Vlasselaer",
        "Bhat",
        "Abulaish",
        "Jiang",
        "Hooi",
        "Rasheed",
        "Wang",
        "Chau",
        "Pandit",
        "Chiu",
        "Bangcharoensap",
        "Yanchun",
        "Van Vlas",
        "selaer",
        "Li",
        "Monamo",
        "Ferrara",
        "Hu",
        "Yan",
        "ini",
        "kouti",
        "vi",
        "vj",
        "sini",
        "souti",
        "Palla",
        "Milo",
        "Breiman",
        "Hastie",
        "Pedregosa",
        "Strobl",
        "Altmann",
        "Ci",
        "Yamamoto",
        "Shunya Kimura",
        ". Kodate",
        "Abdallah A",
        "Maarof MA",
        "Zainal A",
        "Akoglu L",
        "McGlohon M",
        "Faloutsos C",
        "Chandy R",
        "Tong H",
        "Koutra D",
        "Data Min Knowl Discov",
        "Altmann A",
        "Toloşi L",
        "Sander O",
        "Lengauer T",
        "Anderson R",
        "Barton C",
        "Böhme R",
        "Clayton R",
        "Van Eeten MJ",
        "Levi M",
        "Moore T",
        "Savage S",
        "Bangcharoensap P",
        "Kobayashi H",
        "Shimizu N",
        "Yamauchi S",
        "Murata T",
        "Barrat A",
        "Barthelemy M",
        "Pastor-Satorras R",
        "Vespignani A",
        "Bhat SY",
        "Abulaish M",
        "Bhowmick A",
        "Hazarika SM",
        "Bolton RJ",
        "Hand DJ",
        "Breiman L",
        "Friedman JH",
        "Olshen RA",
        "Stone CJ",
        "Chau DH",
        "Pandit S"
      ],
      "organizations": [
        "UK Parliament",
        "McAfee",
        "Commons",
        "iveco",
        "University",
        "Google LLC",
        "eBay",
        "Mercari",
        "Colladon",
        "WSPi",
        "Newman",
        "CYPi",
        "si/ki",
        "AUC",
        "Remondi",
        "ROC AUC",
        "Advanced Computing Research Centre",
        "University of",
        "RC",
        "NM",
        "Mercari, Inc.",
        "Top Global University",
        "Ministry of Education, Culture, Sports, Science and Technology",
        "MEXT",
        "Graduate School of Information Sciences",
        "Tohoku University",
        "Department of",
        "University of Bristol",
        "University at Buffalo",
        "Bioinfo",
        "Springer",
        "IEEE",
        "ACM",
        "Preprint arXiv",
        "Chapman & Hall"
      ],
      "locations": [
        "Buffalo",
        "Buffalo, NY",
        "USA",
        "Ferrara",
        "Japan",
        "US",
        "Yanchun",
        "United States",
        "Colladon",
        "Remondi",
        "Bristol",
        "UK",
        "Tokyo",
        "Department of",
        "NY",
        "Berlin",
        "Boca Raton"
      ],
      "keyphrases": [
        "Creative Commons Attribution 4.0 International License",
        "Computational social science Open Access",
        "other third party material",
        "eight local network indices",
        "various online transaction platforms",
        "Applied Network Science",
        "Creative Commons licence",
        "random forest classifiers",
        "Appl Netw Sci",
        "eight network indices",
        "present descriptive analysis",
        "credit card fraud",
        "Network analysis",
        "present study",
        "problematic transaction",
        "appropriate credit",
        "credit line",
        "online marketplaces",
        "Shun Kodate",
        "Ryusuke Chiba",
        "Shunya Kimura3",
        "Naoki Masuda",
        "rapid growth",
        "electronic transactions",
        "communi- cations",
        "dramatic speed",
        "daily lives",
        "UK Parliament",
        "recent era",
        "money laundering",
        "computer intrusion",
        "typical approach",
        "individual transactions",
        "traditional approach",
        "corresponding buyer",
        "several hundreds",
        "similar number",
        "focal node",
        "twelve features",
        "four types",
        "classification performance",
        "Machine learning",
        "author(s",
        "statutory regulation",
        "copyright holder",
        "RESEARCH Kodate",
        "Full list",
        "malicious users",
        "fraudulent users",
        "normal users",
        "online consumer",
        "intended use",
        "permitted use",
        "Fraud detection",
        "illegal items",
        "consumer marketplace",
        "egocentric networks",
        "author information",
        "orcid.org",
        "user profiles",
        "fraud activity",
        "Introduction",
        "tandem",
        "cybercrimes",
        "billions",
        "dollars",
        "year",
        "security",
        "society",
        "McAfee",
        "system",
        "Anderson",
        "dimension",
        "ranges",
        "plagiarism",
        "Abstract",
        "Providers",
        "behavior",
        "texts",
        "background",
        "frauds",
        "seller",
        "edge",
        "up",
        "connectivity",
        "neighbors",
        "aim",
        "Keywords",
        "article",
        "sharing",
        "adaptation",
        "distribution",
        "reproduction",
        "medium",
        "original",
        "source",
        "link",
        "changes",
        "images",
        "permission",
        "Correspondence",
        "buffalo",
        "4 Department",
        "Mathematics",
        "University",
        "USA",
        "dialog",
        "Page",
        "18Kodate",
        "13 million monthly active users",
        "credit card systems",
        "social security system",
        "various other fields",
        "online review forums",
        "large online consumer",
        "machine learning algorithms",
        "online social networks",
        "mobile phone network",
        "credit card transactions",
        "suspicious connectivity patterns",
        "online auction system",
        "fraud detec- tion",
        "fraud detection techniques",
        "online C2C marketplaces",
        "online auctions",
        "supervised learning",
        "telecommunica- tion",
        "transaction networks",
        "C2C) marketplace",
        "Standard practice",
        "transaction amount",
        "item category",
        "call duration",
        "call type",
        "geographical region",
        "transaction history",
        "advanced fraudsters",
        "Google LLC",
        "particular words",
        "anomalous behavior",
        "one jargon",
        "alternative way",
        "graph-theoretic quantities",
        "Dreżewski",
        "car- booking",
        "Van Vlasselaer",
        "cryptocurrency transaction",
        "accom- plices",
        "bipartite cores",
        "belief propagation",
        "133 billion yen",
        "1.2 billion USD",
        "quarter year",
        "previous studies",
        "anomalous users",
        "fraudulent samples",
        "health-care data",
        "empirical data",
        "data set",
        "statistical methods",
        "transaction frauds",
        "reputation frauds",
        "statistical classifier",
        "Exemplar features",
        "Computational",
        "decades",
        "Bolton",
        "Hand",
        "Phua",
        "Abdallah",
        "West",
        "Bhattacharya",
        "case",
        "day",
        "week",
        "address",
        "number",
        "calls",
        "age",
        "gender",
        "Akoglu",
        "eyes",
        "administrators",
        "authorities",
        "signature",
        "Webb",
        "Hayes",
        "Bhowmick",
        "Hazarika",
        "example",
        "authority",
        "drug",
        "idea",
        "nodes",
        "goods",
        "scores",
        "expectation",
        "insurance",
        "Šubelj",
        "Colladon",
        "Remondi",
        "Liu",
        "Shchur",
        "advertising",
        "Ferrara",
        "Abulaish",
        "Jiang",
        "Hooi",
        "Rasheed",
        "Wang",
        "Chau",
        "Pandit",
        "Chiu",
        "Bangcharoensap",
        "Yanchun",
        "Monamo",
        "reputations",
        "authors",
        "eBay",
        "Mercari",
        "Japan",
        "Many prior network-based fraud detection algorithms",
        "online C2C marketplace service",
        "one problematic sell",
        "local clustering coefficient",
        "local infor- mation",
        "two fraudulent users",
        "two normal users",
        "two indices",
        "sell probability",
        "local indices",
        "two users",
        "local features",
        "connected components",
        "Yan- chun",
        "commercial implementations",
        "various items",
        "United States",
        "Japanese market",
        "following types",
        "non-existing items",
        "medicinal supplies",
        "weighted network",
        "egocentric network",
        "eight indices",
        "trans- actions",
        "separate index",
        "three indices",
        "users’ network",
        "node vi",
        "node strength",
        "methods Data",
        "normal transactions",
        "lematic transactions",
        "Fictive transactions",
        "mean number",
        "focal user",
        "directed edge",
        "networks",
        "global",
        "communities",
        "k-cores",
        "Bhat",
        "Savage",
        "Others",
        "degree",
        "triangles",
        "Materials",
        "July",
        "January",
        "addition",
        "underwear",
        "medicine",
        "weapon",
        "perspective",
        "morality",
        "law",
        "crime",
        "Table",
        "buyer",
        "Figure",
        "Fig.",
        "edges",
        "pairs",
        "information",
        "Barrat",
        "direction",
        "SPi",
        "local clustering coefficient Ci",
        "Random forest classifier",
        "unweighted trian- gles",
        "other three indices",
        "one feedforward triangle",
        "one cyclic triangle",
        "local network",
        "triangle congregation",
        "other four",
        "one item",
        "k ini",
        "weighted version",
        "overlapping set",
        "three-node subnetworks",
        "work motifs",
        "natural interpretation",
        "three nodes",
        "unweighted triangles",
        "two triangles",
        "feedforward triangles",
        "cyclic triangles",
        "many users",
        "different directed",
        "particular neighbors",
        "Such neighbors",
        "bidirectional edges",
        "degree zero",
        "proportion",
        "kouti",
        "Examples",
        "vj",
        "contribution",
        "ki",
        "sini",
        "buys",
        "souti",
        "sells",
        "abundance",
        "undirected",
        "Newman",
        "increase",
        "community",
        "Radicchi",
        "Palla",
        "hypothesis",
        "extent",
        "concept",
        "mi",
        "Note",
        "Frequencies",
        "marketplace",
        "contrast",
        "index",
        "cycle",
        "CYPi",
        "CYi",
        "definition",
        "calculation",
        "weights",
        "random forest clas- sifier",
        "precision–recall (PR) curve",
        "random forest classifier",
        "ensemble learning method",
        "classifica- tion performance",
        "receiver operating characteristic",
        "two performance measures",
        "true positive rate",
        "false positive rate",
        "piecewise linear manner",
        "same test sample",
        "tive training samples",
        "ROC) curve",
        "ROC curve",
        "two sets",
        "same order",
        "single-tree positive",
        "tive samples",
        "test data",
        "positive probability",
        "multiple classifiers",
        "decision-tree classifiers",
        "best split",
        "triangle patterns",
        "Cyclic triangle",
        "three-node patterns",
        "directed triangles",
        "reciprocal edges",
        "fraudulent user",
        "fraudulent types",
        "precision range",
        "descending order",
        "horizontal axis",
        "vertical axis",
        "test samples",
        "training data",
        "fraudulent) samples",
        "terminal node",
        "negative probability",
        "candidate features",
        "decision trees",
        "Feedforward triangle",
        "300 trees",
        "ties",
        "Breiman",
        "Hastie",
        "scikit-learn",
        "Pedregosa",
        "overfitting",
        "basis",
        "replace",
        "ment",
        "fraction",
        "average",
        "count",
        "Five",
        "numbers",
        "figure",
        "The",
        "2",
        "100 random forest classifiers",
        "accurate binary classifier",
        "random number generator",
        "ten different permutations",
        "ten permutations",
        "different types",
        "PR curves",
        "grid search",
        "10-fold cross-validation",
        "maximum depth",
        "max_depth parameter",
        "other hyperparameters",
        "default values",
        "parameter optimization",
        "sampling bias",
        "sampling scheme",
        "Descriptive statistics",
        "survival probability",
        "frac- tion",
        "one transaction",
        "clear distinction",
        "large value",
        "The AUC",
        "lent users",
        "mal users",
        "minimum number",
        "seed number",
        "single set",
        "degree distribution",
        "unweighted degree",
        "AUC value",
        "permutation importance",
        "nal nodes",
        "scikit-learn version",
        "useful information",
        "user type",
        "training set",
        "performance measure",
        "degree ki",
        "normal",
        "area",
        "good",
        "Strobl",
        "Altmann",
        "method",
        "decrease",
        "tree",
        "integers",
        "split",
        "max_features",
        "n_estimators",
        "Results",
        "specified",
        "classification",
        "difference",
        "many",
        "fact",
        "≥",
        "Normal Fictive Underwear Medicine Weapon",
        "unweighted sell probability",
        "different user types",
        "less frequent transactions",
        "one sell transaction",
        "Seed user type",
        "large fraction",
        "smaller value",
        "characteristic behavior",
        "first column",
        "exclusive buyer",
        "fraudulent type",
        "seed users",
        "sell transactions",
        "c Strength",
        "specific neighbor",
        "average number",
        "Total number",
        "Table 1 Properties",
        "conditional mean",
        "Degree",
        "majority",
        "result",
        "ence",
        "median",
        "FFi",
        "samples",
        "peak",
        "smallest possible weighted sell probability",
        "smallest possible out-degree",
        "corresponding frac- tions",
        "tinct frequency distributions",
        "Survival probability",
        "cycle probability",
        "dashed lines",
        "two aspects",
        "larger value",
        "clear difference",
        "descriptive statistics",
        "ous section",
        "horizontal line",
        "Figure  5b",
        "Figure 5b",
        "second feature",
        "relationships",
        "buyers",
        "one",
        "transactions",
        "neighbor",
        "results",
        "analysis",
        "tendency",
        "Classification",
        "12 features",
        "b Survival probability",
        "Fig. 8 Cycle probability",
        "different fraud types",
        "moderate WSPi value",
        "cycle probability CYPi",
        "Fig. 7 Triangle congregation",
        "triangle congregation mi",
        "small value",
        "other types",
        "cyclic triangle",
        "real number",
        "lent type",
        "thin line",
        "AUC values",
        "previous section",
        "strong contributors",
        "neigh- bors",
        "similar classification",
        "third feature",
        "fourth feature",
        "fifth feature",
        "sixth feature",
        "seventh feature",
        "eighth feature",
        "ninth feature",
        "tenth feature",
        "eleventh feature",
        "twelfth feature",
        "focal users",
        "The ROC",
        "thick lines",
        "random classification",
        "Figure 10a",
        "ROC curves",
        "one triangle",
        "highest importance",
        "100 lines",
        "9 ROC",
        "ini",
        "feedforward",
        "100 classifiers",
        "uniformly",
        "possibility",
        "Fictive Underwear Medicine",
        "global transaction networks",
        "practical advan- tage",
        "present fraud-detection method",
        "Other future tasks",
        "mean AUC values",
        "higher classification performance",
        "The AUC values",
        "Table 2 AUC values",
        "performance scores",
        "The bars",
        "future work",
        "easy problem",
        "classification test",
        "error bars",
        "standard deviation",
        "positive samples",
        "clas- sifier",
        "smaller amount",
        "arbitrary choice",
        "present results",
        "ferent types",
        "fraudulent transactions",
        "resale tickets",
        "particular type",
        "nonlinear classifier",
        "100 different sets",
        "different platforms",
        "large number",
        "network features",
        "fraud category",
        "nine features",
        "AUC.",
        "9 features",
        "ROC",
        "reduction",
        "WSPi",
        "More",
        "half",
        "reason",
        "subdata",
        "subsampling",
        "negative",
        "train",
        "Discussion",
        "distinguished",
        "terms",
        "practice",
        "One",
        "generalizability",
        "pornography",
        "items",
        "unknown",
        "desirable non-professional frequent sellers",
        "Prior network-based fraud detection",
        "undesirable professional sellers",
        "single positive category",
        "single broad category",
        "Table 3 AUC values",
        "present local method",
        "Global network properties",
        "similar classification performance",
        "local network properties",
        "other behavioral data",
        "global network structure",
        "local network features",
        "prior study",
        "local properties",
        "entire network",
        "local quantities",
        "local ones",
        "relevant question",
        "connected component",
        "betweenness centrality",
        "dense subgraphs",
        "high classification",
        "computation speed",
        "memory requirement",
        "other methods",
        "industrial contexts",
        "seed node",
        "computational reasons",
        "same marketplace",
        "ROC AUC",
        "specific types",
        "problematic transactions",
        "individual nodes",
        "10 features",
        "7 features",
        "quantity",
        "suspiciousness",
        "users",
        "Hu",
        "time",
        "accuracy",
        "works",
        "angles",
        "Yamamoto",
        "item",
        "purchases",
        "Advanced Computing Research Centre",
        "Top Global University Project",
        "other online marketplaces",
        "Receiver operating characteristic",
        "US market data",
        "present research",
        "Fur- thermore",
        "new questions",
        "predic- tion",
        "Precision–recall",
        "computational facilities",
        "Shunya Kimura",
        "final approval",
        "financial support",
        "S. Kodate",
        "present paper",
        "summary statistics",
        "bona fide",
        "complete dataset",
        "Ethics approval",
        "Competing interests",
        "Author details",
        "1 Graduate School",
        "Tohoku University",
        "users’ behavior",
        "individual users",
        "private information",
        "users’ privacy",
        "Information Sciences",
        "Authors’ contributions",
        "last authors",
        "third authors",
        "web platform",
        "different data",
        "similar data",
        "final manuscript",
        "transaction data",
        "hashed data",
        "present manuscript",
        "Bristol BS8",
        "time information",
        "topics",
        "Abbreviations",
        "C2C",
        "consumer",
        "AUC",
        "Area",
        "curve",
        "Acknowledgements",
        "methodology",
        "funding",
        "NM",
        "publication",
        "part",
        "Ministry",
        "Sports",
        "Technology",
        "MEXT",
        "Availability",
        "materials",
        "condition",
        "collaborators",
        "first",
        "second",
        "employees",
        "company",
        "figures",
        "tables",
        "others",
        "person",
        "researcher",
        "access",
        "reasons",
        "attempt",
        "tos",
        "Article",
        "consent",
        "conflict",
        "analyses",
        "interpretation",
        "bias",
        "merit",
        "Sendai",
        "Engineering",
        "Tokyo",
        "Buffalo",
        "Two step graph-based semi-supervised learning",
        "J Netw Comput Appl",
        "Boca Raton Chau DH",
        "Graph based anomaly detection",
        "Data Min Knowl Discov",
        "online auction fraud detection",
        "Fraud detection system",
        "Opinion fraud detection",
        "Statistical fraud detection",
        "Van Eeten MJ",
        "mail spam filtering",
        "feature importance measure",
        "Böhme R",
        "Toloşi L",
        "social networks analysis",
        "complex weighted networks",
        "Joint European conference",
        "Acad Sci USA",
        "machine learning",
        "online reviews",
        "online auctioneers",
        "social media",
        "weighted graphs",
        "Permutation importance",
        "Stat Sci",
        "data mining",
        "Pacific-Asia conference",
        "Chandy R",
        "AAAI conference",
        "Anderson R",
        "Clayton R",
        "Pastor-Satorras R",
        "international conference",
        "Akoglu L",
        "Breiman L",
        "Data-Enabled Science",
        "Engineering Program",
        "Abdallah A",
        "Maarof MA",
        "Zainal A",
        "McGlohon M",
        "Faloutsos C",
        "knowledge discovery",
        "network effects",
        "7th international",
        "Tong H",
        "Koutra D",
        "Altmann A",
        "Sander O",
        "Lengauer T",
        "Barton C",
        "Levi M",
        "Moore T",
        "Savage S",
        "information security",
        "Bangcharoensap P",
        "Kobayashi H",
        "Yamauchi S",
        "Murata T",
        "Barrat A",
        "Barthelemy M",
        "Vespignani A",
        "Proc Natl",
        "Bhat SY",
        "Abulaish M",
        "Community-based features",
        "Bhowmick A",
        "Hazarika SM",
        "Preprint arXiv",
        "Bolton RJ",
        "Hand DJ",
        "Random forests",
        "Friedman JH",
        "Olshen RA",
        "Stone CJ",
        "regression trees",
        "Pandit S",
        "fraudulent personalities",
        "5 Computational",
        "NY",
        "References",
        "survey",
        "Oddball",
        "anomalies",
        "weblogs",
        "description",
        "Bioinfo",
        "cost",
        "cybercrime",
        "economics",
        "privacy",
        "Springer",
        "Berlin",
        "Shimizu",
        "databases",
        "architecture",
        "spammers",
        "IEEE/ACM",
        "advances",
        "ASONAM",
        "techniques",
        "trends",
        "Chapman",
        "Hall",
        "principles",
        "23"
      ],
      "masked_text": "\nDetecting problematic transactions \nin a consumer‑to‑consumer e‑commerce \nnetwork\n***********1,2, *************3, ************** and ************2,4,5* \n\nIntroduction\nIn tandem with the rapid growth of online and electronic transactions and communi-\ncations, fraud is expanding at a dramatic speed and penetrates our ***** lives. Fraud \nincluding cybercrimes costs billions of dollars per year and threatens the security of our \nsociety (************* ****; ****** ****). In particular, in the recent era where online \nactivity dominates, attacking a system is not too costly, whereas defending the system \nagainst fraud is costly (******** et al. ****). The dimension of fraud is vast and ranges \nfrom credit card fraud, money laundering, computer intrusion, to plagiarism, to name a \nfew.\n\nAbstract \n\nProviders of online marketplaces are constantly combatting against problematic \ntransactions, such as selling illegal items and posting fictive items, exercised by some \nof their *****. A typical approach to detect fraud activity has been to analyze registered \nuser profiles, ****’s behavior, and texts attached to individual transactions and the ****. \nHowever, this traditional approach may be limited because malicious users can easily \nconceal their information. Given this background, network indices have been exploited \nfor detecting frauds in various online transaction platforms. In the present study, we \nanalyzed networks of users of an online consumer-to-consumer marketplace in which \na seller and the corresponding buyer of a transaction are connected by a directed \nedge. We constructed egocentric networks of each of several hundreds of fraudulent \n***** and those of a similar number of normal *****. We calculated eight local network \nindices based on up to connectivity between the neighbors of the focal node. Based \non the present descriptive analysis of these network indices, we fed twelve features \nthat we constructed from the eight network indices to random forest classifiers with \nthe aim of distinguishing between normal users and fraudulent users engaged in each \none of the four types of problematic transactions. We found that the classifier accu-\nrately distinguished the fraudulent users from normal users and that the classification \nperformance did not depend on the type of problematic transaction.\n\nKeywords: Network analysis, Machine learning, Fraud detection, Computational social \nscience\n\nOpen Access\n\n© **********(s) ****. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \n******(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the ****************. To view a copy of this licence, visit http://\ncreat iveco mmons .org/licen ses/by/4.0/.\n\nRESEARCH\n\n****** et al. Appl Netw Sci            (****) 5:90  \n******************************‑020‑00330‑x Applied Network Science\n\n*Correspondence:   \n******************** \n4 Department \nof Mathematics, University \nat Buffalo, Buffalo, NY \n**********, USA\nFull list of author information \nis available at the end of the \narticle\n\n************************************\n*******************************************\n*******************************************\n********************************************************************&domain=pdf\n\n\nPage 2 of 18Kodate et al. Appl Netw Sci            (****) 5:90 \n\nComputational and statistical methods for detecting and preventing fraud have been \ndeveloped and implemented for decades (****** and **** ****; **** et  al. ****; \n******** et al. ****; **** and ************ ****). Standard practice for fraud detec-\ntion is to employ statistical methods including the case of machine learning algorithms. \nIn particular, when both fraudulent and non-fraudulent samples are available, one can \nconstruct a classifier via supervised learning (****** and Hand ****; **** et al. ****; \n******** et al. ****; **** and ************ ****). Exemplar features to be fed to such a \nstatistical classifier include the transaction amount, day of ********, item category, and \n****’s address for detecting frauds in credit card systems, number of calls, call duration, \ncall type, and ****’s ***, gender, and geographical region in the case of telecommunica-\ntion, and user profiles and transaction history in the case of online auctions (******** \net al. ****).\n\nHowever, many of these features can be easily faked by advanced ********** (Akoglu \net al. ****; ********** ****). Furthermore, fraudulent users are adept at escaping the \neyes of the administrators or authorities that would detect the usage of particular words \nas a signature of anomalous behavior (** and **** ****; ***** ****; Bhowmick and \nHazarika ****). For example, if the authority discovers that one jargon means a drug, \nthen fraudulent ***** *** easily switch to another jargon to confuse the authority.\n\nNetwork analysis is an alternative way to construct features and is not new to fraud \ndetection techniques (****** et al. ****; ****** et al. ****). The idea is to use connec-\ntivity between nodes, which are usually users or goods, in the given data and calculate \ngraph-theoretic quantities or scores that characterize nodes. These methods stand on \nthe expectation that anomalous users show connectivity patterns that are distinct from \nthose of normal ***** (****** et al. ****). Network analysis has been deployed for fraud \ndetection in insurance (****** et  al. ****), money laundering (********* et  al. ****; \n******** and Remondi ****; ****** et al. ****), health-care data (*** et al. ****), car-\nbooking (****** et al. ****), a social security system (************** et al. ****), mobile \nadvertising (** et al. ****), a mobile phone network (******* et al. ****), online social \nnetworks (**** and ******** ****; ***** et  al. ****; **** et  al. ****; ******* et  al. \n****), online review forums (****** et al. ****; *** et al. ****; **** et al. ****), online \nauction or marketplaces (**** et  al. ****; ****** et  al. ****; **** and **** ****; \n************** et  al. ****; ******* et  al. ****), credit card transactions (*********\n****** et al. ****; ** et al. ****), cryptocurrency transaction (****** et al. ****), and \nvarious other fields (****** et al. ****). For example, fraudulent ***** and their accom-\nplices were shown to form approximately bipartite cores in a network of users to inflate \ntheir reputations in an online auction system (**** et al. ****). Then, the ******* pro-\nposed an algorithm based on a belief propagation to detect such suspicious connectivity \npatterns. This method has been proven to be also effective on empirical data obtained \nfrom **** (****** et al. ****).\n\nIn the present study, we analyze a data set obtained from a large online consumer-to-\nconsumer (C2C) marketplace, *******, operating in Japan and the US. They are the larg-\nest C2C marketplace in Japan, in which, as of ****, there are 13 million ******* active \nusers and 133 billion yen (approximately 1.2 billion USD) transactions per quarter year \n(******* ****). Note that we analyze transaction frauds based on transaction networks \nof *****, which contrasts with previous studies of online C2C marketplaces that looked \n\n\n\nPage 3 of 18Kodate et al. Appl Netw Sci            (****) 5:90  \n\nat reputation frauds (**** et al. ****; ****** et al. ****; **** and **** ****; Yanchun \net  al. ****). Many prior network-based fraud detection algorithms used global infor-\nmation about networks, such as connected components, communities, betweenness, \nk-cores, and that determined by belief propagation (**** et al. ****; ****** et al. ****; \n**** and **** ****; ****** et  al. ****; ****** et  al. ****; **** and Abulaish ****; \n******* et al. ****; ***** et al. ****; ************** et al. ****; ********* et al. ****; \n************** et al. ****; **** et al. ****; *** et al. ****; ************** et al. ****; \n******** and Remondi ****; ** et al. ****; ** et al. ****; *** et al. ****; ****** et al. \n****; ****** et al. ****; ******* et al. ****; **** et al. ****). Others used local infor-\nmation about the users’ network, such as the degree, the number of triangles, and the \nlocal clustering coefficient (**** et al. ****; ****** et al. ****; ****** et al. ****; ***-\n**** et al. ****; **** and Abulaish ****; ************** et al. ****; ********* et al. \n****; ****** et al. ****; ************** et al. ****; ******** and Remondi ****). We \nwill focus on local features of *****, i.e., features of a node that can be calculated from \nthe connectivity of the **** and the connectivity between ********* of the ****. This is \nbecause local features are easier and faster to calculate and thus practical for commercial \nimplementations.\n\nMaterials and methods\nData\n\nMercari is an online C2C marketplace service, where ***** trade various items among \nthemselves. The service is operating in Japan and the United States. In the present study, \nwe used the data obtained from the Japanese market ***********************************. In addition to normal transactions, we focused on the following types of prob-\nlematic transactions: fictive, underwear, medicine, and weapon. Fictive transactions are \ndefined as selling non-existing items. Underwear refers to transactions of used under-\nwear; they are prohibited by the service from the perspective of morality and hygiene. \nMedicine refers to transactions of medicinal supplies, which are prohibited by the law. \nWeapon refers to transactions of weapons, which are prohibited by the service because \nthey may lead to crime. The number of sampled ***** of each type is shown in Table 1.\n\nNetwork analysis\n\nWe examine a directed and weighted network of users in which a user corresponds to a \nnode and a transaction between two ***** represents a directed edge. The weight of the \nedge is equal to the number of transactions between the ****** and the *****. We con-\nstructed egocentric networks of each of several hundreds of normal users and those of \nfraudulent *****, i.e., those engaged in at least one problematic sell. Figure 1 shows the \negocentric networks of two normal users (Fig. 1a, b) and those of two fraudulent ***** \ninvolved in selling a fictive item (Fig. 1c, d). The egocentric network of either a normal or \nfraudulent user contained the nodes neighboring the focal user, edges between the focal \n**** and these *********, and edges between the pairs of these *********.\n\nWe calculated eight indices for each focal node. They are local indices in the mean-\ning that they require the information up to the connectivity among the neighbors of the \nfocal node.\n\n\n\nPage 4 of ******** et al. Appl Netw Sci            (****) 5:90 \n\nFive out of the eight indices use only the information about the connectivity of the focal \nnode. The degree ki of node vi is the number of its neighbors. The node strength  (****** \net al. ****) (i.e., weighted degree) of node vi , denoted by si , is the number of transactions in \nwhich vi is involved. Using these two indices, we also considered the mean number of trans-\nactions per ********, i.e., si/ki , as a separate index. These three indices do not use informa-\ntion about the direction of edges.\n\nThe sell probability of node vi , denoted by SPi , uses the information about the direction of \nedges and defined as the proportion of the vi ’s neighbors for which vi acts as seller. Precisely, \nthe sell probability is given by\n\n(1)SPi =\nkouti\n\nk ini + kouti\n\n,\n\nFig. 1 Examples of egocentric networks. a, b Egocentric networks of arbitrarily selected two normal *****. c, \nd Egocentric networks of arbitrarily selected two fraudulent users involved in selling a fictive item\n\n a b O- ****** O C 0 O ************* \n\n\n\nPage 5 of ******** et al. Appl Netw Sci            (****) 5:90  \n\nwhere k ini  is vi ’s in-degree (i.e., the number of neighbors from whom vi bought at least \none item) and kouti  is vi ’s out-degree (i.e., the number of neighbors to whom vi sold at \nleast one item). It should be noted that, if vi acted as both seller and buyer towards vj , the \ncontribution of vj to both in- and out-degree of vi is equal to one. Therefore, k ini + kouti  is \nnot equal to ki in general.\n\nThe weighted version of the sell probability, denoted by WSPi , is defined as\n\nwhere sini  is node vi ’s weighted in-degree (i.e., the number of buys) and souti  is vi ’s weighted \nout-degree (i.e., the number of sells).\n\nThe other three indices are based on triangles that involve the focal node. The local \nclustering coefficient Ci quantifies the abundance of undirected and unweighted triangles \naround vi (****** ****). It is defined as the number of undirected and unweighted trian-\ngles including vi divided by ki(ki − 1)/2 . The local clustering coefficient Ci ranges between \n0 and 1.\n\nWe hypothesized that triangles contributing to an increase in the local clustering coef-\nficient are localized around particular neighbors of node vi . Such neighbors together with vi \n*** form an overlapping set of triangles, which may be regarded as a community (******** \net al. ****; ***** et al. ****). Therefore, our hypothesis implies that the extent to which the \nfocal node is involved in communities should be different between normal and fraudulent \n*****. To quantify this concept, we introduce the so-called triangle congregation, denoted \nby mi . It is defined as the extent to which two triangles involving vi share another node and \nis given by\n\nwhere Tri = Ciki(ki − 1)/2 is the number of triangles involving vi . Note that mi ranges \nbetween 0 and 1.\n\nFrequencies of different directed three-node subnetworks, conventionally known as net-\nwork motifs (**** et al. ****), *** distinguish between normal and fraudulent *****. In \nparticular, among triangles composed of directed edges, we hypothesized that feedforward \ntriangles (Fig. 2a) should be natural and that cyclic triangles (Fig. 2b) are not. We hypoth-\nesized so because a natural interpretation of a feedforward triangle is that a node with out-\ndegree two tends to serve as seller while that with out-degree zero tends to serve as buyer \nand there are many such nodes that use the marketplace mostly as buyer or seller but not \nboth. In contrast, an abundance of cyclic triangles *** imply that relatively many ***** use \nthe marketplace as both ***** and ******. We used the index called the cycle probability, \ndenoted by CYPi , which is defined by\n\nwhere FFi and CYi are the numbers of feedforward triangles and cyclic triangles to which \nnode vi belongs. The definition of FFi and CYi , and hence CYPi , is valid even when the \n\n(2)WSPi =\nsouti\n\nsini + souti\n\n,\n\n(3)mi =\n(Number of pairs of triangles involving vi that share another node)\n\nTri(Tri − 1)/2\n,\n\n(4)CYPi =\nCYi\n\nFFi + CYi\n,\n\n\n\nPage 6 of 18Kodate et al. Appl Netw Sci            (****) 5:90 \n\ntriangles involving vi have bidirectional edges. In the case of Fig. 2c, for example, any of \nthe three nodes contains one feedforward triangle and one cyclic triangle. The other four \ncases in which bidirectional edges are involved in triangles are shown in Fig. **–g. In the \ncalculation of CYPi , we ignored the weights of edges.\n\nRandom forest classifier\n\nTo classify users into normal and fraudulent users based on their local network proper-\nties, we employed a random forest classifier (******* ****; ******* et al. ****; ****** \net  al. ****) implemented in scikit-learn (********* et  al. ****). It uses an ensemble \nlearning method that combines multiple classifiers, each of which is a decision tree, \nbuilt from training data and classifies test data avoiding overfitting. We combined 300 \ndecision-tree classifiers to construct a random forest classifier. Each decision tree is con-\nstructed on the basis of training samples that are randomly subsampled with replace-\nment from the set of all the training samples. To compute the best split of each node \nin a tree, one randomly samples the ********* features from the set of all the features. \nThe probability that a test sample is positive in a tree is estimated as follows. Consider \nthe terminal node in the tree that a test sample eventually reaches. The fraction of posi-\ntive training samples at the terminal node gives the probability that the test sample is \nclassified as positive. One minus the positive probability gives the negative probability \nestimated for the same test sample. The positive or negative probability for the random \nforest classifier is obtained as the average of single-tree positive or negative probability \nover all the 300 trees. A sample is classified as positive by the random forest classifier if \nthe positive probability is larger than 0.5, otherwise classified as negative.\n\nWe split samples of each type into two sets such that 75% and 25% of the samples of \neach type are assigned to the training and test samples, respectively. There were more \n\ncyclicfeedforward feedforward: 1\ncyclic: 1\n\nfeedforward: 2\ncyclic: 0\n\nfeedforward: 3\ncyclic: 1\n\nfeedforward: 6\ncyclic: 2\n\na b c d\n\nf g\n\nfeedforward: 2\ncyclic: 0\n\ne\n\nFig. 2 Directed triangle patterns and their count. a Feedforward triangle. b Cyclic triangle. c– g Five \nthree-node patterns that contain directed triangles and reciprocal edges. The numbers shown in the figure \nrepresent the number of feedforward or cyclic triangles to which each three-node pattern contributes\n\n\n\nPage 7 of ******** et al. Appl Netw Sci            (****) 5:90  \n\nnormal ***** than any type of fraudulent ****. Therefore, to balance the number of \nthe negative (i.e., normal) and positive (i.e., fraudulent) samples, we uniformly ran-\ndomly subsampled the negative samples (i.e., under-sampling) such that the number \nof the samples is the same between the normal and fraudulent types in the training \nset. Based on the training sample constructed in this manner, we built each of the 300 \ndecision trees and hence a random forest classifier. Then, we examined the classifica-\ntion performance of the random forest classifier on the set of test samples.\n\nThe true positive rate, also called the recall, is defined as the proportion of the posi-\ntive samples (i.e., fraudulent *****) that the random forest classifier correctly classifies \nas positive. The false positive rate is defined as the proportion of the negative samples \n(i.e., normal *****) that are incorrectly classified as positive. The precision is defined \nas the proportion of the truly positive samples among those that are classified as posi-\ntive. The true positive rate, false positive rate, and precision range between 0 and 1.\n\nWe used the following two performance measures for the random forest classifier. \nTo draw the receiver operating characteristic (ROC) curve for a random forest clas-\nsifier, one first arranges the test samples in descending order of the estimated prob-\nability that they are positive. Then, one plots each test sample, with its false positive \nrate on the horizontal axis and the true positive rate on the vertical axis. By connect-\ning the test samples in a piecewise linear manner, one obtains the ROC curve. The \nprecision–recall (PR) curve is generated by plotting the samples in the same order in \n[0, 1]2 , with the recall on the horizontal axis and the precision on the vertical axis. For \nan accurate binary classifier, both ROC and PR curves visit near (x, y) = (0, 1) . There-\nfore, we quantify the performance of the classifier by the area under the curve (AUC) \nof each curve. The AUC ranges between 0 and 1, and a large value indicates a good \nperformance of the random forest classifier.\n\nTo calculate the importance of each feature in the random forest classifier, we \nused the permutation importance (****** et al. ****; ******* et al. ****). With this \nmethod, the importance of a feature is given by the decrease in the performance of \nthe trained classifier when the feature is randomly permuted among the test samples. \nA large value indicates that the feature considerably contributes to the performance \nof the classifier. To calculate the permutation importance, we used the AUC value of \nthe ROC curve as the performance measure of a random forest classifier. We com-\nputed the permutation importance of each feature with ten different permutations \nand adopted the average over the ten permutations as the importance of the feature.\n\nWe optimized the parameters of the random forest classifier by a grid search with \n10-fold cross-validation on the training set. For the maximum depth of each tree (i.e., \nthe max_depth parameter in scikit-learn), we explored the integers between 3 and 10. \nFor the number of candidate features for each split (i.e., max_features), we explored \nthe integers between 3 and 6. For the minimum number of samples required at termi-\nnal nodes (i.e., min_samples_leaf ), we explored 1, 3, and 5. As mentioned above, the \nnumber of trees (i.e., n_estimators) was set to 300. The seed number for the random \nnumber generator (i.e., random_state) was set to 0. For the other hyperparameters, \nwe used the default values in scikit-learn version 0.22. In the parameter optimization, \nwe evaluated the performance of the random forest classifier with the AUC value of \nthe ROC curve measured on a single set of training and test samples.\n\n\n\nPage 8 of ******** et al. Appl Netw Sci            (****) 5:90 \n\nTo avoid sampling bias, we built 100 random forest classifiers, trained each classifier, \nand tested its performance on a randomly drawn set of train and test samples, whose \nsampling scheme was described above.\n\nResults\nDescriptive statistics\n\nThe survival probability of the degree (i.e., a fraction of nodes whose degree is larger \nthan a specified value) is shown in Fig. 3a for each user type. Approximately 60% of the \nnormal ***** have degree ki = 1 , whereas the fraction of the ***** with ki = 1 is approxi-\nmately equal to 2% or less for any type of fraudulent **** (Table 1). Therefore, we expect \nthat whether ki = 1 or ki ≥ 2 gives useful information for distinguishing between normal \nand fraudulent *****. The degree distribution at ki ≥ ***** provide further information \nuseful for the classification. The survival probability of the degree distribution condi-\ntioned on ki ≥ 2 for the different types of users is shown in Fig. 3b. The figure suggests \nthat the degree distribution is systematically different between the normal and fraudu-\nlent *****. However, we consider that the difference is not as clear-cut as that in the frac-\ntion of ***** having ki = 1 (Table 1).\n\nThe survival probability of the node strength (i.e., weighted degree) is shown in Fig. 3c \nfor each user type. As in the case for the unweighted degree, we found that many nor-\nmal users, but not fraudulent users, have si = 1 . In fact, the number of the normal users \nwith si = 1 is equal to those with ki = 1 (Table 1), implying that all normal ***** with \nki = 1 participated in just one transaction. In contrast, no user had si = 1 for any type \nof fraudulent ****. The survival probability of the node strength conditioned on si ≥ 2 \napparently does not show a clear distinction between the normal and fraudulent users \n(Fig. **, Table 1).\n\na b\n\nc d\n\nFig. 3 Survival probability of the degree for each **** type. a Degree (i.e., ki ) for all nodes. b Degree for the \nnodes with ki ≥ 2 . c Strength (i.e., si ) for all nodes. d Strength for the nodes with si ≥ 2\n\n\n\nPage 9 of ******** et al. Appl Netw Sci            (****) 5:90  \n\nThe distribution of the average number of transactions per edge, i.e., si/ki , is shown \nin Fig. 4a. We found that a majority of normal users have si/ki = 1 . This result indicates \nthat a large fraction of normal users is engaged in just one transaction per neighbor \n(Table 1). This result is consistent with the fact that approximately 60% of the normal \nusers have ki = si = 1 . In contrast, many of any type of fraudulent users have si/ki > 1 . \nHowever, they tend to have a smaller value of si/ki than the normal *****. This differ-\nence is more noticeable when we discraded the ***** with si/ki = 1 (Fig. 4b, Table 1). \nTherefore, less frequent transactions with a specific neighbor seem to be a characteristic \nbehavior of fraudulent *****.\n\nThe distribution of the unweighted sell probability for the different user types is \nshown in Fig.  5a. The distribution for the normal users is peaked ******** and 1, \n\nTable 1 Properties of different types of users\n\nIn the first column, Mean ( A | B ), for example, represents the mean of A conditioned on B. Unless the first column mentions \nthe conditional mean, median, or the number of transactions, the numbers reported in the table represent the number of \nusers\n\nSeed user type Normal Fictive Underwear Medicine Weapon\n\nNumber of seed users 999 440 468 469 416\n\nNumber of transactions \ninvolving the seed user\n\n151,021 66,215 151,278 92,497 81,970\n\nTotal number of transactions 27,683,860 850,739 2,325,898 925,361 533,963\n\nki = 1 587 (58.8%) 8 (1.8%) 3 (0.6%) 2 (0.4%) 5 (1.2%)\n\nMean ( ki | ki ≥ 2) 195.0 138.3 297.8 184.2 179.7\n\nMedian ( ki | ki ≥ 2) 77.5 61.0 170.0 97.0 86.0\n\nsi = 1 587 (58.8%) 8 (1.8%) 3 (0.6%) 2 (0.4%) 5 (1.2%)\n\nMean ( si | si ≥ 2) 365.1 153.3 325.3 198.1 199.4\n\nMedian ( si | si ≥ 2) 89.0 66.5 175.0 100.0 90.0\n\nsi ≥ ***************** 411\n\nsi/ki = 1 97 (23.5%) 97 (22.5%) 86 (18.5%) 156 (33.4%) 121 (29.4%)\n\nMean ( si/ki | si/ki > 1) 1.413 1.135 1.055 1.066 1.092\n\nMedian ( si/ki | si/ki > 1) 1.124 1.059 1.03 1.031 1.055\n\nki ≥ ***************** 411\n\nSPi = 1 157 (38.1%) 15 (3.5%) 21 (4.5%) 16 (3.4%) 17 (4.1%)\n\nk\nout\ni\n\n= 1 118 (28.6%) 21 (4.9%) 2 (0.4%) 2 (0.4%) 9 (2.2%)\n\nsi ≥ ***************** 411\n\nWSPi = 1 157 (38.1%) 15 (3.5%) 21 (4.5%) 16 (3.4%) 17 (4.1%)\n\ns\nout\ni\n\n= 1 118 (28.6%) 14 (3.2%) 2 (0.4%) 2 (0.4%) 9 (2.2%)\n\nki ≥ ***************** 411\n\nCi = 0 118 (28.6%) 152 (35.2%) 108 (23.2%) 154 (33.0%) 128 (31.1%)\n\nMean ( Ci | Ci > 0) 8.554× 10\n−3\n\n8.348× 10\n−3 9.500× 10\n\n−4\n2.231× 10\n\n−3\n3.810× 10\n\n−3\n\n****** ( Ci | Ci > 0) 2.411× 10\n−3\n\n2.039× 10\n−3 5.288× 10\n\n−4\n6.494× 10\n\n−4\n1.337× 10\n\n−3\n\nTri ≥ 2 262 241 317 251 244\n\nmi = 0 17 (6.5%) 27 (11.2%) 54 (17.0%) 44 (17.5%) 32 (13.1%)\n\nmi = 1 12 (4.6%) 9 (3.7%) 4 (1.3%) 6 (2.4%) 11 (4.5%)\n\nMean ( mi | mi > 0) 8.554× 10\n−3\n\n8.348× 10\n−3 9.500× 10\n\n−4\n2.231× 10\n\n−3\n3.810× 10\n\n−3\n\nMedian ( mi | mi > 0) 2.411× 10\n−3\n\n2.039× 10\n−3 5.288× 10\n\n−4\n6.494× 10\n\n−4\n1.337× 10\n\n−3\n\nFFi + CYi ≥ 1 294 280 357 313 283\n\nCYPi = 0 234 (79.6%) 188 (67.1%) 222 (62.2%) 227 (72.5%) 202 (71.4%)\n\nMean ( CYPi | **** > 0) 1.987× 10\n−2\n\n7.367× 10\n−2\n\n6.739× 10\n−2\n\n8.551× 10\n−2\n\n5.544× 10\n−2\n\nMedian ( CYPi | **** > 0) 1.521× 10\n−2\n\n4.481× 10\n−2\n\n3.396× 10\n−2\n\n3.822× 10\n−2\n\n3.618× 10\n−2\n\n\n\nPage 10 of ******** et al. Appl Netw Sci            (****) 5:90 \n\nindicating that a relatively large fraction of normal users is almost exclusive buyer or \nseller. Note that, by definition, the sell probability is at least 1/(k ini + kouti ) because our \nsamples are *******. Therefore, a peak around the sell probability of zero implies that \nthe users probably have no or few sell transactions apart from the one sell transaction \nbased on which the ***** have been sampled as seller. In contrast, the distribution \nfor any fraudulent type is relatively flat. Figure  5b shows the relationships between \nthe unweighted sell probability and the degree. On the dashed line in Fig. 5b, the sell \nprobability is equal to 1/(k ini + kouti ) , indicating that the node has kouti = 1 , which is \nthe smallest possible out-degree. The users on this line were buyers in all but one \n\na b\n\nFig. 4 Survival probability of the average number of transactions per ********. a si/ki for all nodes. b si/ki for \nthe nodes with si/ki > 1\n\na b\n\nc d\n\nFig. 5 Sell probability for each user type. a Distribution of the unweighted sell probability. b Relationship \nbetween the degree and the unweighted sell probability. c Distribution of the weighted sell probability. d \nRelationship between the node strength and the weighted sell probability. The dashed lines in b, d indicate \n1/(k in\n\ni\n+ k\n\nout\ni\n\n) and 1/(sin\ni\n+ s\n\nout\ni\n\n) , respectively\n\n\n\nPage 11 of 18Kodate et al. Appl Netw Sci            (****) 5:90  \n\ntransaction. Figure 5b indicates that a majority of such users are normal as opposed \nto fraudulent *****, which is quantitatively confirmed in Table 1. We also found that \nmost of the normal users were either on the horizontal line with the sell probability \nof one (38.1% of the normal users with ki ≥ 2 ; see Table 1 for the corresponding frac-\ntions of normal ***** with ki = 1 ) or on the dashed line (28.6%). This is not the case \nfor any type of fraudulent **** (Table 1).\n\nThe distribution of the weighted sell probability for the different user types and the \nrelationships between the weighted sell probability and the node strength are shown \nin Fig.  5c, d, respectively. The results are similar to the case of the unweighted sell \nprobability in two aspects. First, the normal ***** and the fraudulent ***** form dis-\ntinct frequency distributions (Fig. 5c). Second, most of the normal users are either on \nthe horizontal line with the weighted sell probability of one or on the dashed line with \nthe smallest possible weighted sell probability, i.e., 1/si (Fig. **, Table 1).\n\nThe survival probability of the local clustering coefficient is shown in Fig.  6a. It \nshould be noted that, in this analysis, we confined ourselves to the users with ki ≥ 2 \nbecause Ci is undefined when ki = 1 . We found that the number of users with Ci = 0 is \nnot considerably different between the normal and fraudulent ***** (also see Table 1). \nFigure  6b shows the survival probability of Ci conditioned on Ci > 0 . The normal \n***** tend to have a larger value of Ci than fraudulent *****, whereas this tendency is \nnot strong (Table 1).\n\nThe survival probability of the triangle congregation is shown in Fig. 7a. Contrary to \nour hypothesis, there is no clear difference between the distribution of the normal and \nfraudulent *****. The triangle congregation tends to be large when the node strength \nis small (Fig. 7b) and the local clustering coefficient is large (Fig. **). It depends little \non the weighted sell probability (Fig. 7c). However, we did not find clear differences in \nthe triangle congregation between the normal and fraudulent ***** (also see Table 1).\n\nThe survival probability of the cycle probability is shown in Fig. 8a. A large fraction \nof any type of ***** has CYPi = 0 (Table 1). When the users with CYPi = 0 are dis-\ncarded, the normal ***** tend to have a smaller value of CYPi than any type of fraudu-\nlent ***** (Fig. 8b, Table 1).\n\na b\n\nFig. 6 Local clustering coefficient for each user type. a Survival probability. b Survival probability conditioned \non Ci > 0\n\n\n\nPage 12 of ******** et al. Appl Netw Sci            (****) 5:90 \n\nClassification of users\n\nBased on the eight indices whose descriptive statistics were analyzed in the previ-\nous section, we defined 12 features and fed them to the random forest classifier. The \naim of the classifier is to distinguish between normal and fraudulent *****. The first \nfeature is binary and whether the degree ki = 1 or ki ≥ 2 . The second feature is also \nbinary and whether the node strength si = 1 or si ≥ 2 . The third feature is si/ki , which \nis a real number greater than or equal to 1. The fourth feature is binary and whether the \nunweighted sell probability SPi = 1 or SPi < 1 . The fifth feature is binary and whether \n\na b\n\nc d\n\nFig. 7 Triangle congregation for each user type. a Survival probability. b Relationship between the triangle \ncongregation, mi , and the node strength. c Relationship between mi and the weighted sell probability. d \nRelationship between mi and the local clustering coefficient\n\na b\n\nFig. 8 Cycle probability for each user type. a Survival probability. b Survival probability conditioned on \nCYPi > 0\n\n\n\nPage 13 of ******** et al. Appl Netw Sci            (****) 5:90  \n\nSPi = 1/(k ini + kouti ) or SPi > 1/(k ini + kouti ) , i.e., whether kouti = 1 or kouti > 1 . The sixth \nfeature is SPi , which ranges between 0 and 1. The seventh feature is binary and whether \nthe weighted sell probability WSPi = 1 or WSPi < 1 . The eighth feature is binary and \nwhether WSPi = 1/(sini + souti ) or WSPi > 1/(sini + souti ) , i.e., whether souti = 1 or \nsouti > 1 . The ninth feature is WSPi , which ranges between 0 and 1. The tenth feature is \nthe local clustering coefficient Ci , which ranges between 0 and 1. When ki = 1 , the local \nclustering coefficient is undefined. In this case, we set Ci = − 1 . The eleventh feature is \nthe triangle congregation mi , which ranges between 0 and 1. When there is no triangle \nor only one triangle involving vi , one cannot calculate mi . In this case, we set mi = − 1 . \nFinally, the twelfth feature is the cycle probability CYPi , which ranges between 0 and 1. \nWhen there is neither feedforward nor cyclic triangle involving vi , CYPi is undefined. In \nthis case, we set CYPi = − 1.\n\nThe ROC and PR curves when all the 12 features of users are used and the fraudu-\nlent type is fictive transactions are shown in Fig. 9a, b, respectively. Each thin line cor-\nresponds to one of the 100 classifiers. The thick lines correspond to the average of the \n100 lines. The dashed lines correspond to the uniformly random classification. Figure 9 \nindicates that the classification performance seems to be high. Quantitatively, for this \nand the other types of fraudulent *****, the AUC values always exceeded 0.91 (Table 2).\n\nThe importance of each feature in the classifier is shown in Fig.  10a, separately for \nthe different fraud types. The importance of each feature is similar across the different \ntypes of fraud. Figure 10a indicates that the average number of transactions per neighbor \n(i.e., si/ki ), whether or not kouti = 1 (i.e., SPi = 1/(k ini + kouti ) ), whether or not souti = 1 \n(i.e., WSPi = 1/(sini + souti ) ), and the weighted sell probability (i.e., WSPi ) are the four \nfeatures of the highest importance. Given the results of the descriptive statistics in the \nprevious section, a small value of si/ki , kouti  = 1 , souti  = 1 , and a moderate WSPi value \nstrongly suggest that the **** may be fraudulent.\n\nFigure 10a also suggests that the features based on the triangles, i.e., Ci , mi , and CYPi , \nare not strong contributors to the classifier’s performance. Because these features are the \nonly ones that require the information about the connectivity between pairs of neigh-\nbors of the focal node, it is practically beneficial if one can realize a similar classification \n\na b\n\nFig. 9 ROC and PR curves when the normal ***** and those involved in fictive transactions are classified. a \nROC curves. b PR curves. Each thin line corresponds to one of the 100 classifiers. The thick lines correspond to \nthe average of the 100 lines. The dashed lines correspond to the uniformly random classification\n\n\n\nPage 14 of ******** et al. Appl Netw Sci            (****) 5:90 \n\nperformance without using these features; then only the information on the connectivity \nof the focal ***** is required. To explore this possibility, we constructed the random for-\nest classifier using the nine out of the twelve features that do not require the connectivity \nbetween neighbors of the focal node. The mean AUC values for the ROC and PR curves \nare shown in Table 2. We find that, despite some reduction in the performance scores \nrelative to the case of the classifier using all the 12 features, the AUC values with the \nnine features are still large, all exceeding 0.88. The permutation importance of the nine \nfeatures is shown in Fig. 10b. The results are similar to those when all the 12 features are \nused, although the importance of WSPi considerably increased in the case of the nine \nfeatures (Fig. 10a).\n\nMore than half of the normal users have ki = 1 , and there are few fraudulent users \nwith ki = 1 in each fraud category (Table 1). The classification between the normal and \nfraudulent users may be an easy problem for this reason, leading to the large AUC val-\nues. To exclude this possibility, we carried out a classification test for the subdata in \nwhich the normal and fraudulent users with ki = 1 were excluded, leaving 412 normal \n***** and a similar number of fraudulent ***** in each category (Table 1). We did not \n\na b\n\nFig. 10 Permutation importance of the features in the random forest classifier. a 12 features. b 9 features. The \nbars indicate the average over the 100 classifiers. The error bars indicate standard deviation\n\nTable 2 AUC values for the random forest classifiers\n\nThe average and standard deviation were calculated based on the 100 classifiers\n\nFictive Underwear Medicine Weapon\n\n12 features\n\nROC 0.962 ± 0.003 0.981 ± 0.001 0.979 ± 0.003 0.969 ± 0.004\n\nPR 0.916 ± 0.009 0.948 ± 0.006 0.947 ± 0.005 0.916 ± 0.015\n\n9 features\n\nROC 0.951 ± 0.003 0.973 ± 0.003 0.971 ± 0.003 0.961 ± 0.004\n\nPR 0.889 ± 0.009 0.923 ± 0.010 0.930 ± 0.009 0.888 ± 0.025\n\n\n\nPage 15 of 18Kodate et al. Appl Netw Sci            (****) 5:90  \n\ncarry out subsampling because the number of the negative and positive samples were \nsimilar. Instead, we generated 100 different sets of train and test samples and built a clas-\nsifier based on each set of train and test samples. The AUC values when either 10 or 7 \nfeatures (i.e., the features excluding whether or not ki = 1 and whether or not si = 1 ) are \nused are shown in Table 3. The table indicates that the AUC values are still competitively \nlarge while they are smaller than those when whether or not ki = 1 and whether or not \nsi = 1 are used as features (Table 2).\n\nDiscussion\nWe showed that a random forest classifier using network features of users distinguished \ndifferent types of fraudulent users from normal users with approximately 0.91–0.98 in \nterms of the ***. We only used the information about local transaction networks cen-\ntered around focal ***** to synthesize their features. We did so because it is better in \npractice not to demand the information about global transaction networks due to the \nlarge number of *****. It should be noted that AUC values of ≈ 0.88–0.97 was also real-\nized when we only used the information about the connectivity of the focal user, not the \nconnectivity between the ********* of the focal ****. This result has a practical advan-\ntage when the present fraud-detection method is implemented online because it allows \none to classify ***** with a smaller amount of data per ****.\n\nThe random forest classifier is an arbitrary choice. One can alternatively use a different \nlinear or nonlinear classifier to pursue a higher classification performance. This is left as \nfuture work. Other future tasks include the generalizability of the present results to dif-\nferent types of fraudulent transactions, such as resale tickets, pornography, and stolen \nitems, and to different platforms. In particular, if a classifier trained with test samples \nfrom fraudulent users of a particular type and normal ***** is effective at detecting dif-\nferent types of fraud, the classifier will also be potentially useful for detecting unknown \ntypes of fraudulent transactions. It is also a potentially relevant question to assess the \nclassification performance when one pools different types of fraud as a single positive \ncategory to train a **********.\n\nPrior network-based fraud detection has employed either global or local network \nproperties to characterize nodes. Global network properties refer to those that require \nthe structure of the entire network for calculating a quantity for individual nodes, such \nas the connected component (****** et al. ****; ****** et al. ****; **** et al. ****), \nbetweenness centrality (****** et al. ****; ********* et al. ****; ******** and Remondi \n****), ****’s suspiciousness determined by belief propagation (**** et al. ****; ****** \n\nTable 3 AUC values for the random forest classifiers excluding ***** with ki = 1\n\nThe average and standard deviation were calculated based on the 100 classifiers\n\nFictive Underwear Medicine Weapon\n\n10 features\n\nROC 0.925 ± 0.016 0.950 ± 0.013 0.954 ± 0.012 0.916 ± 0.019\n\nPR 0.923 ± 0.019 0.950 ± 0.018 0.954 ± 0.016 0.911 ± 0.023\n\n7 features\n\nROC 0.886 ± 0.020 0.921 ± 0.015 0.933 ± 0.014 0.899 ± 0.020\n\nPR 0.874 ± 0.027 0.901 ± 0.021 0.928 ± 0.019 0.880 ± 0.028\n\n\n\nPage 16 of 18Kodate et al. Appl Netw Sci            (****) 5:90 \n\net al. ****; ****** et al. ****; ************** et al. ****; ************** et al. ****, \n****; ** et al. ****; ** et al. ****), dense subgraphs including the case of communities \n(****** et al. ****; **** and Abulaish ****; ******* et al. ****; ***** et al. ****; Hooi \net al. ****; *** et al. ****; ****** et al. ****), and k-core (**** and Chiu ****; ******* \net  al. ****). Although many of these methods have accrued a high classification per-\nformance, they require the information about the entire network. Obtaining such data \nmay be difficult when the network is large or rapidly evolving over time, thus potentially \ncompromising the computation speed, memory requirement, and the accuracy of the \ninformation on the nodes and edges. Alternatively, other methods employed local net-\nwork properties such as the degree including the case of directed and/or weighted net-\nworks (**** et al. ****; ****** et al. ****; ****** et al. ****; ******* et al. ****; Bhat \nand ******** ****; ************** et al. ****; ********* et  al. ****; ****** et al. \n****; ************** et al. ****; ******** and ******* ****) and the abundance of tri-\nangles and quadrangles (****** et al. ****; ************** et al. ****). The use of local \nnetwork properties may be advantageous in industrial contexts, particularly to test sam-\npled *****, because local quantities can be rapidly calculated given a seed node. Another \nreason for which we focused on local properties was that we could not obtain the global \nnetwork structure for computational reasons. It should be noted that, while the use of \nglobal network properties in addition to local ones *** improve the classification accu-\nracy (**** and ******** ****), the present local method attained a similar classification \nperformance to those based on global network properties, i.e., 0.880–0.986 in terms of \nthe ROC AUC (****** et al. ****; ************** et al. ****; ************** et al. ****; \n** et al. ****; ** et al. ****; ****** et al. ****).\n\nA prior study using data from the same marketplace, *******, aimed to distinguish \nbetween desirable non-professional frequent sellers and undesirable professional sellers \n(******** et al. ****). The ******* used information about user profiles, item descrip-\ntions, and other behavioral data such as the number of purchases per day. In contrast, \nwe focused on local network features of the users (while a quantity similar to WSPi was \nused as a feature in ******** et al. (****)). In addition, we used specific types of fraud-\nulent transactions, whereas ******** et al. (****) focused on problematic transactions \nas a single broad category. How the present results generalize to different categoriza-\ntions of fraudulent transactions, the platform’s different data such as their US market \ndata, and similar data obtained from other online marketplaces is unknown. Combining \nnetwork and non-network features *** realize a better classification performance . Fur-\nthermore, using the information about the time of the transactions *** also yield better \nclassification. Using the time information allows us to ask new questions such as predic-\ntion of *****’ behavior. These topics warrant future work.\n\nAbbreviations\nC2C: Consumer-to-consumer; ROC: Receiver operating characteristic; PR: Precision–recall; AUC : Area under the curve.\n\nAcknowledgements\nThis work was carried out using the computational facilities of the **********************************, ************* \nBristol.\n\nAuthors’ contributions\n*********** analyzed data, developed methodology, visualized the results, and drafted the manuscript; RC curated data \nand critically revised the manuscript; ************* coordinated the study, acquired funding, and critically revised the \nmanuscript; ** coordinated the study, acquired funding, developed methodology, drafted the manuscript. All authors \n\n\n\nPage 17 of 18Kodate et al. Appl Netw Sci            (****) 5:90  \n\ngave final approval for publication and agreed to be held accountable for the work performed therein. All authors read \nand approved the final manuscript.\n\nFunding\nThe authors acknowledge financial support by *********************** was supported in part by the ********************* \nProject from the ************************************************************** (****) of Japan.\n\nAvailability of data and materials\n************* approved the use of the data for the present study under the condition that the data were hashed and \nonly released to the ************* of the project (i.e., the first and last *******, because the second and third ******* \nare ********* of the company). The figures and tables of the present paper are summary statistics of the data and not \nsufficient on their own for others to replicate the results of the present study. Although the data have been hashed, the \ncompany cannot share the data with the public. This is because, if anybody traces the transaction data on the Mercari’s \nweb platform and checks them against the hashed data, that person would be able to identify individual users including \ntheir private information. Therefore, hashing/anonymizing does not help to guarantee the *****’ privacy. Any bona fide \n********** could approach the company (*************: ******************* and *************: metalunk@*******.\ncom) to seek access to the complete dataset. However, for the aforementioned reasons, such an attempt is unlikely to be \nsuccessful. The users were made aware that their data may be used for the present research because the *******’s terms \nof use (in Japanese only: https ://www.merca **************), Article 20, Term 2, states that their data can be used for \nresearch by the company and by those who the company permits.\n\nEthics approval and consent to participate\n************* approved the use of the data for the present study under the condition that the data were hashed and \nonly released to the ************* of the project (i.e., the first and last *******, because the second and third ******* are \n********* of the company).\n\nCompeting interests\nThe second and third ******* are ********* of the company that provided the data analysed in the present manuscript. \nHowever, this fact does not cause any conflict of interest because the analyses, results and their interpretation are free of \nany bias towards the merit of the company.\n\nAuthor details\n1 ***************************************, Tohoku University, ***************, Japan. 2 ************************* \nMathematics, *********************, Bristol BS8 1UB, UK. 3 ************., **************, Japan. 4 **********************-\nics, *********************, ***************************. 5 Computational and Data-Enabled Science and Engineering \nProgram, *********************, **********************, USA. \n\nReceived: **************   Accepted: ***************\n\nReferences\n**********, *********, ******** (****) Fraud detection system: a survey. J Netw Comput Appl 68:90–113\n********, **********, *********** (****) Oddball: spotting anomalies in weighted graphs. In: Pacific-Asia conference on \n\nknowledge discovery and data mining, pp 410–421\n********, ********, *********** (****) Opinion fraud detection in online reviews by network effects. In: 7th international \n\nAAAI conference on weblogs and social media, pp 2–11\n********, ******, ******** (****) Graph based anomaly detection and description: a survey. ************** ****** \n\n29:626–688\n*********, ********, ********, ********** (****) Permutation importance: a corrected feature importance measure. ******* \n\n26:1340–1347\n**********, ********, *******, *********, ************, ******, *******, ******** (****) Measuring the cost of cybercrime. \n\nIn: The economics of information security and privacy. ********, Berlin, pp 265–300\n****************, ***********, *********, **********, ******** (****) Two step graph-based semi-supervised learning \n\nfor online auction fraud detection. In: Joint European conference on machine learning and knowledge discovery in \ndatabases, pp 165–179\n\n********, ************, *****************, ************ (****) The architecture of complex weighted networks. Proc Natl \n************ 101:3747–3752\n\n*******, ********** (****) Community-based features for identifying ******** in online social networks. In: **** ******** \ninternational conference on advances in social networks analysis and mining (ASONAM ****), pp 100–107\n\n**********, *********** (****) Machine learning for e-mail spam filtering: review, techniques and trends. Preprint arXiv \n:1606.01042 \n\n*********, ******* (****) Statistical fraud detection: a review. Stat Sci 17:235–249\n********* (****) Random forests. Mach Learn 45:5–32\n*********, ***********, *********, ******** (****) Classification and regression trees. **************, Boca Raton\n*******, ********, *********** (****) Detecting fraudulent personalities in networks of online auctioneers. In: European \n\nconference on principles of data mining and knowledge discovery, pp 103",
      "merged_content": "\nDetecting problematic transactions \nin a consumer‑to‑consumer e‑commerce \nnetwork\nShun Kodate1,2, Ryusuke Chiba3, Shunya Kimura3 and Naoki Masuda2,4,5* \n\nIntroduction\nIn tandem with the rapid growth of online and electronic transactions and communi-\ncations, fraud is expanding at a dramatic speed and penetrates our daily lives. Fraud \nincluding cybercrimes costs billions of dollars per year and threatens the security of our \nsociety (UK Parliament 2017; McAfee 2019). In particular, in the recent era where online \nactivity dominates, attacking a system is not too costly, whereas defending the system \nagainst fraud is costly (Anderson et al. 2013). The dimension of fraud is vast and ranges \nfrom credit card fraud, money laundering, computer intrusion, to plagiarism, to name a \nfew.\n\nAbstract \n\nProviders of online marketplaces are constantly combatting against problematic \ntransactions, such as selling illegal items and posting fictive items, exercised by some \nof their users. A typical approach to detect fraud activity has been to analyze registered \nuser profiles, user’s behavior, and texts attached to individual transactions and the user. \nHowever, this traditional approach may be limited because malicious users can easily \nconceal their information. Given this background, network indices have been exploited \nfor detecting frauds in various online transaction platforms. In the present study, we \nanalyzed networks of users of an online consumer-to-consumer marketplace in which \na seller and the corresponding buyer of a transaction are connected by a directed \nedge. We constructed egocentric networks of each of several hundreds of fraudulent \nusers and those of a similar number of normal users. We calculated eight local network \nindices based on up to connectivity between the neighbors of the focal node. Based \non the present descriptive analysis of these network indices, we fed twelve features \nthat we constructed from the eight network indices to random forest classifiers with \nthe aim of distinguishing between normal users and fraudulent users engaged in each \none of the four types of problematic transactions. We found that the classifier accu-\nrately distinguished the fraudulent users from normal users and that the classification \nperformance did not depend on the type of problematic transaction.\n\nKeywords: Network analysis, Machine learning, Fraud detection, Computational social \nscience\n\nOpen Access\n\n© The Author(s) 2020. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://\ncreat iveco mmons .org/licen ses/by/4.0/.\n\nRESEARCH\n\nKodate et al. Appl Netw Sci            (2020) 5:90  \nhttps://doi.org/10.1007/s41109‑020‑00330‑x Applied Network Science\n\n*Correspondence:   \nnaokimas@buffalo.edu \n4 Department \nof Mathematics, University \nat Buffalo, Buffalo, NY \n14260-2900, USA\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0003-1567-801X\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1007/s41109-020-00330-x&domain=pdf\n\n\nPage 2 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nComputational and statistical methods for detecting and preventing fraud have been \ndeveloped and implemented for decades (Bolton and Hand 2002; Phua et  al. 2010; \nAbdallah et al. 2016; West and Bhattacharya 2016). Standard practice for fraud detec-\ntion is to employ statistical methods including the case of machine learning algorithms. \nIn particular, when both fraudulent and non-fraudulent samples are available, one can \nconstruct a classifier via supervised learning (Bolton and Hand 2002; Phua et al. 2010; \nAbdallah et al. 2016; West and Bhattacharya 2016). Exemplar features to be fed to such a \nstatistical classifier include the transaction amount, day of the week, item category, and \nuser’s address for detecting frauds in credit card systems, number of calls, call duration, \ncall type, and user’s age, gender, and geographical region in the case of telecommunica-\ntion, and user profiles and transaction history in the case of online auctions (Abdallah \net al. 2016).\n\nHowever, many of these features can be easily faked by advanced fraudsters (Akoglu \net al. 2015; Google LLC 2018). Furthermore, fraudulent users are adept at escaping the \neyes of the administrators or authorities that would detect the usage of particular words \nas a signature of anomalous behavior (Pu and Webb 2006; Hayes 2007; Bhowmick and \nHazarika 2016). For example, if the authority discovers that one jargon means a drug, \nthen fraudulent users may easily switch to another jargon to confuse the authority.\n\nNetwork analysis is an alternative way to construct features and is not new to fraud \ndetection techniques (Savage et al. 2014; Akoglu et al. 2015). The idea is to use connec-\ntivity between nodes, which are usually users or goods, in the given data and calculate \ngraph-theoretic quantities or scores that characterize nodes. These methods stand on \nthe expectation that anomalous users show connectivity patterns that are distinct from \nthose of normal users (Akoglu et al. 2015). Network analysis has been deployed for fraud \ndetection in insurance (Šubelj et  al. 2011), money laundering (Dreżewski et  al. 2015; \nColladon and Remondi 2017; Savage et al. 2017), health-care data (Liu et al. 2016), car-\nbooking (Shchur et al. 2018), a social security system (Van Vlasselaer et al. 2016), mobile \nadvertising (Hu et al. 2017), a mobile phone network (Ferrara et al. 2014), online social \nnetworks (Bhat and Abulaish 2013; Jiang et  al. 2014; Hooi et  al. 2016; Rasheed et  al. \n2018), online review forums (Akoglu et al. 2013; Liu et al. 2017; Wang et al. 2018), online \nauction or marketplaces (Chau et  al. 2006; Pandit et  al. 2007; Wang and Chiu 2008; \nBangcharoensap et  al. 2015; Yanchun et  al. 2011), credit card transactions (Van Vlas-\nselaer et al. 2015; Li et al. 2017), cryptocurrency transaction (Monamo et al. 2016), and \nvarious other fields (Akoglu et al. 2010). For example, fraudulent users and their accom-\nplices were shown to form approximately bipartite cores in a network of users to inflate \ntheir reputations in an online auction system (Chau et al. 2006). Then, the authors pro-\nposed an algorithm based on a belief propagation to detect such suspicious connectivity \npatterns. This method has been proven to be also effective on empirical data obtained \nfrom eBay (Pandit et al. 2007).\n\nIn the present study, we analyze a data set obtained from a large online consumer-to-\nconsumer (C2C) marketplace, Mercari, operating in Japan and the US. They are the larg-\nest C2C marketplace in Japan, in which, as of 2019, there are 13 million monthly active \nusers and 133 billion yen (approximately 1.2 billion USD) transactions per quarter year \n(Mercari 2019). Note that we analyze transaction frauds based on transaction networks \nof users, which contrasts with previous studies of online C2C marketplaces that looked \n\n\n\nPage 3 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nat reputation frauds (Chau et al. 2006; Pandit et al. 2007; Wang and Chiu 2008; Yanchun \net  al. 2011). Many prior network-based fraud detection algorithms used global infor-\nmation about networks, such as connected components, communities, betweenness, \nk-cores, and that determined by belief propagation (Chau et al. 2006; Pandit et al. 2007; \nWang and Chiu 2008; Šubelj et  al. 2011; Akoglu et  al. 2013; Bhat and Abulaish 2013; \nFerrara et al. 2014; Jiang et al. 2014; Bangcharoensap et al. 2015; Dreżewski et al. 2015; \nVan Vlasselaer et al. 2015; Hooi et al. 2016; Liu et al. 2016; Van Vlasselaer et al. 2016; \nColladon and Remondi 2017; Hu et al. 2017; Li et al. 2017; Liu et al. 2017; Savage et al. \n2017; Shchur et al. 2018; Rasheed et al. 2018; Wang et al. 2018). Others used local infor-\nmation about the users’ network, such as the degree, the number of triangles, and the \nlocal clustering coefficient (Chau et al. 2006; Akoglu et al. 2010; Šubelj et al. 2011; Yan-\nchun et al. 2011; Bhat and Abulaish 2013; Bangcharoensap et al. 2015; Dreżewski et al. \n2015; Monamo et al. 2016; Van Vlasselaer et al. 2016; Colladon and Remondi 2017). We \nwill focus on local features of users, i.e., features of a node that can be calculated from \nthe connectivity of the user and the connectivity between neighbors of the user. This is \nbecause local features are easier and faster to calculate and thus practical for commercial \nimplementations.\n\nMaterials and methods\nData\n\nMercari is an online C2C marketplace service, where users trade various items among \nthemselves. The service is operating in Japan and the United States. In the present study, \nwe used the data obtained from the Japanese market between July 2013 and January \n2019. In addition to normal transactions, we focused on the following types of prob-\nlematic transactions: fictive, underwear, medicine, and weapon. Fictive transactions are \ndefined as selling non-existing items. Underwear refers to transactions of used under-\nwear; they are prohibited by the service from the perspective of morality and hygiene. \nMedicine refers to transactions of medicinal supplies, which are prohibited by the law. \nWeapon refers to transactions of weapons, which are prohibited by the service because \nthey may lead to crime. The number of sampled users of each type is shown in Table 1.\n\nNetwork analysis\n\nWe examine a directed and weighted network of users in which a user corresponds to a \nnode and a transaction between two users represents a directed edge. The weight of the \nedge is equal to the number of transactions between the seller and the buyer. We con-\nstructed egocentric networks of each of several hundreds of normal users and those of \nfraudulent users, i.e., those engaged in at least one problematic sell. Figure 1 shows the \negocentric networks of two normal users (Fig. 1a, b) and those of two fraudulent users \ninvolved in selling a fictive item (Fig. 1c, d). The egocentric network of either a normal or \nfraudulent user contained the nodes neighboring the focal user, edges between the focal \nuser and these neighbors, and edges between the pairs of these neighbors.\n\nWe calculated eight indices for each focal node. They are local indices in the mean-\ning that they require the information up to the connectivity among the neighbors of the \nfocal node.\n\n\n\nPage 4 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nFive out of the eight indices use only the information about the connectivity of the focal \nnode. The degree ki of node vi is the number of its neighbors. The node strength  (Barrat \net al. 2004) (i.e., weighted degree) of node vi , denoted by si , is the number of transactions in \nwhich vi is involved. Using these two indices, we also considered the mean number of trans-\nactions per neighbor, i.e., si/ki , as a separate index. These three indices do not use informa-\ntion about the direction of edges.\n\nThe sell probability of node vi , denoted by SPi , uses the information about the direction of \nedges and defined as the proportion of the vi ’s neighbors for which vi acts as seller. Precisely, \nthe sell probability is given by\n\n(1)SPi =\nkouti\n\nk ini + kouti\n\n,\n\nFig. 1 Examples of egocentric networks. a, b Egocentric networks of arbitrarily selected two normal users. c, \nd Egocentric networks of arbitrarily selected two fraudulent users involved in selling a fictive item\n\n a b O- 0 0404 O C 0 O 0 20 04 20070 \n\n\n\nPage 5 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nwhere k ini  is vi ’s in-degree (i.e., the number of neighbors from whom vi bought at least \none item) and kouti  is vi ’s out-degree (i.e., the number of neighbors to whom vi sold at \nleast one item). It should be noted that, if vi acted as both seller and buyer towards vj , the \ncontribution of vj to both in- and out-degree of vi is equal to one. Therefore, k ini + kouti  is \nnot equal to ki in general.\n\nThe weighted version of the sell probability, denoted by WSPi , is defined as\n\nwhere sini  is node vi ’s weighted in-degree (i.e., the number of buys) and souti  is vi ’s weighted \nout-degree (i.e., the number of sells).\n\nThe other three indices are based on triangles that involve the focal node. The local \nclustering coefficient Ci quantifies the abundance of undirected and unweighted triangles \naround vi (Newman 2010). It is defined as the number of undirected and unweighted trian-\ngles including vi divided by ki(ki − 1)/2 . The local clustering coefficient Ci ranges between \n0 and 1.\n\nWe hypothesized that triangles contributing to an increase in the local clustering coef-\nficient are localized around particular neighbors of node vi . Such neighbors together with vi \nmay form an overlapping set of triangles, which may be regarded as a community (Radicchi \net al. 2004; Palla et al. 2005). Therefore, our hypothesis implies that the extent to which the \nfocal node is involved in communities should be different between normal and fraudulent \nusers. To quantify this concept, we introduce the so-called triangle congregation, denoted \nby mi . It is defined as the extent to which two triangles involving vi share another node and \nis given by\n\nwhere Tri = Ciki(ki − 1)/2 is the number of triangles involving vi . Note that mi ranges \nbetween 0 and 1.\n\nFrequencies of different directed three-node subnetworks, conventionally known as net-\nwork motifs (Milo et al. 2002), may distinguish between normal and fraudulent users. In \nparticular, among triangles composed of directed edges, we hypothesized that feedforward \ntriangles (Fig. 2a) should be natural and that cyclic triangles (Fig. 2b) are not. We hypoth-\nesized so because a natural interpretation of a feedforward triangle is that a node with out-\ndegree two tends to serve as seller while that with out-degree zero tends to serve as buyer \nand there are many such nodes that use the marketplace mostly as buyer or seller but not \nboth. In contrast, an abundance of cyclic triangles may imply that relatively many users use \nthe marketplace as both buyer and seller. We used the index called the cycle probability, \ndenoted by CYPi , which is defined by\n\nwhere FFi and CYi are the numbers of feedforward triangles and cyclic triangles to which \nnode vi belongs. The definition of FFi and CYi , and hence CYPi , is valid even when the \n\n(2)WSPi =\nsouti\n\nsini + souti\n\n,\n\n(3)mi =\n(Number of pairs of triangles involving vi that share another node)\n\nTri(Tri − 1)/2\n,\n\n(4)CYPi =\nCYi\n\nFFi + CYi\n,\n\n\n\nPage 6 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\ntriangles involving vi have bidirectional edges. In the case of Fig. 2c, for example, any of \nthe three nodes contains one feedforward triangle and one cyclic triangle. The other four \ncases in which bidirectional edges are involved in triangles are shown in Fig. 2d–g. In the \ncalculation of CYPi , we ignored the weights of edges.\n\nRandom forest classifier\n\nTo classify users into normal and fraudulent users based on their local network proper-\nties, we employed a random forest classifier (Breiman 2001; Breiman et al. 1984; Hastie \net  al. 2009) implemented in scikit-learn (Pedregosa et  al. 2011). It uses an ensemble \nlearning method that combines multiple classifiers, each of which is a decision tree, \nbuilt from training data and classifies test data avoiding overfitting. We combined 300 \ndecision-tree classifiers to construct a random forest classifier. Each decision tree is con-\nstructed on the basis of training samples that are randomly subsampled with replace-\nment from the set of all the training samples. To compute the best split of each node \nin a tree, one randomly samples the candidate features from the set of all the features. \nThe probability that a test sample is positive in a tree is estimated as follows. Consider \nthe terminal node in the tree that a test sample eventually reaches. The fraction of posi-\ntive training samples at the terminal node gives the probability that the test sample is \nclassified as positive. One minus the positive probability gives the negative probability \nestimated for the same test sample. The positive or negative probability for the random \nforest classifier is obtained as the average of single-tree positive or negative probability \nover all the 300 trees. A sample is classified as positive by the random forest classifier if \nthe positive probability is larger than 0.5, otherwise classified as negative.\n\nWe split samples of each type into two sets such that 75% and 25% of the samples of \neach type are assigned to the training and test samples, respectively. There were more \n\ncyclicfeedforward feedforward: 1\ncyclic: 1\n\nfeedforward: 2\ncyclic: 0\n\nfeedforward: 3\ncyclic: 1\n\nfeedforward: 6\ncyclic: 2\n\na b c d\n\nf g\n\nfeedforward: 2\ncyclic: 0\n\ne\n\nFig. 2 Directed triangle patterns and their count. a Feedforward triangle. b Cyclic triangle. c– g Five \nthree-node patterns that contain directed triangles and reciprocal edges. The numbers shown in the figure \nrepresent the number of feedforward or cyclic triangles to which each three-node pattern contributes\n\n\n\nPage 7 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nnormal users than any type of fraudulent user. Therefore, to balance the number of \nthe negative (i.e., normal) and positive (i.e., fraudulent) samples, we uniformly ran-\ndomly subsampled the negative samples (i.e., under-sampling) such that the number \nof the samples is the same between the normal and fraudulent types in the training \nset. Based on the training sample constructed in this manner, we built each of the 300 \ndecision trees and hence a random forest classifier. Then, we examined the classifica-\ntion performance of the random forest classifier on the set of test samples.\n\nThe true positive rate, also called the recall, is defined as the proportion of the posi-\ntive samples (i.e., fraudulent users) that the random forest classifier correctly classifies \nas positive. The false positive rate is defined as the proportion of the negative samples \n(i.e., normal users) that are incorrectly classified as positive. The precision is defined \nas the proportion of the truly positive samples among those that are classified as posi-\ntive. The true positive rate, false positive rate, and precision range between 0 and 1.\n\nWe used the following two performance measures for the random forest classifier. \nTo draw the receiver operating characteristic (ROC) curve for a random forest clas-\nsifier, one first arranges the test samples in descending order of the estimated prob-\nability that they are positive. Then, one plots each test sample, with its false positive \nrate on the horizontal axis and the true positive rate on the vertical axis. By connect-\ning the test samples in a piecewise linear manner, one obtains the ROC curve. The \nprecision–recall (PR) curve is generated by plotting the samples in the same order in \n[0, 1]2 , with the recall on the horizontal axis and the precision on the vertical axis. For \nan accurate binary classifier, both ROC and PR curves visit near (x, y) = (0, 1) . There-\nfore, we quantify the performance of the classifier by the area under the curve (AUC) \nof each curve. The AUC ranges between 0 and 1, and a large value indicates a good \nperformance of the random forest classifier.\n\nTo calculate the importance of each feature in the random forest classifier, we \nused the permutation importance (Strobl et al. 2007; Altmann et al. 2010). With this \nmethod, the importance of a feature is given by the decrease in the performance of \nthe trained classifier when the feature is randomly permuted among the test samples. \nA large value indicates that the feature considerably contributes to the performance \nof the classifier. To calculate the permutation importance, we used the AUC value of \nthe ROC curve as the performance measure of a random forest classifier. We com-\nputed the permutation importance of each feature with ten different permutations \nand adopted the average over the ten permutations as the importance of the feature.\n\nWe optimized the parameters of the random forest classifier by a grid search with \n10-fold cross-validation on the training set. For the maximum depth of each tree (i.e., \nthe max_depth parameter in scikit-learn), we explored the integers between 3 and 10. \nFor the number of candidate features for each split (i.e., max_features), we explored \nthe integers between 3 and 6. For the minimum number of samples required at termi-\nnal nodes (i.e., min_samples_leaf ), we explored 1, 3, and 5. As mentioned above, the \nnumber of trees (i.e., n_estimators) was set to 300. The seed number for the random \nnumber generator (i.e., random_state) was set to 0. For the other hyperparameters, \nwe used the default values in scikit-learn version 0.22. In the parameter optimization, \nwe evaluated the performance of the random forest classifier with the AUC value of \nthe ROC curve measured on a single set of training and test samples.\n\n\n\nPage 8 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nTo avoid sampling bias, we built 100 random forest classifiers, trained each classifier, \nand tested its performance on a randomly drawn set of train and test samples, whose \nsampling scheme was described above.\n\nResults\nDescriptive statistics\n\nThe survival probability of the degree (i.e., a fraction of nodes whose degree is larger \nthan a specified value) is shown in Fig. 3a for each user type. Approximately 60% of the \nnormal users have degree ki = 1 , whereas the fraction of the users with ki = 1 is approxi-\nmately equal to 2% or less for any type of fraudulent user (Table 1). Therefore, we expect \nthat whether ki = 1 or ki ≥ 2 gives useful information for distinguishing between normal \nand fraudulent users. The degree distribution at ki ≥ 2 may provide further information \nuseful for the classification. The survival probability of the degree distribution condi-\ntioned on ki ≥ 2 for the different types of users is shown in Fig. 3b. The figure suggests \nthat the degree distribution is systematically different between the normal and fraudu-\nlent users. However, we consider that the difference is not as clear-cut as that in the frac-\ntion of users having ki = 1 (Table 1).\n\nThe survival probability of the node strength (i.e., weighted degree) is shown in Fig. 3c \nfor each user type. As in the case for the unweighted degree, we found that many nor-\nmal users, but not fraudulent users, have si = 1 . In fact, the number of the normal users \nwith si = 1 is equal to those with ki = 1 (Table 1), implying that all normal users with \nki = 1 participated in just one transaction. In contrast, no user had si = 1 for any type \nof fraudulent user. The survival probability of the node strength conditioned on si ≥ 2 \napparently does not show a clear distinction between the normal and fraudulent users \n(Fig. 3d, Table 1).\n\na b\n\nc d\n\nFig. 3 Survival probability of the degree for each user type. a Degree (i.e., ki ) for all nodes. b Degree for the \nnodes with ki ≥ 2 . c Strength (i.e., si ) for all nodes. d Strength for the nodes with si ≥ 2\n\n\n\nPage 9 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nThe distribution of the average number of transactions per edge, i.e., si/ki , is shown \nin Fig. 4a. We found that a majority of normal users have si/ki = 1 . This result indicates \nthat a large fraction of normal users is engaged in just one transaction per neighbor \n(Table 1). This result is consistent with the fact that approximately 60% of the normal \nusers have ki = si = 1 . In contrast, many of any type of fraudulent users have si/ki > 1 . \nHowever, they tend to have a smaller value of si/ki than the normal users. This differ-\nence is more noticeable when we discraded the users with si/ki = 1 (Fig. 4b, Table 1). \nTherefore, less frequent transactions with a specific neighbor seem to be a characteristic \nbehavior of fraudulent users.\n\nThe distribution of the unweighted sell probability for the different user types is \nshown in Fig.  5a. The distribution for the normal users is peaked around 0 and 1, \n\nTable 1 Properties of different types of users\n\nIn the first column, Mean ( A | B ), for example, represents the mean of A conditioned on B. Unless the first column mentions \nthe conditional mean, median, or the number of transactions, the numbers reported in the table represent the number of \nusers\n\nSeed user type Normal Fictive Underwear Medicine Weapon\n\nNumber of seed users 999 440 468 469 416\n\nNumber of transactions \ninvolving the seed user\n\n151,021 66,215 151,278 92,497 81,970\n\nTotal number of transactions 27,683,860 850,739 2,325,898 925,361 533,963\n\nki = 1 587 (58.8%) 8 (1.8%) 3 (0.6%) 2 (0.4%) 5 (1.2%)\n\nMean ( ki | ki ≥ 2) 195.0 138.3 297.8 184.2 179.7\n\nMedian ( ki | ki ≥ 2) 77.5 61.0 170.0 97.0 86.0\n\nsi = 1 587 (58.8%) 8 (1.8%) 3 (0.6%) 2 (0.4%) 5 (1.2%)\n\nMean ( si | si ≥ 2) 365.1 153.3 325.3 198.1 199.4\n\nMedian ( si | si ≥ 2) 89.0 66.5 175.0 100.0 90.0\n\nsi ≥ 2 412 432 465 467 411\n\nsi/ki = 1 97 (23.5%) 97 (22.5%) 86 (18.5%) 156 (33.4%) 121 (29.4%)\n\nMean ( si/ki | si/ki > 1) 1.413 1.135 1.055 1.066 1.092\n\nMedian ( si/ki | si/ki > 1) 1.124 1.059 1.03 1.031 1.055\n\nki ≥ 2 412 432 465 467 411\n\nSPi = 1 157 (38.1%) 15 (3.5%) 21 (4.5%) 16 (3.4%) 17 (4.1%)\n\nk\nout\ni\n\n= 1 118 (28.6%) 21 (4.9%) 2 (0.4%) 2 (0.4%) 9 (2.2%)\n\nsi ≥ 2 412 432 465 467 411\n\nWSPi = 1 157 (38.1%) 15 (3.5%) 21 (4.5%) 16 (3.4%) 17 (4.1%)\n\ns\nout\ni\n\n= 1 118 (28.6%) 14 (3.2%) 2 (0.4%) 2 (0.4%) 9 (2.2%)\n\nki ≥ 2 412 432 465 467 411\n\nCi = 0 118 (28.6%) 152 (35.2%) 108 (23.2%) 154 (33.0%) 128 (31.1%)\n\nMean ( Ci | Ci > 0) 8.554× 10\n−3\n\n8.348× 10\n−3 9.500× 10\n\n−4\n2.231× 10\n\n−3\n3.810× 10\n\n−3\n\nMedian ( Ci | Ci > 0) 2.411× 10\n−3\n\n2.039× 10\n−3 5.288× 10\n\n−4\n6.494× 10\n\n−4\n1.337× 10\n\n−3\n\nTri ≥ 2 262 241 317 251 244\n\nmi = 0 17 (6.5%) 27 (11.2%) 54 (17.0%) 44 (17.5%) 32 (13.1%)\n\nmi = 1 12 (4.6%) 9 (3.7%) 4 (1.3%) 6 (2.4%) 11 (4.5%)\n\nMean ( mi | mi > 0) 8.554× 10\n−3\n\n8.348× 10\n−3 9.500× 10\n\n−4\n2.231× 10\n\n−3\n3.810× 10\n\n−3\n\nMedian ( mi | mi > 0) 2.411× 10\n−3\n\n2.039× 10\n−3 5.288× 10\n\n−4\n6.494× 10\n\n−4\n1.337× 10\n\n−3\n\nFFi + CYi ≥ 1 294 280 357 313 283\n\nCYPi = 0 234 (79.6%) 188 (67.1%) 222 (62.2%) 227 (72.5%) 202 (71.4%)\n\nMean ( CYPi | CYPi > 0) 1.987× 10\n−2\n\n7.367× 10\n−2\n\n6.739× 10\n−2\n\n8.551× 10\n−2\n\n5.544× 10\n−2\n\nMedian ( CYPi | CYPi > 0) 1.521× 10\n−2\n\n4.481× 10\n−2\n\n3.396× 10\n−2\n\n3.822× 10\n−2\n\n3.618× 10\n−2\n\n\n\nPage 10 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nindicating that a relatively large fraction of normal users is almost exclusive buyer or \nseller. Note that, by definition, the sell probability is at least 1/(k ini + kouti ) because our \nsamples are sellers. Therefore, a peak around the sell probability of zero implies that \nthe users probably have no or few sell transactions apart from the one sell transaction \nbased on which the users have been sampled as seller. In contrast, the distribution \nfor any fraudulent type is relatively flat. Figure  5b shows the relationships between \nthe unweighted sell probability and the degree. On the dashed line in Fig. 5b, the sell \nprobability is equal to 1/(k ini + kouti ) , indicating that the node has kouti = 1 , which is \nthe smallest possible out-degree. The users on this line were buyers in all but one \n\na b\n\nFig. 4 Survival probability of the average number of transactions per neighbor. a si/ki for all nodes. b si/ki for \nthe nodes with si/ki > 1\n\na b\n\nc d\n\nFig. 5 Sell probability for each user type. a Distribution of the unweighted sell probability. b Relationship \nbetween the degree and the unweighted sell probability. c Distribution of the weighted sell probability. d \nRelationship between the node strength and the weighted sell probability. The dashed lines in b, d indicate \n1/(k in\n\ni\n+ k\n\nout\ni\n\n) and 1/(sin\ni\n+ s\n\nout\ni\n\n) , respectively\n\n\n\nPage 11 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\ntransaction. Figure 5b indicates that a majority of such users are normal as opposed \nto fraudulent users, which is quantitatively confirmed in Table 1. We also found that \nmost of the normal users were either on the horizontal line with the sell probability \nof one (38.1% of the normal users with ki ≥ 2 ; see Table 1 for the corresponding frac-\ntions of normal users with ki = 1 ) or on the dashed line (28.6%). This is not the case \nfor any type of fraudulent user (Table 1).\n\nThe distribution of the weighted sell probability for the different user types and the \nrelationships between the weighted sell probability and the node strength are shown \nin Fig.  5c, d, respectively. The results are similar to the case of the unweighted sell \nprobability in two aspects. First, the normal users and the fraudulent users form dis-\ntinct frequency distributions (Fig. 5c). Second, most of the normal users are either on \nthe horizontal line with the weighted sell probability of one or on the dashed line with \nthe smallest possible weighted sell probability, i.e., 1/si (Fig. 5d, Table 1).\n\nThe survival probability of the local clustering coefficient is shown in Fig.  6a. It \nshould be noted that, in this analysis, we confined ourselves to the users with ki ≥ 2 \nbecause Ci is undefined when ki = 1 . We found that the number of users with Ci = 0 is \nnot considerably different between the normal and fraudulent users (also see Table 1). \nFigure  6b shows the survival probability of Ci conditioned on Ci > 0 . The normal \nusers tend to have a larger value of Ci than fraudulent users, whereas this tendency is \nnot strong (Table 1).\n\nThe survival probability of the triangle congregation is shown in Fig. 7a. Contrary to \nour hypothesis, there is no clear difference between the distribution of the normal and \nfraudulent users. The triangle congregation tends to be large when the node strength \nis small (Fig. 7b) and the local clustering coefficient is large (Fig. 7d). It depends little \non the weighted sell probability (Fig. 7c). However, we did not find clear differences in \nthe triangle congregation between the normal and fraudulent users (also see Table 1).\n\nThe survival probability of the cycle probability is shown in Fig. 8a. A large fraction \nof any type of users has CYPi = 0 (Table 1). When the users with CYPi = 0 are dis-\ncarded, the normal users tend to have a smaller value of CYPi than any type of fraudu-\nlent users (Fig. 8b, Table 1).\n\na b\n\nFig. 6 Local clustering coefficient for each user type. a Survival probability. b Survival probability conditioned \non Ci > 0\n\n\n\nPage 12 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nClassification of users\n\nBased on the eight indices whose descriptive statistics were analyzed in the previ-\nous section, we defined 12 features and fed them to the random forest classifier. The \naim of the classifier is to distinguish between normal and fraudulent users. The first \nfeature is binary and whether the degree ki = 1 or ki ≥ 2 . The second feature is also \nbinary and whether the node strength si = 1 or si ≥ 2 . The third feature is si/ki , which \nis a real number greater than or equal to 1. The fourth feature is binary and whether the \nunweighted sell probability SPi = 1 or SPi < 1 . The fifth feature is binary and whether \n\na b\n\nc d\n\nFig. 7 Triangle congregation for each user type. a Survival probability. b Relationship between the triangle \ncongregation, mi , and the node strength. c Relationship between mi and the weighted sell probability. d \nRelationship between mi and the local clustering coefficient\n\na b\n\nFig. 8 Cycle probability for each user type. a Survival probability. b Survival probability conditioned on \nCYPi > 0\n\n\n\nPage 13 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nSPi = 1/(k ini + kouti ) or SPi > 1/(k ini + kouti ) , i.e., whether kouti = 1 or kouti > 1 . The sixth \nfeature is SPi , which ranges between 0 and 1. The seventh feature is binary and whether \nthe weighted sell probability WSPi = 1 or WSPi < 1 . The eighth feature is binary and \nwhether WSPi = 1/(sini + souti ) or WSPi > 1/(sini + souti ) , i.e., whether souti = 1 or \nsouti > 1 . The ninth feature is WSPi , which ranges between 0 and 1. The tenth feature is \nthe local clustering coefficient Ci , which ranges between 0 and 1. When ki = 1 , the local \nclustering coefficient is undefined. In this case, we set Ci = − 1 . The eleventh feature is \nthe triangle congregation mi , which ranges between 0 and 1. When there is no triangle \nor only one triangle involving vi , one cannot calculate mi . In this case, we set mi = − 1 . \nFinally, the twelfth feature is the cycle probability CYPi , which ranges between 0 and 1. \nWhen there is neither feedforward nor cyclic triangle involving vi , CYPi is undefined. In \nthis case, we set CYPi = − 1.\n\nThe ROC and PR curves when all the 12 features of users are used and the fraudu-\nlent type is fictive transactions are shown in Fig. 9a, b, respectively. Each thin line cor-\nresponds to one of the 100 classifiers. The thick lines correspond to the average of the \n100 lines. The dashed lines correspond to the uniformly random classification. Figure 9 \nindicates that the classification performance seems to be high. Quantitatively, for this \nand the other types of fraudulent users, the AUC values always exceeded 0.91 (Table 2).\n\nThe importance of each feature in the classifier is shown in Fig.  10a, separately for \nthe different fraud types. The importance of each feature is similar across the different \ntypes of fraud. Figure 10a indicates that the average number of transactions per neighbor \n(i.e., si/ki ), whether or not kouti = 1 (i.e., SPi = 1/(k ini + kouti ) ), whether or not souti = 1 \n(i.e., WSPi = 1/(sini + souti ) ), and the weighted sell probability (i.e., WSPi ) are the four \nfeatures of the highest importance. Given the results of the descriptive statistics in the \nprevious section, a small value of si/ki , kouti  = 1 , souti  = 1 , and a moderate WSPi value \nstrongly suggest that the user may be fraudulent.\n\nFigure 10a also suggests that the features based on the triangles, i.e., Ci , mi , and CYPi , \nare not strong contributors to the classifier’s performance. Because these features are the \nonly ones that require the information about the connectivity between pairs of neigh-\nbors of the focal node, it is practically beneficial if one can realize a similar classification \n\na b\n\nFig. 9 ROC and PR curves when the normal users and those involved in fictive transactions are classified. a \nROC curves. b PR curves. Each thin line corresponds to one of the 100 classifiers. The thick lines correspond to \nthe average of the 100 lines. The dashed lines correspond to the uniformly random classification\n\n\n\nPage 14 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nperformance without using these features; then only the information on the connectivity \nof the focal users is required. To explore this possibility, we constructed the random for-\nest classifier using the nine out of the twelve features that do not require the connectivity \nbetween neighbors of the focal node. The mean AUC values for the ROC and PR curves \nare shown in Table 2. We find that, despite some reduction in the performance scores \nrelative to the case of the classifier using all the 12 features, the AUC values with the \nnine features are still large, all exceeding 0.88. The permutation importance of the nine \nfeatures is shown in Fig. 10b. The results are similar to those when all the 12 features are \nused, although the importance of WSPi considerably increased in the case of the nine \nfeatures (Fig. 10a).\n\nMore than half of the normal users have ki = 1 , and there are few fraudulent users \nwith ki = 1 in each fraud category (Table 1). The classification between the normal and \nfraudulent users may be an easy problem for this reason, leading to the large AUC val-\nues. To exclude this possibility, we carried out a classification test for the subdata in \nwhich the normal and fraudulent users with ki = 1 were excluded, leaving 412 normal \nusers and a similar number of fraudulent users in each category (Table 1). We did not \n\na b\n\nFig. 10 Permutation importance of the features in the random forest classifier. a 12 features. b 9 features. The \nbars indicate the average over the 100 classifiers. The error bars indicate standard deviation\n\nTable 2 AUC values for the random forest classifiers\n\nThe average and standard deviation were calculated based on the 100 classifiers\n\nFictive Underwear Medicine Weapon\n\n12 features\n\nROC 0.962 ± 0.003 0.981 ± 0.001 0.979 ± 0.003 0.969 ± 0.004\n\nPR 0.916 ± 0.009 0.948 ± 0.006 0.947 ± 0.005 0.916 ± 0.015\n\n9 features\n\nROC 0.951 ± 0.003 0.973 ± 0.003 0.971 ± 0.003 0.961 ± 0.004\n\nPR 0.889 ± 0.009 0.923 ± 0.010 0.930 ± 0.009 0.888 ± 0.025\n\n\n\nPage 15 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\ncarry out subsampling because the number of the negative and positive samples were \nsimilar. Instead, we generated 100 different sets of train and test samples and built a clas-\nsifier based on each set of train and test samples. The AUC values when either 10 or 7 \nfeatures (i.e., the features excluding whether or not ki = 1 and whether or not si = 1 ) are \nused are shown in Table 3. The table indicates that the AUC values are still competitively \nlarge while they are smaller than those when whether or not ki = 1 and whether or not \nsi = 1 are used as features (Table 2).\n\nDiscussion\nWe showed that a random forest classifier using network features of users distinguished \ndifferent types of fraudulent users from normal users with approximately 0.91–0.98 in \nterms of the AUC. We only used the information about local transaction networks cen-\ntered around focal users to synthesize their features. We did so because it is better in \npractice not to demand the information about global transaction networks due to the \nlarge number of users. It should be noted that AUC values of ≈ 0.88–0.97 was also real-\nized when we only used the information about the connectivity of the focal user, not the \nconnectivity between the neighbors of the focal user. This result has a practical advan-\ntage when the present fraud-detection method is implemented online because it allows \none to classify users with a smaller amount of data per user.\n\nThe random forest classifier is an arbitrary choice. One can alternatively use a different \nlinear or nonlinear classifier to pursue a higher classification performance. This is left as \nfuture work. Other future tasks include the generalizability of the present results to dif-\nferent types of fraudulent transactions, such as resale tickets, pornography, and stolen \nitems, and to different platforms. In particular, if a classifier trained with test samples \nfrom fraudulent users of a particular type and normal users is effective at detecting dif-\nferent types of fraud, the classifier will also be potentially useful for detecting unknown \ntypes of fraudulent transactions. It is also a potentially relevant question to assess the \nclassification performance when one pools different types of fraud as a single positive \ncategory to train a classifier.\n\nPrior network-based fraud detection has employed either global or local network \nproperties to characterize nodes. Global network properties refer to those that require \nthe structure of the entire network for calculating a quantity for individual nodes, such \nas the connected component (Šubelj et al. 2011; Savage et al. 2017; Wang et al. 2018), \nbetweenness centrality (Šubelj et al. 2011; Dreżewski et al. 2015; Colladon and Remondi \n2017), user’s suspiciousness determined by belief propagation (Chau et al. 2006; Pandit \n\nTable 3 AUC values for the random forest classifiers excluding users with ki = 1\n\nThe average and standard deviation were calculated based on the 100 classifiers\n\nFictive Underwear Medicine Weapon\n\n10 features\n\nROC 0.925 ± 0.016 0.950 ± 0.013 0.954 ± 0.012 0.916 ± 0.019\n\nPR 0.923 ± 0.019 0.950 ± 0.018 0.954 ± 0.016 0.911 ± 0.023\n\n7 features\n\nROC 0.886 ± 0.020 0.921 ± 0.015 0.933 ± 0.014 0.899 ± 0.020\n\nPR 0.874 ± 0.027 0.901 ± 0.021 0.928 ± 0.019 0.880 ± 0.028\n\n\n\nPage 16 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\net al. 2007; Akoglu et al. 2013; Bangcharoensap et al. 2015; Van Vlasselaer et al. 2015, \n2016; Li et al. 2017; Hu et al. 2017), dense subgraphs including the case of communities \n(Šubelj et al. 2011; Bhat and Abulaish 2013; Ferrara et al. 2014; Jiang et al. 2014; Hooi \net al. 2016; Liu et al. 2016; Shchur et al. 2018), and k-core (Wang and Chiu 2008; Rasheed \net  al. 2018). Although many of these methods have accrued a high classification per-\nformance, they require the information about the entire network. Obtaining such data \nmay be difficult when the network is large or rapidly evolving over time, thus potentially \ncompromising the computation speed, memory requirement, and the accuracy of the \ninformation on the nodes and edges. Alternatively, other methods employed local net-\nwork properties such as the degree including the case of directed and/or weighted net-\nworks (Chau et al. 2006; Akoglu et al. 2010; Šubelj et al. 2011; Yanchun et al. 2011; Bhat \nand Abulaish 2013; Bangcharoensap et al. 2015; Dreżewski et  al. 2015; Monamo et al. \n2016; Van Vlasselaer et al. 2016; Colladon and Remondi 2017) and the abundance of tri-\nangles and quadrangles (Monamo et al. 2016; Van Vlasselaer et al. 2016). The use of local \nnetwork properties may be advantageous in industrial contexts, particularly to test sam-\npled users, because local quantities can be rapidly calculated given a seed node. Another \nreason for which we focused on local properties was that we could not obtain the global \nnetwork structure for computational reasons. It should be noted that, while the use of \nglobal network properties in addition to local ones may improve the classification accu-\nracy (Bhat and Abulaish 2013), the present local method attained a similar classification \nperformance to those based on global network properties, i.e., 0.880–0.986 in terms of \nthe ROC AUC (Šubelj et al. 2011; Van Vlasselaer et al. 2015; Van Vlasselaer et al. 2016; \nHu et al. 2017; Li et al. 2017; Savage et al. 2017).\n\nA prior study using data from the same marketplace, Mercari, aimed to distinguish \nbetween desirable non-professional frequent sellers and undesirable professional sellers \n(Yamamoto et al. 2019). The authors used information about user profiles, item descrip-\ntions, and other behavioral data such as the number of purchases per day. In contrast, \nwe focused on local network features of the users (while a quantity similar to WSPi was \nused as a feature in Yamamoto et al. (2019)). In addition, we used specific types of fraud-\nulent transactions, whereas Yamamoto et al. (2019) focused on problematic transactions \nas a single broad category. How the present results generalize to different categoriza-\ntions of fraudulent transactions, the platform’s different data such as their US market \ndata, and similar data obtained from other online marketplaces is unknown. Combining \nnetwork and non-network features may realize a better classification performance . Fur-\nthermore, using the information about the time of the transactions may also yield better \nclassification. Using the time information allows us to ask new questions such as predic-\ntion of users’ behavior. These topics warrant future work.\n\nAbbreviations\nC2C: Consumer-to-consumer; ROC: Receiver operating characteristic; PR: Precision–recall; AUC : Area under the curve.\n\nAcknowledgements\nThis work was carried out using the computational facilities of the Advanced Computing Research Centre, University of \nBristol.\n\nAuthors’ contributions\nShun Kodate analyzed data, developed methodology, visualized the results, and drafted the manuscript; RC curated data \nand critically revised the manuscript; Shunya Kimura coordinated the study, acquired funding, and critically revised the \nmanuscript; NM coordinated the study, acquired funding, developed methodology, drafted the manuscript. All authors \n\n\n\nPage 17 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\ngave final approval for publication and agreed to be held accountable for the work performed therein. All authors read \nand approved the final manuscript.\n\nFunding\nThe authors acknowledge financial support by Mercari, Inc. S. Kodate was supported in part by the Top Global University \nProject from the Ministry of Education, Culture, Sports, Science and Technology (MEXT) of Japan.\n\nAvailability of data and materials\nMercari, Inc. approved the use of the data for the present study under the condition that the data were hashed and \nonly released to the collaborators of the project (i.e., the first and last authors, because the second and third authors \nare employees of the company). The figures and tables of the present paper are summary statistics of the data and not \nsufficient on their own for others to replicate the results of the present study. Although the data have been hashed, the \ncompany cannot share the data with the public. This is because, if anybody traces the transaction data on the Mercari’s \nweb platform and checks them against the hashed data, that person would be able to identify individual users including \ntheir private information. Therefore, hashing/anonymizing does not help to guarantee the users’ privacy. Any bona fide \nresearcher could approach the company (Shunya Kimura: kimuras@mercari.com and Ryusuke Chiba: metalunk@mercari.\ncom) to seek access to the complete dataset. However, for the aforementioned reasons, such an attempt is unlikely to be \nsuccessful. The users were made aware that their data may be used for the present research because the Mercari’s terms \nof use (in Japanese only: https ://www.merca ri.com/jp/tos/), Article 20, Term 2, states that their data can be used for \nresearch by the company and by those who the company permits.\n\nEthics approval and consent to participate\nMercari, Inc. approved the use of the data for the present study under the condition that the data were hashed and \nonly released to the collaborators of the project (i.e., the first and last authors, because the second and third authors are \nemployees of the company).\n\nCompeting interests\nThe second and third authors are employees of the company that provided the data analysed in the present manuscript. \nHowever, this fact does not cause any conflict of interest because the analyses, results and their interpretation are free of \nany bias towards the merit of the company.\n\nAuthor details\n1 Graduate School of Information Sciences, Tohoku University, Sendai 980-8579, Japan. 2 Department of Engineering \nMathematics, University of Bristol, Bristol BS8 1UB, UK. 3 Mercari, Inc., Tokyo 106-6118, Japan. 4 Department of Mathemat-\nics, University at Buffalo, Buffalo, NY 14260-2900, USA. 5 Computational and Data-Enabled Science and Engineering \nProgram, University at Buffalo, Buffalo, NY 14260-5030, USA. \n\nReceived: 12 August 2020   Accepted: 23 October 2020\n\nReferences\nAbdallah A, Maarof MA, Zainal A (2016) Fraud detection system: a survey. J Netw Comput Appl 68:90–113\nAkoglu L, McGlohon M, Faloutsos C (2010) Oddball: spotting anomalies in weighted graphs. In: Pacific-Asia conference on \n\nknowledge discovery and data mining, pp 410–421\nAkoglu L, Chandy R, Faloutsos C (2013) Opinion fraud detection in online reviews by network effects. In: 7th international \n\nAAAI conference on weblogs and social media, pp 2–11\nAkoglu L, Tong H, Koutra D (2015) Graph based anomaly detection and description: a survey. Data Min Knowl Discov \n\n29:626–688\nAltmann A, Toloşi L, Sander O, Lengauer T (2010) Permutation importance: a corrected feature importance measure. Bioinfo \n\n26:1340–1347\nAnderson R, Barton C, Böhme R, Clayton R, Van Eeten MJ, Levi M, Moore T, Savage S (2013) Measuring the cost of cybercrime. \n\nIn: The economics of information security and privacy. Springer, Berlin, pp 265–300\nBangcharoensap P, Kobayashi H, Shimizu N, Yamauchi S, Murata T (2015) Two step graph-based semi-supervised learning \n\nfor online auction fraud detection. In: Joint European conference on machine learning and knowledge discovery in \ndatabases, pp 165–179\n\nBarrat A, Barthelemy M, Pastor-Satorras R, Vespignani A (2004) The architecture of complex weighted networks. Proc Natl \nAcad Sci USA 101:3747–3752\n\nBhat SY, Abulaish M (2013) Community-based features for identifying spammers in online social networks. In: 2013 IEEE/ACM \ninternational conference on advances in social networks analysis and mining (ASONAM 2013), pp 100–107\n\nBhowmick A, Hazarika SM (2016) Machine learning for e-mail spam filtering: review, techniques and trends. Preprint arXiv \n:1606.01042 \n\nBolton RJ, Hand DJ (2002) Statistical fraud detection: a review. Stat Sci 17:235–249\nBreiman L (2001) Random forests. Mach Learn 45:5–32\nBreiman L, Friedman JH, Olshen RA, Stone CJ (1984) Classification and regression trees. Chapman & Hall, Boca Raton\nChau DH, Pandit S, Faloutsos C (2006) Detecting fraudulent personalities in networks of online auctioneers. In: European \n\nconference on principles of data mining and knowledge discovery, pp 103–114\nColladon AF, Remondi E (2017) Using social network analysis to prevent money laundering. Expert Syst Appl 67:49–58\nDreżewski R, Sepielak J, Filipkowski W (2015) The application of social network analysis algorithms in a system supporting \n\nmoney laundering detection. Inf Sci 295:18–32\nFerrara E, De Meo P, Catanese S, Fiumara G (2014) Detecting criminal organizations in mobile phone networks. Expert Syst \n\nAppl 41:5733–5750\nGoogle LLC and White Ops, Inc (2018) The Hunt for 3ve. https ://servi ces.googl e.com/fh/files /blogs /3ve_googl e_white \n\nops_white paper _final _nov_2018.pdf. Accessed: 10 May 2019\n\n Published online: 16 November 2020 \n\nhttps://www.mercari.com/jp/tos/\nhttp://arxiv.org/abs/1606.01042\nhttp://arxiv.org/abs/1606.01042\nhttps://services.google.com/fh/files/blogs/3ve_google_whiteops_whitepaper_final_nov_2018.pdf\nhttps://services.google.com/fh/files/blogs/3ve_google_whiteops_whitepaper_final_nov_2018.pdf\n\n\nPage 18 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nHastie T, Tibshirani R, Friedman J (2009) The elements of statistical learning: data mining, inference, and prediction. Springer, \nNew York\n\nHayes B (2007) How many ways can you spell v1@gra? Am Sci 95:298–302\nHooi B, Song HA, Beutel A, Shah N, Shin K, Faloutsos C (2016) Fraudar: bounding graph fraud in the face of camouflage. In: \n\nProceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pp 895–904\nHu J, Liang J, Dong S (2017) ibgp: a bipartite graph propagation approach for mobile advertising fraud detection. Mobile Inf \n\nSyst 2017:1–12\nJiang M, Cui P, Beutel A, Faloutsos C, Yang S (2014) Inferring strange behavior from connectivity pattern in social networks. In: \n\nPacific-Asia conference on knowledge discovery and data mining, pp 126–138\nLi Y, Sun Y, Contractor N (2017) Graph mining assisted semi-supervised learning for fraudulent cash-out detection. In: Pro-\n\nceedings of the 2017 IEEE/ACM international conference on advances in social networks analysis and mining 2017, pp \n546–553\n\nLiu J, Bier E, Wilson A, Guerra-Gomez JA, Honda T, Sricharan K, Gilpin L, Davies D (2016) Graph analysis for detecting fraud, \nwaste, and abuse in healthcare data. AI Mag 37:33–46\n\nLiu S, Hooi B, Faloutsos C (2017) Holoscope: topology-and-spike aware fraud detection. In: Proceedings of the 2017 ACM on \nconference on information and knowledge management, pp 1539–1548\n\nMcAfee LLC (2019) Economic impact of cybercrime report. https ://www.mcafe e.com/enter prise /en-us/solut ions/lp/econo \nmics-cyber crime .html. Accessed: 25 Apr 2018\n\nMercari Inc (2019) FY2019.6 Q3 Presentation Material. https ://about .merca ri.com/en/ir/libra ry/resul ts/. Accessed 1 Nov 2020\nMilo R, Shen-Orr S, Itzkovitz S, Kashtan N, Chklovskii D, Alon U (2002) Network motifs: simple building blocks of complex \n\nnetworks. Science 298:824–827\nMonamo P, Marivate V, Twala B (2016) Unsupervised learning for robust Bitcoin fraud detection. In: 2016 information security \n\nfor South Africa (ISSA), pp 129–134\nNewman M (2010) Networks: an introduction. Oxford University Press, Oxford\nPalla G, Derényi I, Farkas I, Vicsek T (2005) Uncovering the overlapping community structure of complex networks in nature \n\nand society. Nature 435:814–818\nPandit S, Chau DH, Wang S, Faloutsos C (2007) Netprobe: a fast and scalable system for fraud detection in online auction \n\nnetworks. In: Proceedings of the 16th international conference on world wide web, pp 201–210\nPedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O, Blondel M, Prettenhofer P, Weiss R, Dubourg V et al (2011) \n\nScikit-learn: machine learning in Python. J Mach Learn Res 12:2825–2830\nPhua C, Lee V, Smith K, Gayler R (2010) A comprehensive survey of data mining-based fraud detection research. Preprint arXiv \n\n:1009.6119\nPu C, Webb S (2006) Observed trends in spam construction techniques: a case study of spam evolution. In: CEAS, pp 104–112\nRadicchi F, Castellano C, Cecconi F, Loreto V, Parisi D (2004) Defining and identifying communities in networks. Proc Natl Acad \n\nSci USA 101:2658–2663\nRasheed J, Akram U, Malik AK (2018) Terrorist network analysis and identification of main actors using machine learning tech-\n\nniques. In: Proceedings of the 6th international conference on information technology: IoT and smart city, pp 7–12\nSavage D, Zhang X, Yu X, Chou P, Wang Q (2014) Anomaly detection in online social networks. Soc Netw 39:62–70\nSavage D, Wang Q, Zhang X, Chou P, Yu X (2017) Detection of money laundering groups: supervised learning on small net-\n\nworks. In: Workshops at the 31st AAAI conference on artificial intelligence, pp 43–49\nShchur O, Bojchevski A, Farghal M, Günnemann S, Saber Y (2018) Anomaly detection in car-booking graphs. In: 2018 IEEE \n\ninternational conference on data mining workshops (ICDMW), pp 604–607\nStrobl C, Boulesteix A-L, Zeileis A, Hothorn T (2007) Bias in random forest variable importance measures: illustrations, sources \n\nand a solution. BMC Bioinform 8:25\nŠubelj L, Furlan Š, Bajec M (2011) An expert system for detecting automobile insurance fraud using social network analysis. \n\nExpert Syst Appl 38:1039–1052\nUK Parliament: The Growing Threat of Online Fraud (2017). https ://old.parli ament .uk/busin ess/commi ttees /commi ttees -a-z/\n\ncommo ns-selec t/publi c-accou nts-commi ttee/inqui ries/parli ament -2017/growi ng-threa t-onlin e-fraud -17-19/publi catio \nns/. Accessed 1 Nov 2020\n\nVan Vlasselaer V, Bravo C, Caelen O, Eliassi-Rad T, Akoglu L, Snoeck M, Baesens B (2015) Apate: a novel approach for automated \ncredit card transaction fraud detection using network-based extensions. Decis Support Syst 75:38–48\n\nVan Vlasselaer V, Eliassi-Rad T, Akoglu L, Snoeck M, Baesens B (2016) Gotcha! network-based fraud detection for social security \nfraud. Manag Sci 63:3090–3110\n\nWang J-C, Chiu C-C (2008) Recommending trusted online auction sellers using social network analysis. Expert Syst Appl \n34:1666–1679\n\nWang Z, Gu S, Zhao X, Xu X (2018) Graph-based review spammer group detection. Knowl Inf Syst 55:571–597\nWest J, Bhattacharya M (2016) Intelligent financial fraud detection: a comprehensive review. Comput Secur 57:47–66\nYamamoto H, Sugiyama N, Toriumi F, Kashida H, Yamaguchi T (2019) Angels or demons? Classifying desirable heavy users and \n\nundesirable power sellers in online C2C marketplace. J Comput Soc Sci 2:315–329\nYanchun Z, Wei Z, Changhai Y (2011) Detection of feedback reputation fraud in Taobao using social network theory. In: 2011 \n\ninternational joint conference on service sciences, pp 188–192\n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nhttps://www.mcafee.com/enterprise/en-us/solutions/lp/economics-cybercrime.html\nhttps://www.mcafee.com/enterprise/en-us/solutions/lp/economics-cybercrime.html\nhttps://about.mercari.com/en/ir/library/results/\nhttp://arxiv.org/abs/1009.6119\nhttp://arxiv.org/abs/1009.6119\nhttps://old.parliament.uk/business/committees/committees-a-z/commons-select/public-accounts-committee/inquiries/parliament-2017/growing-threat-online-fraud-17-19/publications/\nhttps://old.parliament.uk/business/committees/committees-a-z/commons-select/public-accounts-committee/inquiries/parliament-2017/growing-threat-online-fraud-17-19/publications/\nhttps://old.parliament.uk/business/committees/committees-a-z/commons-select/public-accounts-committee/inquiries/parliament-2017/growing-threat-online-fraud-17-19/publications/\n\n\tDetecting problematic transactions in a consumer-to-consumer e-commerce network\n\tAbstract \n\tIntroduction\n\tMaterials and methods\n\tData\n\tNetwork analysis\n\tRandom forest classifier\n\n\tResults\n\tDescriptive statistics\n\tClassification of users\n\n\tDiscussion\n\tAcknowledgements\n\tReferences\n\n\n",
      "text": [
        "a b O- 0 0404 O C 0 O 0 20 04 20070",
        "Published online: 16 November 2020"
      ],
      "layoutText": [
        "{\"language\":\"en\",\"text\":\"a b O- 0 0404 O C 0 O 0 20 04 20070\",\"lines\":[{\"boundingBox\":[{\"x\":40,\"y\":30},{\"x\":67,\"y\":28},{\"x\":67,\"y\":57},{\"x\":40,\"y\":59}],\"text\":\"a\"},{\"boundingBox\":[{\"x\":750,\"y\":23},{\"x\":778,\"y\":24},{\"x\":777,\"y\":58},{\"x\":750,\"y\":57}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1219,\"y\":129},{\"x\":1211,\"y\":157},{\"x\":1186,\"y\":148},{\"x\":1195,\"y\":120}],\"text\":\"O-\"},{\"boundingBox\":[{\"x\":1063,\"y\":151},{\"x\":1075,\"y\":190},{\"x\":1058,\"y\":197},{\"x\":1045,\"y\":158}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":1249,\"y\":176},{\"x\":1182,\"y\":262},{\"x\":1163,\"y\":246},{\"x\":1226,\"y\":160}],\"text\":\"0404\"},{\"boundingBox\":[{\"x\":772,\"y\":925},{\"x\":772,\"y\":962},{\"x\":754,\"y\":965},{\"x\":752,\"y\":928}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":37,\"y\":933},{\"x\":69,\"y\":933},{\"x\":71,\"y\":962},{\"x\":40,\"y\":962}],\"text\":\"C\"},{\"boundingBox\":[{\"x\":1080,\"y\":1074},{\"x\":1083,\"y\":1123},{\"x\":1053,\"y\":1124},{\"x\":1050,\"y\":1074}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":1116,\"y\":1186},{\"x\":1142,\"y\":1172},{\"x\":1150,\"y\":1190},{\"x\":1123,\"y\":1203}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":1261,\"y\":1349},{\"x\":1346,\"y\":1351},{\"x\":1346,\"y\":1372},{\"x\":1261,\"y\":1370}],\"text\":\"0 20\"},{\"boundingBox\":[{\"x\":887,\"y\":1375},{\"x\":939,\"y\":1369},{\"x\":942,\"y\":1393},{\"x\":888,\"y\":1398}],\"text\":\"04\"},{\"boundingBox\":[{\"x\":1005,\"y\":1454},{\"x\":928,\"y\":1595},{\"x\":903,\"y\":1581},{\"x\":978,\"y\":1439}],\"text\":\"20070\"}],\"words\":[{\"boundingBox\":[{\"x\":40,\"y\":30},{\"x\":57,\"y\":29},{\"x\":59,\"y\":57},{\"x\":41,\"y\":58}],\"text\":\"a\"},{\"boundingBox\":[{\"x\":750,\"y\":23},{\"x\":769,\"y\":24},{\"x\":768,\"y\":58},{\"x\":750,\"y\":57}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1219,\"y\":131},{\"x\":1212,\"y\":154},{\"x\":1187,\"y\":146},{\"x\":1194,\"y\":123}],\"text\":\"O-\"},{\"boundingBox\":[{\"x\":1071,\"y\":175},{\"x\":1075,\"y\":186},{\"x\":1057,\"y\":192},{\"x\":1053,\"y\":180}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":1238,\"y\":190},{\"x\":1183,\"y\":259},{\"x\":1165,\"y\":247},{\"x\":1216,\"y\":174}],\"text\":\"0404\"},{\"boundingBox\":[{\"x\":772,\"y\":931},{\"x\":772,\"y\":942},{\"x\":752,\"y\":943},{\"x\":752,\"y\":931}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":38,\"y\":933},{\"x\":55,\"y\":933},{\"x\":55,\"y\":962},{\"x\":38,\"y\":962}],\"text\":\"C\"},{\"boundingBox\":[{\"x\":1083,\"y\":1105},{\"x\":1084,\"y\":1120},{\"x\":1054,\"y\":1122},{\"x\":1053,\"y\":1106}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":1128,\"y\":1178},{\"x\":1139,\"y\":1172},{\"x\":1148,\"y\":1190},{\"x\":1137,\"y\":1196}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":1278,\"y\":1351},{\"x\":1291,\"y\":1351},{\"x\":1288,\"y\":1371},{\"x\":1275,\"y\":1370}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":1310,\"y\":1352},{\"x\":1346,\"y\":1352},{\"x\":1344,\"y\":1373},{\"x\":1307,\"y\":1372}],\"text\":\"20\"},{\"boundingBox\":[{\"x\":902,\"y\":1372},{\"x\":933,\"y\":1369},{\"x\":936,\"y\":1393},{\"x\":904,\"y\":1396}],\"text\":\"04\"},{\"boundingBox\":[{\"x\":1003,\"y\":1458},{\"x\":929,\"y\":1593},{\"x\":904,\"y\":1579},{\"x\":976,\"y\":1443}],\"text\":\"20070\"}]}",
        "{\"language\":\"en\",\"text\":\"Published online: 16 November 2020\",\"lines\":[{\"boundingBox\":[{\"x\":5,\"y\":16},{\"x\":1059,\"y\":16},{\"x\":1059,\"y\":70},{\"x\":5,\"y\":69}],\"text\":\"Published online: 16 November 2020\"}],\"words\":[{\"boundingBox\":[{\"x\":6,\"y\":17},{\"x\":268,\"y\":17},{\"x\":269,\"y\":70},{\"x\":6,\"y\":70}],\"text\":\"Published\"},{\"boundingBox\":[{\"x\":290,\"y\":17},{\"x\":501,\"y\":17},{\"x\":501,\"y\":70},{\"x\":291,\"y\":70}],\"text\":\"online:\"},{\"boundingBox\":[{\"x\":512,\"y\":17},{\"x\":575,\"y\":17},{\"x\":575,\"y\":70},{\"x\":513,\"y\":70}],\"text\":\"16\"},{\"boundingBox\":[{\"x\":593,\"y\":17},{\"x\":899,\"y\":17},{\"x\":899,\"y\":71},{\"x\":594,\"y\":71}],\"text\":\"November\"},{\"boundingBox\":[{\"x\":911,\"y\":17},{\"x\":1051,\"y\":17},{\"x\":1051,\"y\":71},{\"x\":911,\"y\":71}],\"text\":\"2020\"}]}"
      ],
      "pii_entities": [
        {
          "text": "Shun Kodate",
          "type": "Person",
          "subtype": null,
          "offset": 83,
          "length": 11,
          "score": 0.98
        },
        {
          "text": "Ryusuke Chiba",
          "type": "Person",
          "subtype": null,
          "offset": 99,
          "length": 13,
          "score": 0.98
        },
        {
          "text": "Shunya Kimura3",
          "type": "Person",
          "subtype": null,
          "offset": 115,
          "length": 14,
          "score": 0.98
        },
        {
          "text": "Naoki Masuda",
          "type": "Person",
          "subtype": null,
          "offset": 134,
          "length": 12,
          "score": 0.97
        },
        {
          "text": "daily",
          "type": "DateTime",
          "subtype": "Set",
          "offset": 318,
          "length": 5,
          "score": 0.8
        },
        {
          "text": "UK Parliament",
          "type": "Organization",
          "subtype": null,
          "offset": 439,
          "length": 13,
          "score": 0.82
        },
        {
          "text": "2017",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 453,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "McAfee",
          "type": "Organization",
          "subtype": null,
          "offset": 459,
          "length": 6,
          "score": 0.76
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 466,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Anderson",
          "type": "Person",
          "subtype": null,
          "offset": 633,
          "length": 8,
          "score": 0.97
        },
        {
          "text": "2013",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 649,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 983,
          "length": 5,
          "score": 0.92
        },
        {
          "text": "user",
          "type": "PersonType",
          "subtype": null,
          "offset": 1081,
          "length": 4,
          "score": 0.87
        },
        {
          "text": "user",
          "type": "PersonType",
          "subtype": null,
          "offset": 1152,
          "length": 4,
          "score": 0.95
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 1670,
          "length": 5,
          "score": 0.93
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 1716,
          "length": 5,
          "score": 0.91
        },
        {
          "text": "The Author",
          "type": "Organization",
          "subtype": null,
          "offset": 2451,
          "length": 10,
          "score": 0.66
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 2465,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "author",
          "type": "PersonType",
          "subtype": null,
          "offset": 2724,
          "length": 6,
          "score": 0.95
        },
        {
          "text": "copyright holder",
          "type": "PersonType",
          "subtype": null,
          "offset": 3227,
          "length": 16,
          "score": 0.62
        },
        {
          "text": "Kodate",
          "type": "Person",
          "subtype": null,
          "offset": 3344,
          "length": 6,
          "score": 0.93
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 3384,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "https://doi.org/10.1007/s41109",
          "type": "URL",
          "subtype": null,
          "offset": 3397,
          "length": 30,
          "score": 0.8
        },
        {
          "text": "naokimas@buffalo.edu",
          "type": "Email",
          "subtype": null,
          "offset": 3485,
          "length": 20,
          "score": 0.8
        },
        {
          "text": "14260-2900",
          "type": "PhoneNumber",
          "subtype": null,
          "offset": 3574,
          "length": 10,
          "score": 0.8
        },
        {
          "text": "http://orcid.org/0000-0003-1567-801X",
          "type": "URL",
          "subtype": null,
          "offset": 3664,
          "length": 36,
          "score": 0.8
        },
        {
          "text": "http://creativecommons.org/licenses/by/4.0/",
          "type": "URL",
          "subtype": null,
          "offset": 3701,
          "length": 43,
          "score": 0.8
        },
        {
          "text": "http://creativecommons.org/licenses/by/4.0/",
          "type": "URL",
          "subtype": null,
          "offset": 3745,
          "length": 43,
          "score": 0.8
        },
        {
          "text": "http://crossmark.crossref.org/dialog/?doi=10.1007/s41109-020-00330-x",
          "type": "URL",
          "subtype": null,
          "offset": 3789,
          "length": 68,
          "score": 0.8
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 3923,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Bolton",
          "type": "Person",
          "subtype": null,
          "offset": 4059,
          "length": 6,
          "score": 0.92
        },
        {
          "text": "Hand",
          "type": "Person",
          "subtype": null,
          "offset": 4070,
          "length": 4,
          "score": 0.55
        },
        {
          "text": "2002",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 4075,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Phua",
          "type": "Person",
          "subtype": null,
          "offset": 4081,
          "length": 4,
          "score": 0.97
        },
        {
          "text": "2010",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 4094,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Abdallah",
          "type": "Person",
          "subtype": null,
          "offset": 4101,
          "length": 8,
          "score": 0.97
        },
        {
          "text": "2016",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 4117,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "West",
          "type": "Person",
          "subtype": null,
          "offset": 4123,
          "length": 4,
          "score": 0.87
        },
        {
          "text": "Bhattacharya",
          "type": "Person",
          "subtype": null,
          "offset": 4132,
          "length": 12,
          "score": 0.58
        },
        {
          "text": "2016",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 4145,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Bolton",
          "type": "Person",
          "subtype": null,
          "offset": 4412,
          "length": 6,
          "score": 0.81
        },
        {
          "text": "2002",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 4428,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Phua",
          "type": "Person",
          "subtype": null,
          "offset": 4434,
          "length": 4,
          "score": 0.97
        },
        {
          "text": "2010",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 4446,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Abdallah",
          "type": "Person",
          "subtype": null,
          "offset": 4453,
          "length": 8,
          "score": 0.97
        },
        {
          "text": "2016",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 4469,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "West",
          "type": "Person",
          "subtype": null,
          "offset": 4475,
          "length": 4,
          "score": 0.87
        },
        {
          "text": "Bhattacharya",
          "type": "Person",
          "subtype": null,
          "offset": 4484,
          "length": 12,
          "score": 0.58
        },
        {
          "text": "2016",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 4497,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "the week",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 4605,
          "length": 8,
          "score": 0.8
        },
        {
          "text": "user",
          "type": "PersonType",
          "subtype": null,
          "offset": 4635,
          "length": 4,
          "score": 0.75
        },
        {
          "text": "user",
          "type": "PersonType",
          "subtype": null,
          "offset": 4743,
          "length": 4,
          "score": 0.85
        },
        {
          "text": "age",
          "type": "Quantity",
          "subtype": "Age",
          "offset": 4750,
          "length": 3,
          "score": 0.8
        },
        {
          "text": "Abdallah",
          "type": "Person",
          "subtype": null,
          "offset": 4897,
          "length": 8,
          "score": 0.67
        },
        {
          "text": "2016",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 4914,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "fraudsters",
          "type": "PersonType",
          "subtype": null,
          "offset": 4986,
          "length": 10,
          "score": 0.64
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 5013,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Google LLC",
          "type": "Organization",
          "subtype": null,
          "offset": 5019,
          "length": 10,
          "score": 0.79
        },
        {
          "text": "2018",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 5030,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Pu",
          "type": "Person",
          "subtype": null,
          "offset": 5223,
          "length": 2,
          "score": 0.91
        },
        {
          "text": "Webb",
          "type": "Person",
          "subtype": null,
          "offset": 5230,
          "length": 4,
          "score": 0.76
        },
        {
          "text": "2006",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 5235,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Hayes",
          "type": "Person",
          "subtype": null,
          "offset": 5241,
          "length": 5,
          "score": 0.71
        },
        {
          "text": "2007",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 5247,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "2016",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 5276,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 5370,
          "length": 5,
          "score": 0.84
        },
        {
          "text": "may",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 5376,
          "length": 3,
          "score": 0.8
        },
        {
          "text": "Savage",
          "type": "Person",
          "subtype": null,
          "offset": 5547,
          "length": 6,
          "score": 0.96
        },
        {
          "text": "2014",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 5561,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Akoglu",
          "type": "Person",
          "subtype": null,
          "offset": 5567,
          "length": 6,
          "score": 0.95
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 5581,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 5894,
          "length": 5,
          "score": 0.72
        },
        {
          "text": "Akoglu",
          "type": "Person",
          "subtype": null,
          "offset": 5901,
          "length": 6,
          "score": 0.95
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 5915,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Šubelj",
          "type": "Person",
          "subtype": null,
          "offset": 5992,
          "length": 6,
          "score": 0.97
        },
        {
          "text": "2011",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 6007,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Dreżewski",
          "type": "Person",
          "subtype": null,
          "offset": 6032,
          "length": 9,
          "score": 0.96
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 6050,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Colladon",
          "type": "Person",
          "subtype": null,
          "offset": 6057,
          "length": 8,
          "score": 0.52
        },
        {
          "text": "2017",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 6078,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Savage",
          "type": "Person",
          "subtype": null,
          "offset": 6084,
          "length": 6,
          "score": 0.96
        },
        {
          "text": "2017",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 6098,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Liu",
          "type": "Person",
          "subtype": null,
          "offset": 6123,
          "length": 3,
          "score": 0.91
        },
        {
          "text": "2016",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 6134,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Shchur",
          "type": "Person",
          "subtype": null,
          "offset": 6155,
          "length": 6,
          "score": 0.96
        },
        {
          "text": "2018",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 6169,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Van Vlasselaer",
          "type": "Person",
          "subtype": null,
          "offset": 6202,
          "length": 14,
          "score": 0.98
        },
        {
          "text": "2016",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 6224,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Hu",
          "type": "Person",
          "subtype": null,
          "offset": 6252,
          "length": 2,
          "score": 0.89
        },
        {
          "text": "2017",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 6262,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Ferrara",
          "type": "Person",
          "subtype": null,
          "offset": 6293,
          "length": 7,
          "score": 0.9
        },
        {
          "text": "2014",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 6308,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Bhat",
          "type": "Person",
          "subtype": null,
          "offset": 6340,
          "length": 4,
          "score": 0.91
        },
        {
          "text": "Abulaish",
          "type": "Person",
          "subtype": null,
          "offset": 6349,
          "length": 8,
          "score": 0.7
        },
        {
          "text": "2013",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 6358,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Jiang",
          "type": "Person",
          "subtype": null,
          "offset": 6364,
          "length": 5,
          "score": 0.95
        },
        {
          "text": "2014",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 6378,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Hooi",
          "type": "Person",
          "subtype": null,
          "offset": 6384,
          "length": 4,
          "score": 0.96
        },
        {
          "text": "2016",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 6397,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Rasheed",
          "type": "Person",
          "subtype": null,
          "offset": 6403,
          "length": 7,
          "score": 0.97
        },
        {
          "text": "2018",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 6420,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Akoglu",
          "type": "Person",
          "subtype": null,
          "offset": 6449,
          "length": 6,
          "score": 0.95
        },
        {
          "text": "2013",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 6463,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Liu",
          "type": "Person",
          "subtype": null,
          "offset": 6469,
          "length": 3,
          "score": 0.96
        },
        {
          "text": "2017",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 6480,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Wang",
          "type": "Person",
          "subtype": null,
          "offset": 6486,
          "length": 4,
          "score": 0.95
        },
        {
          "text": "2018",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 6498,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Chau",
          "type": "Person",
          "subtype": null,
          "offset": 6538,
          "length": 4,
          "score": 0.93
        },
        {
          "text": "2006",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 6551,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Pandit",
          "type": "Person",
          "subtype": null,
          "offset": 6557,
          "length": 6,
          "score": 0.98
        },
        {
          "text": "2007",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 6572,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Wang",
          "type": "Person",
          "subtype": null,
          "offset": 6578,
          "length": 4,
          "score": 0.94
        },
        {
          "text": "Chiu",
          "type": "Person",
          "subtype": null,
          "offset": 6587,
          "length": 4,
          "score": 0.61
        },
        {
          "text": "2008",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 6592,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Bangcharoensap",
          "type": "Person",
          "subtype": null,
          "offset": 6599,
          "length": 14,
          "score": 0.94
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 6622,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Yanchun",
          "type": "Person",
          "subtype": null,
          "offset": 6628,
          "length": 7,
          "score": 0.87
        },
        {
          "text": "2011",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 6644,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Van Vlas-",
          "type": "Person",
          "subtype": null,
          "offset": 6677,
          "length": 9,
          "score": 0.78
        },
        {
          "text": "selaer",
          "type": "Person",
          "subtype": null,
          "offset": 6687,
          "length": 6,
          "score": 0.93
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 6701,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Li",
          "type": "Person",
          "subtype": null,
          "offset": 6707,
          "length": 2,
          "score": 0.92
        },
        {
          "text": "2017",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 6717,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Monamo",
          "type": "Person",
          "subtype": null,
          "offset": 6752,
          "length": 6,
          "score": 0.87
        },
        {
          "text": "2016",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 6766,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Akoglu",
          "type": "Person",
          "subtype": null,
          "offset": 6800,
          "length": 6,
          "score": 0.94
        },
        {
          "text": "2010",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 6814,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 6845,
          "length": 5,
          "score": 0.67
        },
        {
          "text": "Chau",
          "type": "Person",
          "subtype": null,
          "offset": 7005,
          "length": 4,
          "score": 0.93
        },
        {
          "text": "2006",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 7017,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "authors",
          "type": "PersonType",
          "subtype": null,
          "offset": 7034,
          "length": 7,
          "score": 0.76
        },
        {
          "text": "eBay",
          "type": "Organization",
          "subtype": null,
          "offset": 7228,
          "length": 4,
          "score": 0.88
        },
        {
          "text": "Pandit",
          "type": "Person",
          "subtype": null,
          "offset": 7234,
          "length": 6,
          "score": 0.95
        },
        {
          "text": "2007",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 7248,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Mercari",
          "type": "Organization",
          "subtype": null,
          "offset": 7370,
          "length": 7,
          "score": 0.89
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 7475,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "monthly",
          "type": "DateTime",
          "subtype": "Set",
          "offset": 7502,
          "length": 7,
          "score": 0.8
        },
        {
          "text": "Mercari",
          "type": "Organization",
          "subtype": null,
          "offset": 7608,
          "length": 7,
          "score": 0.59
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 7616,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 7697,
          "length": 5,
          "score": 0.56
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 7837,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Chau",
          "type": "Person",
          "subtype": null,
          "offset": 7873,
          "length": 4,
          "score": 0.94
        },
        {
          "text": "2006",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 7885,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Pandit",
          "type": "Person",
          "subtype": null,
          "offset": 7891,
          "length": 6,
          "score": 0.98
        },
        {
          "text": "2007",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 7905,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Wang",
          "type": "Person",
          "subtype": null,
          "offset": 7911,
          "length": 4,
          "score": 0.94
        },
        {
          "text": "Chiu",
          "type": "Person",
          "subtype": null,
          "offset": 7920,
          "length": 4,
          "score": 0.61
        },
        {
          "text": "2008",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 7925,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "2011",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 7948,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Chau",
          "type": "Person",
          "subtype": null,
          "offset": 8158,
          "length": 4,
          "score": 0.94
        },
        {
          "text": "2006",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 8170,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Pandit",
          "type": "Person",
          "subtype": null,
          "offset": 8176,
          "length": 6,
          "score": 0.98
        },
        {
          "text": "2007",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 8190,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Wang",
          "type": "Person",
          "subtype": null,
          "offset": 8197,
          "length": 4,
          "score": 0.94
        },
        {
          "text": "Chiu",
          "type": "Person",
          "subtype": null,
          "offset": 8206,
          "length": 4,
          "score": 0.61
        },
        {
          "text": "2008",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 8211,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Šubelj",
          "type": "Person",
          "subtype": null,
          "offset": 8217,
          "length": 6,
          "score": 0.98
        },
        {
          "text": "2011",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 8232,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Akoglu",
          "type": "Person",
          "subtype": null,
          "offset": 8238,
          "length": 6,
          "score": 0.96
        },
        {
          "text": "2013",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 8253,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Bhat",
          "type": "Person",
          "subtype": null,
          "offset": 8259,
          "length": 4,
          "score": 0.82
        },
        {
          "text": "2013",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 8277,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Ferrara",
          "type": "Person",
          "subtype": null,
          "offset": 8284,
          "length": 7,
          "score": 0.92
        },
        {
          "text": "2014",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 8299,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Jiang",
          "type": "Person",
          "subtype": null,
          "offset": 8305,
          "length": 5,
          "score": 0.95
        },
        {
          "text": "2014",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 8318,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Bangcharoensap",
          "type": "Person",
          "subtype": null,
          "offset": 8324,
          "length": 14,
          "score": 0.94
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 8346,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Dreżewski",
          "type": "Person",
          "subtype": null,
          "offset": 8352,
          "length": 9,
          "score": 0.98
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 8369,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Van Vlasselaer",
          "type": "Person",
          "subtype": null,
          "offset": 8376,
          "length": 14,
          "score": 0.98
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 8398,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Hooi",
          "type": "Person",
          "subtype": null,
          "offset": 8404,
          "length": 4,
          "score": 0.96
        },
        {
          "text": "2016",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 8416,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Liu",
          "type": "Person",
          "subtype": null,
          "offset": 8422,
          "length": 3,
          "score": 0.96
        },
        {
          "text": "2016",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 8433,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Van Vlasselaer",
          "type": "Person",
          "subtype": null,
          "offset": 8439,
          "length": 14,
          "score": 0.98
        },
        {
          "text": "2016",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 8461,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Colladon",
          "type": "Person",
          "subtype": null,
          "offset": 8468,
          "length": 8,
          "score": 0.52
        },
        {
          "text": "2017",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 8489,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Hu",
          "type": "Person",
          "subtype": null,
          "offset": 8495,
          "length": 2,
          "score": 0.94
        },
        {
          "text": "2017",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 8505,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Li",
          "type": "Person",
          "subtype": null,
          "offset": 8511,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "2017",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 8521,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Liu",
          "type": "Person",
          "subtype": null,
          "offset": 8527,
          "length": 3,
          "score": 0.96
        },
        {
          "text": "2017",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 8538,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Savage",
          "type": "Person",
          "subtype": null,
          "offset": 8544,
          "length": 6,
          "score": 0.97
        },
        {
          "text": "2017",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 8559,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Shchur",
          "type": "Person",
          "subtype": null,
          "offset": 8565,
          "length": 6,
          "score": 0.97
        },
        {
          "text": "2018",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 8579,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Rasheed",
          "type": "Person",
          "subtype": null,
          "offset": 8585,
          "length": 7,
          "score": 0.95
        },
        {
          "text": "2018",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 8600,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Wang",
          "type": "Person",
          "subtype": null,
          "offset": 8606,
          "length": 4,
          "score": 0.81
        },
        {
          "text": "2018",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 8618,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Chau",
          "type": "Person",
          "subtype": null,
          "offset": 8767,
          "length": 4,
          "score": 0.92
        },
        {
          "text": "2006",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 8779,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Akoglu",
          "type": "Person",
          "subtype": null,
          "offset": 8785,
          "length": 6,
          "score": 0.97
        },
        {
          "text": "2010",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 8799,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Šubelj",
          "type": "Person",
          "subtype": null,
          "offset": 8805,
          "length": 6,
          "score": 0.98
        },
        {
          "text": "2011",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 8819,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Yan",
          "type": "Person",
          "subtype": null,
          "offset": 8825,
          "length": 3,
          "score": 0.77
        },
        {
          "text": "chun",
          "type": "Person",
          "subtype": null,
          "offset": 8830,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "2011",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 8842,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Bhat",
          "type": "Person",
          "subtype": null,
          "offset": 8848,
          "length": 4,
          "score": 0.82
        },
        {
          "text": "2013",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 8866,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Bangcharoensap",
          "type": "Person",
          "subtype": null,
          "offset": 8872,
          "length": 14,
          "score": 0.94
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 8894,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Dreżewski",
          "type": "Person",
          "subtype": null,
          "offset": 8900,
          "length": 9,
          "score": 0.98
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 8918,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Monamo",
          "type": "Person",
          "subtype": null,
          "offset": 8924,
          "length": 6,
          "score": 0.97
        },
        {
          "text": "2016",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 8938,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Van Vlasselaer",
          "type": "Person",
          "subtype": null,
          "offset": 8944,
          "length": 14,
          "score": 0.98
        },
        {
          "text": "2016",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 8966,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Colladon",
          "type": "Person",
          "subtype": null,
          "offset": 8972,
          "length": 8,
          "score": 0.82
        },
        {
          "text": "2017",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 8993,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 9036,
          "length": 5,
          "score": 0.72
        },
        {
          "text": "user",
          "type": "PersonType",
          "subtype": null,
          "offset": 9121,
          "length": 4,
          "score": 0.89
        },
        {
          "text": "neighbors",
          "type": "PersonType",
          "subtype": null,
          "offset": 9155,
          "length": 9,
          "score": 0.59
        },
        {
          "text": "user",
          "type": "PersonType",
          "subtype": null,
          "offset": 9172,
          "length": 4,
          "score": 0.91
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 9378,
          "length": 5,
          "score": 0.6
        },
        {
          "text": "between July 2013 and January \n2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 9554,
          "length": 35,
          "score": 0.8
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 10157,
          "length": 5,
          "score": 0.84
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 10337,
          "length": 5,
          "score": 0.84
        },
        {
          "text": "seller",
          "type": "PersonType",
          "subtype": null,
          "offset": 10446,
          "length": 6,
          "score": 0.95
        },
        {
          "text": "buyer",
          "type": "PersonType",
          "subtype": null,
          "offset": 10461,
          "length": 5,
          "score": 0.92
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 10574,
          "length": 5,
          "score": 0.9
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 10736,
          "length": 5,
          "score": 0.53
        },
        {
          "text": "user",
          "type": "PersonType",
          "subtype": null,
          "offset": 10927,
          "length": 4,
          "score": 0.76
        },
        {
          "text": "neighbors",
          "type": "PersonType",
          "subtype": null,
          "offset": 10942,
          "length": 9,
          "score": 0.78
        },
        {
          "text": "neighbors",
          "type": "PersonType",
          "subtype": null,
          "offset": 10990,
          "length": 9,
          "score": 0.83
        },
        {
          "text": "18Kodate",
          "type": "Person",
          "subtype": null,
          "offset": 11201,
          "length": 8,
          "score": 0.92
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 11243,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Barrat",
          "type": "Person",
          "subtype": null,
          "offset": 11431,
          "length": 6,
          "score": 0.53
        },
        {
          "text": "2004",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 11446,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "neighbor",
          "type": "PersonType",
          "subtype": null,
          "offset": 11643,
          "length": 8,
          "score": 0.86
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 12128,
          "length": 5,
          "score": 0.67
        },
        {
          "text": "0 0404",
          "type": "PhoneNumber",
          "subtype": null,
          "offset": 12250,
          "length": 6,
          "score": 0.8
        },
        {
          "text": "0 20 04 20070",
          "type": "PhoneNumber",
          "subtype": null,
          "offset": 12265,
          "length": 13,
          "score": 0.8
        },
        {
          "text": "18Kodate",
          "type": "Person",
          "subtype": null,
          "offset": 12293,
          "length": 8,
          "score": 0.94
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 12335,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Newman",
          "type": "Organization",
          "subtype": null,
          "offset": 13170,
          "length": 6,
          "score": 0.53
        },
        {
          "text": "2010",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 13177,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "may",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 13539,
          "length": 3,
          "score": 0.8
        },
        {
          "text": "Radicchi",
          "type": "Person",
          "subtype": null,
          "offset": 13619,
          "length": 8,
          "score": 0.55
        },
        {
          "text": "2004",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 13636,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Palla",
          "type": "Person",
          "subtype": null,
          "offset": 13642,
          "length": 5,
          "score": 0.96
        },
        {
          "text": "2005",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 13655,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 13815,
          "length": 5,
          "score": 0.72
        },
        {
          "text": "Milo",
          "type": "Person",
          "subtype": null,
          "offset": 14224,
          "length": 4,
          "score": 0.95
        },
        {
          "text": "2002",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 14236,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "may",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 14243,
          "length": 3,
          "score": 0.8
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 14289,
          "length": 5,
          "score": 0.8
        },
        {
          "text": "may",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 14813,
          "length": 3,
          "score": 0.8
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 14844,
          "length": 5,
          "score": 0.65
        },
        {
          "text": "buyer",
          "type": "PersonType",
          "subtype": null,
          "offset": 14879,
          "length": 5,
          "score": 0.79
        },
        {
          "text": "seller",
          "type": "PersonType",
          "subtype": null,
          "offset": 14889,
          "length": 6,
          "score": 0.79
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 15377,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "2d",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 15654,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "Breiman",
          "type": "Person",
          "subtype": null,
          "offset": 15884,
          "length": 7,
          "score": 0.93
        },
        {
          "text": "2001",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 15892,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Breiman",
          "type": "Person",
          "subtype": null,
          "offset": 15898,
          "length": 7,
          "score": 0.98
        },
        {
          "text": "1984",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 15913,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Hastie",
          "type": "Person",
          "subtype": null,
          "offset": 15919,
          "length": 6,
          "score": 0.52
        },
        {
          "text": "2009",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 15935,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Pedregosa",
          "type": "Person",
          "subtype": null,
          "offset": 15970,
          "length": 9,
          "score": 0.94
        },
        {
          "text": "2011",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 15988,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "candidate",
          "type": "PersonType",
          "subtype": null,
          "offset": 16494,
          "length": 9,
          "score": 0.65
        },
        {
          "text": "18Kodate",
          "type": "Person",
          "subtype": null,
          "offset": 17935,
          "length": 8,
          "score": 0.9
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 17977,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 17998,
          "length": 5,
          "score": 0.88
        },
        {
          "text": "user",
          "type": "PersonType",
          "subtype": null,
          "offset": 18032,
          "length": 4,
          "score": 0.85
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 18703,
          "length": 5,
          "score": 0.57
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 18871,
          "length": 5,
          "score": 0.87
        },
        {
          "text": "Strobl",
          "type": "Person",
          "subtype": null,
          "offset": 20244,
          "length": 6,
          "score": 0.96
        },
        {
          "text": "2007",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 20258,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Altmann",
          "type": "Person",
          "subtype": null,
          "offset": 20264,
          "length": 7,
          "score": 0.94
        },
        {
          "text": "2010",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 20279,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "18Kodate",
          "type": "Person",
          "subtype": null,
          "offset": 21825,
          "length": 8,
          "score": 0.93
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 21867,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 22312,
          "length": 5,
          "score": 0.54
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 22367,
          "length": 5,
          "score": 0.62
        },
        {
          "text": "user",
          "type": "PersonType",
          "subtype": null,
          "offset": 22451,
          "length": 4,
          "score": 0.78
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 22594,
          "length": 5,
          "score": 0.78
        },
        {
          "text": "2 may",
          "type": "DateTime",
          "subtype": "Date",
          "offset": 22633,
          "length": 5,
          "score": 0.8
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 22943,
          "length": 5,
          "score": 0.74
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 23040,
          "length": 5,
          "score": 0.82
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 23419,
          "length": 5,
          "score": 0.62
        },
        {
          "text": "user",
          "type": "PersonType",
          "subtype": null,
          "offset": 23536,
          "length": 4,
          "score": 0.64
        },
        {
          "text": "3d",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 23703,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "user",
          "type": "PersonType",
          "subtype": null,
          "offset": 23779,
          "length": 4,
          "score": 0.59
        },
        {
          "text": "18Kodate",
          "type": "Person",
          "subtype": null,
          "offset": 23952,
          "length": 8,
          "score": 0.92
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 23994,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 24527,
          "length": 5,
          "score": 0.85
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 24593,
          "length": 5,
          "score": 0.84
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 24750,
          "length": 5,
          "score": 0.84
        },
        {
          "text": "around 0",
          "type": "DateTime",
          "subtype": "Time",
          "offset": 24910,
          "length": 8,
          "score": 0.8
        },
        {
          "text": "2 412 432 465 467",
          "type": "PhoneNumber",
          "subtype": null,
          "offset": 25809,
          "length": 17,
          "score": 0.8
        },
        {
          "text": "2 412 432 465 467",
          "type": "PhoneNumber",
          "subtype": null,
          "offset": 26020,
          "length": 17,
          "score": 0.8
        },
        {
          "text": "2 412 432 465 467",
          "type": "PhoneNumber",
          "subtype": null,
          "offset": 26172,
          "length": 17,
          "score": 0.8
        },
        {
          "text": "2 412 432 465 467",
          "type": "PhoneNumber",
          "subtype": null,
          "offset": 26325,
          "length": 17,
          "score": 0.8
        },
        {
          "text": "Median",
          "type": "Person",
          "subtype": null,
          "offset": 26506,
          "length": 6,
          "score": 0.68
        },
        {
          "text": "CYPi",
          "type": "Organization",
          "subtype": null,
          "offset": 27045,
          "length": 4,
          "score": 0.85
        },
        {
          "text": "CYPi",
          "type": "Organization",
          "subtype": null,
          "offset": 27141,
          "length": 4,
          "score": 0.53
        },
        {
          "text": "18Kodate",
          "type": "Person",
          "subtype": null,
          "offset": 27234,
          "length": 8,
          "score": 0.92
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 27276,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "sellers",
          "type": "PersonType",
          "subtype": null,
          "offset": 27490,
          "length": 7,
          "score": 0.92
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 27675,
          "length": 5,
          "score": 0.8
        },
        {
          "text": "neighbor",
          "type": "PersonType",
          "subtype": null,
          "offset": 28175,
          "length": 8,
          "score": 0.94
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 28708,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 28822,
          "length": 5,
          "score": 0.8
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 29086,
          "length": 5,
          "score": 0.62
        },
        {
          "text": "user",
          "type": "PersonType",
          "subtype": null,
          "offset": 29186,
          "length": 4,
          "score": 0.76
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 29512,
          "length": 5,
          "score": 0.63
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 29537,
          "length": 5,
          "score": 0.63
        },
        {
          "text": "5d",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 29798,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 30137,
          "length": 5,
          "score": 0.79
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 30248,
          "length": 5,
          "score": 0.63
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 30304,
          "length": 5,
          "score": 0.76
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 30548,
          "length": 5,
          "score": 0.85
        },
        {
          "text": "7d",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 30694,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 30869,
          "length": 5,
          "score": 0.83
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 31000,
          "length": 5,
          "score": 0.77
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 31087,
          "length": 5,
          "score": 0.6
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 31160,
          "length": 5,
          "score": 0.67
        },
        {
          "text": "18Kodate",
          "type": "Person",
          "subtype": null,
          "offset": 31332,
          "length": 8,
          "score": 0.93
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 31374,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 31653,
          "length": 5,
          "score": 0.85
        },
        {
          "text": "18Kodate",
          "type": "Person",
          "subtype": null,
          "offset": 32457,
          "length": 8,
          "score": 0.92
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 32499,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 34040,
          "length": 5,
          "score": 0.81
        },
        {
          "text": "user",
          "type": "PersonType",
          "subtype": null,
          "offset": 34778,
          "length": 4,
          "score": 0.82
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 35222,
          "length": 5,
          "score": 0.85
        },
        {
          "text": "18Kodate",
          "type": "Person",
          "subtype": null,
          "offset": 35515,
          "length": 8,
          "score": 0.93
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 35557,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 35672,
          "length": 5,
          "score": 0.61
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 36837,
          "length": 5,
          "score": 0.9
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 36878,
          "length": 5,
          "score": 0.82
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 37632,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "AUC",
          "type": "Organization",
          "subtype": "Sports",
          "offset": 38425,
          "length": 3,
          "score": 0.73
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 38516,
          "length": 5,
          "score": 0.69
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 38688,
          "length": 5,
          "score": 0.9
        },
        {
          "text": "neighbors",
          "type": "PersonType",
          "subtype": null,
          "offset": 38875,
          "length": 9,
          "score": 0.5
        },
        {
          "text": "user",
          "type": "PersonType",
          "subtype": null,
          "offset": 38898,
          "length": 4,
          "score": 0.54
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 39041,
          "length": 5,
          "score": 0.9
        },
        {
          "text": "user",
          "type": "PersonType",
          "subtype": null,
          "offset": 39081,
          "length": 4,
          "score": 0.9
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 39604,
          "length": 5,
          "score": 0.55
        },
        {
          "text": "classifier",
          "type": "PersonType",
          "subtype": null,
          "offset": 39935,
          "length": 10,
          "score": 0.64
        },
        {
          "text": "Šubelj",
          "type": "Person",
          "subtype": null,
          "offset": 40237,
          "length": 6,
          "score": 0.95
        },
        {
          "text": "2011",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 40251,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Savage",
          "type": "Person",
          "subtype": null,
          "offset": 40257,
          "length": 6,
          "score": 0.98
        },
        {
          "text": "2017",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 40271,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Wang",
          "type": "Person",
          "subtype": null,
          "offset": 40277,
          "length": 4,
          "score": 0.83
        },
        {
          "text": "2018",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 40289,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Šubelj",
          "type": "Person",
          "subtype": null,
          "offset": 40321,
          "length": 6,
          "score": 0.96
        },
        {
          "text": "2011",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 40335,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Dreżewski",
          "type": "Person",
          "subtype": null,
          "offset": 40341,
          "length": 9,
          "score": 0.98
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 40358,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Colladon",
          "type": "Person",
          "subtype": null,
          "offset": 40364,
          "length": 8,
          "score": 0.51
        },
        {
          "text": "2017",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 40386,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "user",
          "type": "PersonType",
          "subtype": null,
          "offset": 40393,
          "length": 4,
          "score": 0.86
        },
        {
          "text": "Chau",
          "type": "Person",
          "subtype": null,
          "offset": 40449,
          "length": 4,
          "score": 0.93
        },
        {
          "text": "2006",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 40461,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Pandit",
          "type": "Organization",
          "subtype": null,
          "offset": 40467,
          "length": 6,
          "score": 0.64
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 40539,
          "length": 5,
          "score": 0.51
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 40996,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "2007",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 41016,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Akoglu",
          "type": "Person",
          "subtype": null,
          "offset": 41022,
          "length": 6,
          "score": 0.96
        },
        {
          "text": "2013",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 41036,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Bangcharoensap",
          "type": "Person",
          "subtype": null,
          "offset": 41042,
          "length": 14,
          "score": 0.94
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 41064,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Van Vlasselaer",
          "type": "Person",
          "subtype": null,
          "offset": 41070,
          "length": 14,
          "score": 0.98
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 41092,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "2016",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 41099,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Li",
          "type": "Person",
          "subtype": null,
          "offset": 41105,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "2017",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 41115,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Hu",
          "type": "Person",
          "subtype": null,
          "offset": 41121,
          "length": 2,
          "score": 0.93
        },
        {
          "text": "2017",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 41131,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Šubelj",
          "type": "Person",
          "subtype": null,
          "offset": 41190,
          "length": 6,
          "score": 0.96
        },
        {
          "text": "2011",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 41204,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Bhat",
          "type": "Person",
          "subtype": null,
          "offset": 41210,
          "length": 4,
          "score": 0.82
        },
        {
          "text": "2013",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 41228,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Ferrara",
          "type": "Person",
          "subtype": null,
          "offset": 41234,
          "length": 7,
          "score": 0.92
        },
        {
          "text": "2014",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 41249,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Jiang",
          "type": "Person",
          "subtype": null,
          "offset": 41255,
          "length": 5,
          "score": 0.95
        },
        {
          "text": "2014",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 41268,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "2016",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 41287,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Liu",
          "type": "Person",
          "subtype": null,
          "offset": 41293,
          "length": 3,
          "score": 0.96
        },
        {
          "text": "2016",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 41304,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Shchur",
          "type": "Person",
          "subtype": null,
          "offset": 41310,
          "length": 6,
          "score": 0.94
        },
        {
          "text": "2018",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 41324,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Wang",
          "type": "Person",
          "subtype": null,
          "offset": 41343,
          "length": 4,
          "score": 0.75
        },
        {
          "text": "2008",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 41357,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Rasheed",
          "type": "Person",
          "subtype": null,
          "offset": 41363,
          "length": 7,
          "score": 0.69
        },
        {
          "text": "2018",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 41380,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Chau",
          "type": "Person",
          "subtype": null,
          "offset": 41896,
          "length": 4,
          "score": 0.88
        },
        {
          "text": "2006",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 41908,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Akoglu",
          "type": "Person",
          "subtype": null,
          "offset": 41914,
          "length": 6,
          "score": 0.97
        },
        {
          "text": "2010",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 41928,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Šubelj",
          "type": "Person",
          "subtype": null,
          "offset": 41934,
          "length": 6,
          "score": 0.98
        },
        {
          "text": "2011",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 41948,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Yanchun",
          "type": "Person",
          "subtype": null,
          "offset": 41954,
          "length": 7,
          "score": 0.97
        },
        {
          "text": "2011",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 41969,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Abulaish",
          "type": "Person",
          "subtype": null,
          "offset": 41985,
          "length": 8,
          "score": 0.58
        },
        {
          "text": "2013",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 41994,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Bangcharoensap",
          "type": "Person",
          "subtype": null,
          "offset": 42000,
          "length": 14,
          "score": 0.94
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 42022,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Dreżewski",
          "type": "Person",
          "subtype": null,
          "offset": 42028,
          "length": 9,
          "score": 0.98
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 42046,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Monamo",
          "type": "Person",
          "subtype": null,
          "offset": 42052,
          "length": 6,
          "score": 0.93
        },
        {
          "text": "2016",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 42067,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Van Vlasselaer",
          "type": "Person",
          "subtype": null,
          "offset": 42073,
          "length": 14,
          "score": 0.98
        },
        {
          "text": "2016",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 42095,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Colladon",
          "type": "Person",
          "subtype": null,
          "offset": 42101,
          "length": 8,
          "score": 0.93
        },
        {
          "text": "Remondi",
          "type": "Person",
          "subtype": null,
          "offset": 42114,
          "length": 7,
          "score": 0.51
        },
        {
          "text": "2017",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 42122,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Monamo",
          "type": "Person",
          "subtype": null,
          "offset": 42178,
          "length": 6,
          "score": 0.92
        },
        {
          "text": "2016",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 42192,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Van Vlasselaer",
          "type": "Person",
          "subtype": null,
          "offset": 42198,
          "length": 14,
          "score": 0.97
        },
        {
          "text": "2016",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 42220,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 42339,
          "length": 5,
          "score": 0.74
        },
        {
          "text": "may",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 42654,
          "length": 3,
          "score": 0.8
        },
        {
          "text": "Bhat",
          "type": "Person",
          "subtype": null,
          "offset": 42697,
          "length": 4,
          "score": 0.93
        },
        {
          "text": "Abulaish",
          "type": "Person",
          "subtype": null,
          "offset": 42706,
          "length": 8,
          "score": 0.74
        },
        {
          "text": "2013",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 42715,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Šubelj",
          "type": "Person",
          "subtype": null,
          "offset": 42883,
          "length": 6,
          "score": 0.96
        },
        {
          "text": "2011",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 42897,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Van Vlasselaer",
          "type": "Person",
          "subtype": null,
          "offset": 42903,
          "length": 14,
          "score": 0.98
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 42925,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Van Vlasselaer",
          "type": "Person",
          "subtype": null,
          "offset": 42931,
          "length": 14,
          "score": 0.98
        },
        {
          "text": "2016",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 42953,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Hu",
          "type": "Person",
          "subtype": null,
          "offset": 42960,
          "length": 2,
          "score": 0.94
        },
        {
          "text": "2017",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 42970,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Li",
          "type": "Person",
          "subtype": null,
          "offset": 42976,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "2017",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 42986,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Savage",
          "type": "Person",
          "subtype": null,
          "offset": 42992,
          "length": 6,
          "score": 0.95
        },
        {
          "text": "2017",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 43006,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Mercari",
          "type": "Organization",
          "subtype": null,
          "offset": 43066,
          "length": 7,
          "score": 0.86
        },
        {
          "text": "Yamamoto",
          "type": "Person",
          "subtype": null,
          "offset": 43188,
          "length": 8,
          "score": 0.94
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 43204,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "authors",
          "type": "PersonType",
          "subtype": null,
          "offset": 43215,
          "length": 7,
          "score": 0.94
        },
        {
          "text": "Yamamoto",
          "type": "Person",
          "subtype": null,
          "offset": 43473,
          "length": 8,
          "score": 0.93
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 43490,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Yamamoto",
          "type": "Person",
          "subtype": null,
          "offset": 43572,
          "length": 8,
          "score": 0.95
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 43589,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "may",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 43928,
          "length": 3,
          "score": 0.8
        },
        {
          "text": "may",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 44050,
          "length": 3,
          "score": 0.8
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 44171,
          "length": 5,
          "score": 0.83
        },
        {
          "text": "Advanced Computing Research Centre",
          "type": "Organization",
          "subtype": null,
          "offset": 44440,
          "length": 34,
          "score": 0.73
        },
        {
          "text": "University of",
          "type": "Organization",
          "subtype": null,
          "offset": 44476,
          "length": 13,
          "score": 0.82
        },
        {
          "text": "Shun Kodate",
          "type": "Person",
          "subtype": null,
          "offset": 44524,
          "length": 11,
          "score": 0.97
        },
        {
          "text": "Shunya Kimura",
          "type": "Person",
          "subtype": null,
          "offset": 44682,
          "length": 13,
          "score": 0.99
        },
        {
          "text": "NM",
          "type": "Organization",
          "subtype": "Sports",
          "offset": 44777,
          "length": 2,
          "score": 0.87
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 44937,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Mercari, Inc. S. Kodate",
          "type": "Organization",
          "subtype": null,
          "offset": 45160,
          "length": 23,
          "score": 0.66
        },
        {
          "text": "Top Global University",
          "type": "Organization",
          "subtype": null,
          "offset": 45213,
          "length": 21,
          "score": 0.88
        },
        {
          "text": "Ministry of Education, Culture, Sports, Science and Technology",
          "type": "Organization",
          "subtype": null,
          "offset": 45253,
          "length": 62,
          "score": 0.93
        },
        {
          "text": "MEXT",
          "type": "Organization",
          "subtype": null,
          "offset": 45317,
          "length": 4,
          "score": 0.89
        },
        {
          "text": "Mercari, Inc.",
          "type": "Organization",
          "subtype": null,
          "offset": 45369,
          "length": 13,
          "score": 0.92
        },
        {
          "text": "collaborators",
          "type": "PersonType",
          "subtype": null,
          "offset": 45506,
          "length": 13,
          "score": 0.86
        },
        {
          "text": "authors",
          "type": "PersonType",
          "subtype": null,
          "offset": 45561,
          "length": 7,
          "score": 0.91
        },
        {
          "text": "authors",
          "type": "PersonType",
          "subtype": null,
          "offset": 45599,
          "length": 7,
          "score": 0.89
        },
        {
          "text": "employees",
          "type": "PersonType",
          "subtype": null,
          "offset": 45612,
          "length": 9,
          "score": 0.88
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 46180,
          "length": 5,
          "score": 0.92
        },
        {
          "text": "researcher",
          "type": "PersonType",
          "subtype": null,
          "offset": 46211,
          "length": 10,
          "score": 0.96
        },
        {
          "text": "Shunya Kimura",
          "type": "Person",
          "subtype": null,
          "offset": 46250,
          "length": 13,
          "score": 0.99
        },
        {
          "text": "kimuras@mercari.com",
          "type": "Email",
          "subtype": null,
          "offset": 46265,
          "length": 19,
          "score": 0.8
        },
        {
          "text": "Ryusuke Chiba",
          "type": "Person",
          "subtype": null,
          "offset": 46289,
          "length": 13,
          "score": 0.99
        },
        {
          "text": "mercari",
          "type": "Organization",
          "subtype": null,
          "offset": 46313,
          "length": 7,
          "score": 0.56
        },
        {
          "text": "Mercari",
          "type": "Organization",
          "subtype": null,
          "offset": 46546,
          "length": 7,
          "score": 0.63
        },
        {
          "text": "ri.com/jp/tos/",
          "type": "URL",
          "subtype": null,
          "offset": 46608,
          "length": 14,
          "score": 0.8
        },
        {
          "text": "Mercari, Inc.",
          "type": "Organization",
          "subtype": null,
          "offset": 46791,
          "length": 13,
          "score": 0.92
        },
        {
          "text": "collaborators",
          "type": "PersonType",
          "subtype": null,
          "offset": 46928,
          "length": 13,
          "score": 0.82
        },
        {
          "text": "authors",
          "type": "PersonType",
          "subtype": null,
          "offset": 46983,
          "length": 7,
          "score": 0.91
        },
        {
          "text": "authors",
          "type": "PersonType",
          "subtype": null,
          "offset": 47021,
          "length": 7,
          "score": 0.85
        },
        {
          "text": "employees",
          "type": "PersonType",
          "subtype": null,
          "offset": 47034,
          "length": 9,
          "score": 0.85
        },
        {
          "text": "authors",
          "type": "PersonType",
          "subtype": null,
          "offset": 47103,
          "length": 7,
          "score": 0.72
        },
        {
          "text": "employees",
          "type": "PersonType",
          "subtype": null,
          "offset": 47115,
          "length": 9,
          "score": 0.9
        },
        {
          "text": "Graduate School of Information Sciences",
          "type": "Organization",
          "subtype": null,
          "offset": 47388,
          "length": 39,
          "score": 0.61
        },
        {
          "text": "Sendai 980-8579",
          "type": "Address",
          "subtype": null,
          "offset": 47448,
          "length": 15,
          "score": 0.88
        },
        {
          "text": "980-8579",
          "type": "PhoneNumber",
          "subtype": null,
          "offset": 47455,
          "length": 8,
          "score": 0.8
        },
        {
          "text": "Department of Engineering",
          "type": "Organization",
          "subtype": null,
          "offset": 47474,
          "length": 25,
          "score": 0.75
        },
        {
          "text": "University of Bristol",
          "type": "Organization",
          "subtype": null,
          "offset": 47514,
          "length": 21,
          "score": 0.74
        },
        {
          "text": "Mercari, Inc",
          "type": "Organization",
          "subtype": null,
          "offset": 47560,
          "length": 12,
          "score": 0.81
        },
        {
          "text": "Tokyo 106-6118",
          "type": "Address",
          "subtype": null,
          "offset": 47575,
          "length": 14,
          "score": 0.94
        },
        {
          "text": "Department of Mathemat",
          "type": "Organization",
          "subtype": null,
          "offset": 47600,
          "length": 22,
          "score": 0.73
        },
        {
          "text": "University at Buffalo",
          "type": "Organization",
          "subtype": "Sports",
          "offset": 47629,
          "length": 21,
          "score": 0.67
        },
        {
          "text": "Buffalo, NY 14260-2900, USA",
          "type": "Address",
          "subtype": null,
          "offset": 47652,
          "length": 27,
          "score": 0.86
        },
        {
          "text": "14260-2900",
          "type": "PhoneNumber",
          "subtype": null,
          "offset": 47664,
          "length": 10,
          "score": 0.8
        },
        {
          "text": "University at Buffalo",
          "type": "Organization",
          "subtype": "Sports",
          "offset": 47748,
          "length": 21,
          "score": 0.62
        },
        {
          "text": "Buffalo, NY 14260-5030",
          "type": "Address",
          "subtype": null,
          "offset": 47771,
          "length": 22,
          "score": 0.94
        },
        {
          "text": "14260-5030",
          "type": "PhoneNumber",
          "subtype": null,
          "offset": 47783,
          "length": 10,
          "score": 0.8
        },
        {
          "text": "12 August 2020",
          "type": "DateTime",
          "subtype": "Date",
          "offset": 47812,
          "length": 14,
          "score": 0.8
        },
        {
          "text": "23 October 2020",
          "type": "DateTime",
          "subtype": "Date",
          "offset": 47839,
          "length": 15,
          "score": 0.8
        },
        {
          "text": "Abdallah A",
          "type": "Person",
          "subtype": null,
          "offset": 47867,
          "length": 10,
          "score": 0.97
        },
        {
          "text": "Maarof MA",
          "type": "Person",
          "subtype": null,
          "offset": 47879,
          "length": 9,
          "score": 0.92
        },
        {
          "text": "Zainal A",
          "type": "Person",
          "subtype": null,
          "offset": 47890,
          "length": 8,
          "score": 0.96
        },
        {
          "text": "2016",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 47900,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Akoglu L",
          "type": "Person",
          "subtype": null,
          "offset": 47969,
          "length": 8,
          "score": 0.95
        },
        {
          "text": "McGlohon M",
          "type": "Person",
          "subtype": null,
          "offset": 47979,
          "length": 10,
          "score": 0.94
        },
        {
          "text": "Faloutsos C",
          "type": "Person",
          "subtype": null,
          "offset": 47991,
          "length": 11,
          "score": 0.93
        },
        {
          "text": "2010",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 48004,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Akoglu L",
          "type": "Person",
          "subtype": null,
          "offset": 48139,
          "length": 8,
          "score": 0.94
        },
        {
          "text": "Chandy R",
          "type": "Person",
          "subtype": null,
          "offset": 48149,
          "length": 8,
          "score": 0.94
        },
        {
          "text": "Faloutsos C",
          "type": "Person",
          "subtype": null,
          "offset": 48159,
          "length": 11,
          "score": 0.94
        },
        {
          "text": "2013",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 48172,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Akoglu L",
          "type": "Person",
          "subtype": null,
          "offset": 48317,
          "length": 8,
          "score": 0.96
        },
        {
          "text": "Tong H",
          "type": "Person",
          "subtype": null,
          "offset": 48327,
          "length": 6,
          "score": 0.95
        },
        {
          "text": "Koutra D",
          "type": "Person",
          "subtype": null,
          "offset": 48335,
          "length": 8,
          "score": 0.91
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 48345,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Data Min Knowl",
          "type": "Person",
          "subtype": null,
          "offset": 48408,
          "length": 14,
          "score": 0.63
        },
        {
          "text": "Discov",
          "type": "Organization",
          "subtype": null,
          "offset": 48423,
          "length": 6,
          "score": 0.54
        },
        {
          "text": "Altmann A",
          "type": "Person",
          "subtype": null,
          "offset": 48443,
          "length": 9,
          "score": 0.95
        },
        {
          "text": "Toloşi L",
          "type": "Person",
          "subtype": null,
          "offset": 48454,
          "length": 8,
          "score": 0.98
        },
        {
          "text": "Sander O",
          "type": "Person",
          "subtype": null,
          "offset": 48464,
          "length": 8,
          "score": 0.95
        },
        {
          "text": "Lengauer T",
          "type": "Person",
          "subtype": null,
          "offset": 48474,
          "length": 10,
          "score": 0.92
        },
        {
          "text": "2010",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 48486,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Bioinfo",
          "type": "Organization",
          "subtype": null,
          "offset": 48556,
          "length": 7,
          "score": 0.77
        },
        {
          "text": "Anderson R",
          "type": "Person",
          "subtype": null,
          "offset": 48579,
          "length": 10,
          "score": 0.95
        },
        {
          "text": "Barton C",
          "type": "Person",
          "subtype": null,
          "offset": 48591,
          "length": 8,
          "score": 0.97
        },
        {
          "text": "Böhme R",
          "type": "Person",
          "subtype": null,
          "offset": 48601,
          "length": 7,
          "score": 0.94
        },
        {
          "text": "Clayton R",
          "type": "Person",
          "subtype": null,
          "offset": 48610,
          "length": 9,
          "score": 0.95
        },
        {
          "text": "Van Eeten MJ",
          "type": "Person",
          "subtype": null,
          "offset": 48621,
          "length": 12,
          "score": 0.97
        },
        {
          "text": "Levi M",
          "type": "Person",
          "subtype": null,
          "offset": 48635,
          "length": 6,
          "score": 0.95
        },
        {
          "text": "Moore T",
          "type": "Person",
          "subtype": null,
          "offset": 48643,
          "length": 7,
          "score": 0.93
        },
        {
          "text": "Savage S",
          "type": "Person",
          "subtype": null,
          "offset": 48652,
          "length": 8,
          "score": 0.93
        },
        {
          "text": "2013",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 48662,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Springer",
          "type": "Organization",
          "subtype": null,
          "offset": 48759,
          "length": 8,
          "score": 0.78
        },
        {
          "text": "Bangcharoensap P",
          "type": "Person",
          "subtype": null,
          "offset": 48788,
          "length": 16,
          "score": 0.95
        },
        {
          "text": "Kobayashi H",
          "type": "Person",
          "subtype": null,
          "offset": 48806,
          "length": 11,
          "score": 0.94
        },
        {
          "text": "Shimizu N",
          "type": "Person",
          "subtype": null,
          "offset": 48819,
          "length": 9,
          "score": 0.92
        },
        {
          "text": "Yamauchi S",
          "type": "Person",
          "subtype": null,
          "offset": 48830,
          "length": 10,
          "score": 0.95
        },
        {
          "text": "Murata T",
          "type": "Person",
          "subtype": null,
          "offset": 48842,
          "length": 8,
          "score": 0.92
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 48852,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Barrat A",
          "type": "Person",
          "subtype": null,
          "offset": 49043,
          "length": 8,
          "score": 0.94
        },
        {
          "text": "Barthelemy M",
          "type": "Person",
          "subtype": null,
          "offset": 49053,
          "length": 12,
          "score": 0.96
        },
        {
          "text": "Pastor-Satorras R",
          "type": "Person",
          "subtype": null,
          "offset": 49067,
          "length": 17,
          "score": 0.97
        },
        {
          "text": "Vespignani A",
          "type": "Person",
          "subtype": null,
          "offset": 49086,
          "length": 12,
          "score": 0.93
        },
        {
          "text": "2004",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 49100,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Acad Sci USA",
          "type": "Organization",
          "subtype": null,
          "offset": 49164,
          "length": 12,
          "score": 0.74
        },
        {
          "text": "Bhat SY",
          "type": "Person",
          "subtype": null,
          "offset": 49192,
          "length": 7,
          "score": 0.94
        },
        {
          "text": "Abulaish M",
          "type": "Person",
          "subtype": null,
          "offset": 49201,
          "length": 10,
          "score": 0.93
        },
        {
          "text": "2013",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 49213,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "spammers",
          "type": "PersonType",
          "subtype": null,
          "offset": 49260,
          "length": 8,
          "score": 0.7
        },
        {
          "text": "2013",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 49300,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "IEEE/ACM",
          "type": "Organization",
          "subtype": null,
          "offset": 49305,
          "length": 8,
          "score": 0.67
        },
        {
          "text": "2013",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 49399,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Bhowmick A",
          "type": "Person",
          "subtype": null,
          "offset": 49418,
          "length": 10,
          "score": 0.91
        },
        {
          "text": "Hazarika SM",
          "type": "Person",
          "subtype": null,
          "offset": 49430,
          "length": 11,
          "score": 0.89
        },
        {
          "text": "2016",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 49443,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Bolton RJ",
          "type": "Person",
          "subtype": null,
          "offset": 49554,
          "length": 9,
          "score": 0.84
        },
        {
          "text": "Hand DJ",
          "type": "Person",
          "subtype": null,
          "offset": 49565,
          "length": 7,
          "score": 0.63
        },
        {
          "text": "2002",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 49574,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Breiman L",
          "type": "Person",
          "subtype": null,
          "offset": 49639,
          "length": 9,
          "score": 0.93
        },
        {
          "text": "2001",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 49650,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Breiman L",
          "type": "Person",
          "subtype": null,
          "offset": 49691,
          "length": 9,
          "score": 0.95
        },
        {
          "text": "Friedman JH",
          "type": "Person",
          "subtype": null,
          "offset": 49702,
          "length": 11,
          "score": 0.91
        },
        {
          "text": "Olshen RA",
          "type": "Person",
          "subtype": null,
          "offset": 49715,
          "length": 9,
          "score": 0.88
        },
        {
          "text": "Stone CJ",
          "type": "Person",
          "subtype": null,
          "offset": 49726,
          "length": 8,
          "score": 0.9
        },
        {
          "text": "1984",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 49736,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Chapman & Hall",
          "type": "Organization",
          "subtype": null,
          "offset": 49779,
          "length": 14,
          "score": 0.92
        },
        {
          "text": "Chau DH",
          "type": "Person",
          "subtype": null,
          "offset": 49806,
          "length": 7,
          "score": 0.91
        },
        {
          "text": "Pandit S",
          "type": "Person",
          "subtype": null,
          "offset": 49815,
          "length": 8,
          "score": 0.94
        },
        {
          "text": "Faloutsos C",
          "type": "Person",
          "subtype": null,
          "offset": 49825,
          "length": 11,
          "score": 0.9
        },
        {
          "text": "2006",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 49838,
          "length": 4,
          "score": 0.8
        }
      ]
    },
    {
      "@search.score": 3.79054,
      "content": "\nRESEARCH Open Access\n\nA classification method for social\ninformation of sellers on social network\nHaoliang Cui1, Shuai Shao2* , Shaozhang Niu1, Chengjie Shi3 and Lingyu Zhou1\n\n* Correspondence: shaoshuaib@163.\ncom\n2China Information Technology\nSecurity Evaluation Center, Beijing\n100085, China\nFull list of author information is\navailable at the end of the article\n\nAbstract\n\nSocial e-commerce has been a hot topic in recent years, with the number of users\nincreasing year by year and the transaction money exploding. Unlike traditional e-\ncommerce, the main activities of social e-commerce are on social network apps. To\nclassify sellers by the merchandise, this article designs and implements a social\nnetwork seller classification scheme. We develop an app, which runs on the mobile\nphones of the sellers and provides the operating environment and automated\nassistance capabilities of social network applications. The app can collect social\ninformation published by the sellers during the assistance process, uploads to the\nserver to perform model training on the data. We collect 38,970 sellers’ information,\nextract the text information in the picture with the help of OCR, and establish a\ndeep learning model based on BERT to classify the merchandise of sellers. In the\nfinal experiment, we achieve an accuracy of more than 90%, which shows that the\nmodel can accurately classify sellers on a social network.\n\nKeywords: User model, Machine learning, Social e-commerce\n\n1 Introduction\nWith the continuous improvement of social network and mobile payment technology,\n\none kind of commodity trading based on social relations called social e-commerce is in\n\nrapid development. According to the 2019 China social e-commerce industry develop-\n\nment report released by the Internet society of China, the number of employees of so-\n\ncial e-commerce in China is expected to reach 48.01 million in 2019, up by 58.3\n\npercent year on year, and the market size is expected to reach 2060.58 billion yuan, up\n\nby 63.2% year on year. Social e-commerce has become a large scale, and the high\n\ngrowth cannot be ignored. Different from e-commerce platforms such as Taobao, so-\n\ncial e-commerce is at the end of online retail. It carries out trading activities through\n\nsocial software and uses social interaction, user generated content and other means to\n\nassist the purchase and sale of goods. At the same time, sellers on social network use\n\ndifferent social software without uniform registration, have no systematic classification\n\nof products for sale, and there are no standardized terms for product description.\n\nThese bring great difficulty to the accurate classification of user portrait. This paper\n\nproposes a method based on the NLP classification model, which can realize accurate\n\n© The Author(s). 2021 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the\noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or\nother third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit\nline to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a\ncopy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\n\nEURASIP Journal on Image\nand Video Processing\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 \nhttps://doi.org/10.1186/s13640-020-00545-z\n\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13640-020-00545-z&domain=pdf\nhttp://orcid.org/0000-0001-9638-0201\nmailto:shaoshuaib@163.com\nmailto:shaoshuaib@163.com\nhttp://creativecommons.org/licenses/by/4.0/\n\n\nbusiness classification of social e-commerce based on social information of social e-\n\ncommerce. This method analyzes 38,970 sellers on social networks and establishes a\n\ndeep learning model based on BERT to accurately classify the merchandise of sellers.\n\nIn addition, we introduced the OCR algorithm to extract the text information in the\n\npicture and superimposed it on the social content data, which effectively improved the\n\nclassification accuracy. The final experiment shows that the measured accuracy is more\n\nthan 90%.\n\n2 Related work\n2.1 Natural language processing\n\nIn order to analyze e-commerce business classification based on social data of sellers on a\n\nsocial network, the text needs to be analyzed based on the NLP correlation algorithm.\n\nThe rapid development of NLP at the present stage is due to the neural network language\n\nmodel (NNLM) Bengio et al. [1] proposed in 2003. Researchers have been trying to realize\n\nthe end-to-end classification recognition by using a neural network as a classifier in the\n\ntext classification research based on word embedding. Kim first introduces the convolu-\n\ntional neural network (CNN) into the study of text classification. The network structure is\n\na dropout full connection layer and a softmax layer connected after one convolution layer\n\n[2]. Although this algorithm achieves good results in various benchmark tests, it cannot\n\nobtain long-distance text dependency due to the limitation of network structure. There-\n\nfore, Tencent AI Lab proposed DPCNN, which further enhanced the extraction capacity\n\nof long-distance text dependency by deepening CNN [3].\n\nSocial content data includes multimedia text data and picture data. With the help of\n\nOCR, we extract the text in the picture and convert the picture data into text data. Text\n\nis a kind of sequential data, and the classification of it by recurrent neural network\n\n(RNN) has been the focus of long-term research in academia [4]. As a variation of\n\nRNN, long short-term memory (LSTM) adds control units such as forgetting gate, in-\n\nput gate, and output gate on the original basis, which solves the problem of gradient\n\nexplosion and gradient disappearance in the long sequence training of RNN and pro-\n\nmotes the use of RNN [5]. By introducing the sharing information mechanism, Liu\n\net al. further improved the accuracy of the RNN algorithm in the text multi-\n\nclassification task and achieved good results in four benchmark text classifications [6].\n\nHowever, Word vectors cannot be constructed in Word embedding to solve the\n\nproblem of polysemy. Even though different semantic environments are considered\n\nduring training, the result of training is still one word corresponding to one row vector.\n\nConsidering the widespread phenomenon of polysemy, Peters et al. propose embed-\n\ndings from language model (ELMO) to address the impact of polysemy on natural lan-\n\nguage modeling [7]. ELMO uses a feature-based form of pre-training. First, two-way\n\nLSTM is used to pre-train the corpus, and then word embedding resulting from train-\n\ning is adjusted by double-layer two-way LSTM when processing downstream tasks to\n\nadd more grammatical and semantic information according to the context words.\n\nThe ability of ELMO to extract features is limited for choosing LSTM as the feature\n\nextractor instead of Transformer [8], and ELMO’s bidirectional splicing method is also\n\nweak in feature fusion. Therefore, Devlin et al. propose the BERT model, taking Trans-\n\nformer as a feature extractor to pre-train large-scale text corpus [9].\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 2 of 12\n\n\n\n2.2 User analysis of social networks\n\nUser analysis is an important part of social network analysis. Most existing studies use\n\nuser-generated content or social links between users to simulate users. Wu et al. mod-\n\neled users on the content curation social network (CCSN) in the unified framework by\n\nmining user-generated content and social links [10]. They proposed a potential Bayes-\n\nian model, multilevel LDA (MLLDA), that could represent users of potential interest\n\nfound in social links formed by text descriptions contributed by users and information\n\nsharing. In 2017, Wu et al. proposed a latent model [11], trying to explain how the so-\n\ncial network structure and users’ historical preferences change over time affect each\n\nuser’s future behavior and predict each user’s consumption preferences and social con-\n\nnections in the near future. Malli et al. proposed a new online social network user pro-\n\nfile rating model [12], which solved the problem of large and complicated user data. In\n\nterms of data analysis platform, Chen et al. [13] developed a big data platform for the\n\nstudy of the garlic industry chain. Garlic planting management, price control, and pre-\n\ndiction were realized through data collection, storage, and pretreatment. Ning et al.\n\n[14] designed a ga-bp hybrid algorithm based on the fuzzy theory and constructed an\n\nair quality evaluation model by combining the knowledge of BP neural network, genetic\n\nalgorithm, and fuzzy theory. Yin et al. [15] studied two methods of extracting supervis-\n\nory relations and applied them to the field of English news. One is the combination of\n\nsupport vector machine and principal component analysis, and the other is the combin-\n\nation of support vector machine and CNN, which can extract high-quality feature vec-\n\ntors from sentences of support vector machine. In the social apps, the data we obtain is\n\nmostly image data, so we introduced the OCR technology to identify text information\n\nin images.\n\n3 Data collection\nIn order to analyze the behavior patterns of social e-commerce, we developed an auxil-\n\niary tool for social e-commerce. In this tool, sellers on a social network are provided\n\nwith the independent running environment of social software and the automatic auxil-\n\niary ability, and the information acquisition module of the auxiliary process is used to\n\ncollect the social information published by sellers on a social network, which is\n\nuploaded to the background server for model training. We provided this tool to nearly\n\n10,000 sellers on a social network who participated in the experiment to obtain their\n\nsocial information in their e-commerce activities.\n\n3.1 Overall structure\n\nThe whole data collection scheme is mainly composed of two parts: intelligent space\n\napp and background server. The overall architecture is shown in Fig. 1. Intelligent\n\nspace app is deployed in the mobile phones of sellers on a social network and imple-\n\nmented based on the application layer of the Android platform, providing sellers on a\n\nsocial network with a secure container for the independent operation of social software.\n\nThe app contains the automatic assistant module, which provides the automatic assist-\n\nant capability of various business processes for seller, and collects the social informa-\n\ntion in the auxiliary process through the information grasping module. The collected\n\ninformation is cached and uploaded locally through the information collection service.\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 3 of 12\n\n\n\nThe background server is responsible for receiving the collected data uploaded by the\n\nintelligent space, preprocessing the data first, and then classifying the social e-\n\ncommerce through the data based on the machine learning classification model, and fi-\n\nnally storing the classification results.\n\n3.1.1 Security container\n\nThe security container is designed to allow social software to run independently with-\n\nout modifying the OS or gaining root privileges. The basic principle of its realization is\n\nto create an independent container process; load APK file of social software dynamic-\n\nally; monitor and intercept process communication interface such as Binder IPC\n\nthrough Libc hook, Java reflection, dynamic proxy, and other technical means; and col-\n\nlect social information through an automatic assistant module. The main part of the\n\ncontainer is composed of an application layer module and a service layer module.\n\nThe application layer module is responsible for the process startup and execution of\n\nsocial software, and its main functions include three parts.\n\n3.1.1.1 Interactive interception The application layer module intercepts the inter-\n\naction between the application process and the underlying system in the container and\n\nmodifies the calling logic. By hook or dynamic proxy of system library API and Binder\n\ncommunication interface, the application layer module blocks all interfaces that interact\n\nwith the system during the execution of social software and controls the process\n\nboundary of interaction between social applications and system services.\n\n3.1.1.2 Social information collection The loading of the automatic auxiliary module\n\nby social software is realized when initializing the process of social application.\n\nThe application layer module injects the corresponding plugins in the automatic\n\nassistant module into the social application process. The automatic assistance mod-\n\nule provides a number of e-commerce auxiliary functions for sellers on a social\n\nnetwork, including customer acquisition, social customer relationship management\n\nLinux Kernel\n\nBinder Mode\n\nIntelligent Space\n\nService Layer Mode\nAMS Proxy PMS Proxy\n\nApplication Layer Mode\nSocial App\n\nInteractive \ninterception\n\nautomatic \nassistance  \nmodule\n\nInformation\nCollection \n\nBinder IPC\n\nBinder IPC\n\nBinder \nIPC Backgroud\n\n Server\n\nInternet\n\nFig. 1 The overall architecture diagram of the data acquisition scheme\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 4 of 12\n\n\n\n(SCRM), group management, sales assistance, and daily affairs. Sellers on social\n\nnetworks publish social information with commercial attributes through auxiliary\n\nfunctions, then the automatic auxiliary module will automatically collect the social\n\ninformation and send it to the information collection service for processing.\n\n3.1.1.3 Local processing of social information When the information collection ser-\n\nvice receives the social information collected by the automatic auxiliary module, the\n\ndata will be compressed and encrypted in the local cache. The service then uploads the\n\ncollected data to the background server periodically through the timer, and HTTPS is\n\nused to ensure data transmission security.\n\nThe main function of the service layer module is to take over the call logic modified\n\nby the application layer module by simulating the system service modify the parameters\n\nin the communication process and finally call the real system service. The service layer\n\nmodule exists in the container as an independent process. It focuses on the simulation\n\nof activity manager service (AMS) and package manager service (PMS) and realizes the\n\nsupport of system services in the process of launching and running social software.\n\n3.1.2 Background server\n\nThe background server mainly realizes the machine learning model processing of the\n\ncollected social data, including the functions of data preprocessing, data training, classi-\n\nfication, and result storage. The core processing logic will be described in chapter 5.\n\n3.2 Key processes\n\nThere are four key processes in the process of social information collection and pro-\n\ncessing. They are social software process initialization, social software process\n\nIntelligent \nSpace App \nlaunched\n\nProcess Boundaries\n\nUser Process\n\nSocial software process \ninitialization\n\nlaunching social \nsoftware\n\ninject automatic \nauxiliary\n\nSocial software process \nexecution\n\nRun the plug-in\n\nCollect social \ninformation\n\nProcess Boundaries\n\nUser Process\n\nLocal processing of \nsocial information\n\nBatch upload \nprocessed social \n\ninformation\n\nEncrypt, compress \nand store social \n\ninformation\n\nInternet\n\nInformation collecting \nservice process\n\nBackground processing \nof social information\n\nReceiving social \ninformation\n\nPreprocessing social \ninformation\n\nThe Server\n\nMachine learning \ncategorizes social \n\ninformation\n\nStore the \nclassification results\n\nFig. 2 Key flow chart of data acquisition scheme\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 5 of 12\n\n\n\nexecution, local processing of social information, and background processing of social\n\ninformation. The complete process is shown in Fig. 2.\n\n3.2.1 Social software process initialization\n\nWhen launching social software, the intelligent space will first intercept the callback\n\nfunction of the life cycle of all its components, then realize the process loading of the\n\nautomatic auxiliary module during the process initialization.\n\n3.2.2 Social software process execution\n\nThe process execution is completed by the application layer module and service layer\n\nmodule together. Sellers on a social network use automatic auxiliary modules to\n\ncomplete business activities, trigger information capture module to collect social infor-\n\nmation, and send it to the information collection service for subsequent processing.\n\n3.2.3 Local processing of social information\n\nThe local processing of social information is mainly completed by the information col-\n\nlection service. In order to ensure the safe storage and transmission of the collected so-\n\ncial information, the information collection service first adopts the encryption and\n\ncompression method to realize the local security cache and then adopts the HTTPS se-\n\ncure communication and transmission protocol to upload the data.\n\n3.2.4 Background processing of social information\n\nThe background processing of social information is completed by the background ser-\n\nver. The server first receives the social information uploaded by the intelligent space,\n\nnext decrypts and decompresses the social information, cleans the plaintext data, uses\n\nthird-party OCR technology to identify text information in images, and adds it to the\n\nuser’s social information after simple data processing. Then, the classification of sellers\n\non a social network is realized through the data based on machine learning modeling.\n\nFinally, the classification results are stored in the target database.\n\n4 Methods\nTo classify the business attributes of social e-commerce based on the information of\n\nsellers on a social network, traditional feature matching scheme and classification clus-\n\ntering scheme based on machine learning can be used to establish the model. In this\n\nchapter, we introduce the scheme based on term frequency-inverse document fre-\n\nquency (TF-IDF) clustering and the classification scheme based on BERT.\n\n4.1 Feature classification and TF-IDF clustering\n\n4.1.1 Feature classification\n\nWe randomly select 5000 sellers on a social network from the data collected by the\n\nbackground server and extracted the text data of their social information for analysis.\n\nEach social e-commerce user contains an average of 50 social text data. Based on the\n\ncontent, we manually classify social e-commerce into 11 categories. With the help of e-\n\ncommerce platforms like JD.COM, 50–100 keywords are sorted out for each category,\n\nand these keywords are screened and expanded according to the language habits of\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 6 of 12\n\n\n\nsellers on a social network. On this basis, we collect all the social information of each\n\nsocial network seller, cut and remove word segmentation, and match the results with\n\nthe keywords of the selected 11 categories. The number of keywords that are matched\n\nis counted as the matching degree. According to the situation of different classification,\n\nthe threshold of matching degree is determined by manual screening of some results,\n\nand then all social e-commerce is classified according to the threshold. After\n\noptimization and verification, the accuracy of the classical feature matching scheme fi-\n\nnally reached 40%. However, due to the simplicity of the rules of the feature matching\n\nscheme, the small optimization space, the high misjudgment rate of the scheme, and\n\nthe large human intervention in the basic word segmentation process, it is difficult to\n\ncover various situations of social e-commerce due to the limitation of these basic key-\n\nwords, thus making it insensitive to the dynamic changes of new hot words of social e-\n\ncommerce.\n\n4.1.2 TF-IDF clustering\n\nTo achieve the goal of accurate classification of social e-commerce, we designed a\n\nscheme based on TF-IDF clustering. Term frequency-inverse document frequency (TF-\n\nIDF) is a commonly used weighted technique for information retrieval and text mining\n\nto evaluate the importance of a single word to a document in a set of documents or a\n\ncorpus. In this scheme, the social information of each social e-commerce user is\n\nmapped as one file set of TF-IDF, and all texts of all sellers on a social network are\n\nmapped as the whole corpus. The words with the highest frequency used by each social\n\ne-commerce user are the most representative words in this document and become key-\n\nwords. Category labels can be generated to calculate the probability that a document\n\nbelongs to a certain category using the naive Bayes algorithm formula. The advantages\n\nof TF-IDF clustering to achieve the classification of sellers on the social network in-\n\nclude the following: (1) clear mapping; (2) emphasize the weight of keywords and lower\n\nthe weight of non-keywords; (3) compared with other machine learning algorithms, the\n\ncharacteristic dimension of the model is greatly reduced to avoid the dimension disas-\n\nter; and (4) while improving the efficiency of classification calculation, ensure that the\n\nclassification effect has a good accuracy and recall rate. The architecture of the entire\n\nsolution is shown in Fig. 3.\n\nIn the text preprocessing stage, the first thing to do is to format the social informa-\n\ntion, mainly including deleting the space, deleting the newline character, merging the\n\nsocial e-commerce text, and so on, and finally getting the text to be processed for word\n\nsegmentation. In this scheme, we choose Jieba’s simplified mode for word segmenta-\n\ntion, then filter out the noise by filtering the stop words (e.g., yes, ah, etc.).\n\nIn the stage of establishing the vector space model, the first step is to load the train-\n\ning set and take the pre-processed social information of each social e-commerce user\n\nas a document. The next step is to generate a dictionary, by adding every word that ap-\n\npears in the training set to it, using the complete dictionary to calculate the TF-IDF\n\nvalue of each document. In this scheme, CountVectorizer and TfidfTransformer in Py-\n\nthon’s Scikit-Learn library are used. CountVectorizer is used to convert words in the\n\ntext into word frequency matrix, TfidfTransformer is used to count the TF-IDF value\n\nof each word in each document, and the top20 words in each document are taken as\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 7 of 12\n\n\n\nkeywords of sellers on a social network. After this step, the keywords with a large TF-\n\nIDF value in each document are the most representative words in the document, which\n\nbecome the keyword set of the social e-commerce user. Finally, the naive Bayes method\n\nis used to generate the category label, and the document vectors belonging to the same\n\ncategory in the TF-IDF matrix are added to form a matrix of m*n, where m represents\n\nthe number of categories and n represents the number of documents. The weight of\n\neach word is divided by the total weight of all words of the class, to calculate the prob-\n\nability that a document belongs to a certain class.\n\nIn the model optimization stage, we optimize the whole scheme model by adjusting\n\nthe stop word set, adjusting parameters (including CountVectorizer, TfidfTransformer\n\nclass construction parameters), and adjusting the category label generation method.\n\nThe main idea of TFIDF is if a word or phrase appears in an article with a high fre-\n\nquency of TF, and rarely appears in other articles, it is considered that the word or\n\nphrase has a good classification ability and is suitable for classification. TFIDF is actu-\n\nally: TF * IDF, TF is term frequency and IDF is inverse document frequency.\n\nIn a given document, word frequency refers to the frequency of a given word in the\n\ndocument. This number is a normalization of the number of words to prevent it from\n\nbeing biased towards long documents. For the word ti in a particular document, its im-\n\nportance can be expressed as:\n\ntf i; j ¼\nj D j\n\nj j : ti∈d j\n� � j\n\namong them:\n\n|D|: The total number of files in the corpus\n\n∣{j : ti ∈ dj}∣: The number of documents containing the term ti (i.e., the number of\n\ndocuments in ni, j ≠ 0). If the term is not in the corpus, it will cause the dividend to be\n\nzero, so it is generally used 1 + ∣ {j : ti ∈ dj}∣.\n\nand then:\n\n Social e-\ncommerce data\n\nData preparation\n\nFormat processing\n\nFilter stop words\n\nText preprocessing\n\nGenerate directory\n\nBuild the vector space and \nTF-IDF\n\nGenerate category tags\n\nBayesian classifier\n\nText articiple\n\nLoad training set \n\nBuild tf matrix \n\nbuild vector\n\nbuild matrix \n\nConditional probability \nmatrix\n\nModel optimization\n\nFig. 3 TF-IDF scheme framework\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 8 of 12\n\n\n\ntfidf i; j ¼ tf i; j � idf i\n\nA high word frequency in a particular document and a low document frequency of\n\nthe word in the entire document collection can produce a high-weight TF-IDF. There-\n\nfore, TF-IDF tends to filter out common words and keep important words.\n\n4.2 Classification scheme based on BERT\n\n4.2.1 Data label\n\nWe manually classify and mark the data of sellers on a social network according to the\n\ncharacteristics of the products. Classified labels include 38,970 items and 17 categories\n\nof data, including 3c, dress, food, car, house, beauty, makeup, training, jewelry, promo-\n\ntion, medicine and health, phone charge recharge, finance, card category, cigarettes, es-\n\nsays, and others. The pre-processing phase removes emojis, numbers, and spaces from\n\nthe text through Unicode encoding.\n\n4.2.2 Classification scheme\n\nIn the BERT model, Transformer, as an encoder-decoder model based on attention\n\nmechanism, solves the problem that RNN cannot deal with long-distance dependence\n\nand the model cannot be parallel, improving the performance of the model without re-\n\nducing the accuracy. At the same time, BERT introduced the shading language model\n\n(MLM, masked language model) and context prediction method, further enhance the\n\ntwo-way training of the ability of feature extraction and text. MLM uses Transformer\n\nencoders and bilateral contexts to predict random masked tokens to pre-train two-way\n\ntransformers. This makes BERT different from the GPT model, which can only conduct\n\none-way training and can better extract context information through feature fusion.\n\nAnaphase prediction is more embodied in QA and NLI. Therefore, we choose the\n\nBERT model based on the bidirectional coding technology of pre-training and attention\n\nmechanism to classify sellers on a social network.\n\nWe chose the official Chinese pre-training model of Google as the pre-training model\n\nof the experiment: BERT-Base which is Chinese simplified and traditional, 12-layer,\n\n768-hidden, 12-head, 110M parameters [16]. This pre-training model is obtained by\n\nGoogle’s unsupervised pre-training on a large-scale Chinese corpus. On this basis, we\n\nwill carry out fine-tuning to realize the classification model of sellers on a social net-\n\nwork. When dividing the data set, we divided 38,970 pieces of data into training set\n\nand verification set according to the ratio of 6:4, that is, 23,382 pieces of training set\n\nand 15,588 pieces of verification set.\n\n5 Results and discussion\n5.1 TF-IDF clustering scheme\n\nThe computer used in the experiment is configured with AMD Ryzen R5-4600H CPU,\n\n16G memory, and windows10 64bit operating system. First, the default construction\n\nparameters are used, and the average accuracy of each classification is 45.7%. Next, the\n\nparameters are adjusted through a genetic algorithm, and 100 rounds of genetic algo-\n\nrithm optimization are performed, then the average accuracy reached the highest value\n\nof 52.5%. In the process of genetic algorithm, statistical estimation of algorithm time is\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 9 of 12\n\n\n\nalso carried out. On average, on this data set, the running time of each round of the\n\nTF-IDF model is about 28 s.\n\nExperiments show that the accuracy of the TF-IDF clustering scheme has been\n\nimproved after optimization, and it has a certain reference value for the classifica-\n\ntion of sellers on a social network, but there is still a big gap from the accurate\n\nclassification. We found three reasons after analyzing the experimental results. (1)\n\nCompared to the feature matching scheme, the TF-IDF-based model is improved\n\nto some extent. However, the input of the model is still the result of direct word\n\nsegmentation, and more information is lost in the word segmentation process, such\n\nas the semantic information of previous and later texts and the repetition fre-\n\nquency of corpus, which are relatively important in the process of natural language\n\nprocessing. (2) The classification problem of sellers on a social network is compli-\n\ncated. This model does not analyze the correlation between words and is essen-\n\ntially an upgraded version of word frequency statistics, which makes it difficult to\n\nimprove the accuracy after reaching a certain value. (3) For the optimization of the\n\nmodel, only the parameters of the intermediate function are adjusted, and the\n\nmethod is not upgraded. Therefore, the machine learning scheme based on TF-IDF\n\nclustering cannot solve the problem of accurate classification of sellers on a social\n\nnetwork. In the next chapter, we will introduce a scheme based on deep learning\n\nto achieve the goal of classifying sellers on a social network.\n\n5.2 Classification scheme based on BERT\n\nText classification fine-tuning is to serialize the preprocessed text information\n\ntoken and input BERT, and select the final hidden state of the first token [CLS] as\n\na sentence vector to output to the full connection layer, and then output the prob-\n\nability of obtaining various labels corresponding to the text through the softmax\n\nlayer. The experimental schematic diagram is shown in Figs. 4 and 5. The max-\n\nimum length of the sequence (ma_seq_length) is set to 256 according to the actual\n\ntext length of the social information data set of the sellers on a social network and\n\nFig. 4 Text message token serialization\n\nFig. 5 Text classification BERT fine-tuning model structure diagram\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 10 of 12\n\n\n\n\n\nthe batch_size and learning rate adopt the official recommended values of 32 and\n\n2e−5. In addition, we also adjust the super parameter num_train_epochs and in-\n\ncrease the number of training epochs (num_train_epochs) from 3 to 9 to improve\n\nthe recognition rate of the model (Table 1). The results are shown in Table 2.\n\nWe select an additional 9500 text data of sellers on social networks and test the\n\nmodel after the same preprocessing. The accuracy rate is 90.5%, which is lower than\n\nthat of the verification set (96.2%). The reason may be that the data of the test set con-\n\ntains a large number of commodity terms not included in the corpus and training set,\n\nand the text description of these commodities is too colloquial. Sellers on a social net-\n\nwork often use colloquial words in the industry to replace the standard product names\n\nwhen releasing product information, such as “Bobo” instead of “Botox,” which to some\n\nextent limits the accuracy of text-based classification in the social e-commerce market\n\nscene.\n\n6 Conclusion\nThe classification model proposes in this paper achieves an accuracy of 90.5% in the\n\ntest data. However, there are still some problems such as non-standard description text.\n\nA corpus with a high correlation with a social e-commerce environment will be estab-\n\nlished in order to further improve the accuracy of social e-commerce classification. At\n\nthe same time, we will use the knowledge distillation technology to compress and refine\n\nthe existing model, so as to improve the model recognition rate while simplifying the\n\nmodel and improving the operational performance [16]. In addition, in view of the high\n\nlabor cost and time cost of large-scale data marking, the next step will be trying to\n\nmake full use of semi-supervised learning to train unlabeled data and labeled completed\n\ndata [17]. The full use of large-scale unlabeled data is conducive to further improving\n\nthe accuracy and generalization ability of the model, as well as the analysis and process-\n\ning of emerging products, providing strong data support for the model landing. Since\n\nthe image data have also been studied to profiling the users in a social network [18]\n\nand perceptual image hashing schemes are proposed [19], we will improve our model\n\nso that the image and text data are combined for analysis.\n\nTable 1 Corresponding table of epoch and accuracy\n\nEpoch eval_accuracy (%)\n\n3 95.84\n\n6 96.05\n\n9 96.2\n\nThe training results are shown in Table 2, and the recognition rate is 96.2%\n\nTable 2 Text information classification results of sellers on social network\n\nResults Value\n\neval_accuracy 96.2%\n\neval_loss 0.25033528\n\nglobal_step 6024\n\nLoss 0.25023073\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 11 of 12\n\n\n\nAbbreviations\nBERT: Bidirectional Encoder Representations from Transformers; DPCNN: Deep pyramid convolutional neural networks;\nOCR: Optical character recognition\n\nAcknowledgements\nNot applicable\n\nAuthors’ contributions\nHaoliang Cui designed the scheme and carried out the experiments. Shuai Shao gave suggestions on the structure of\nthe manuscript and participated in modifying the manuscript. All authors read and approved the final manuscript.\n\nFunding\nNational Natural Science Foundation of China (Award Number 61370195, U1536121)\n\nAvailability of data and materials\nhttps://github.com/cuihaoliang/User-portraits-of-social-e-commerce\n\nCompeting interests\nThe authors declare that they have no competing interests.\n\nAuthor details\n1Beijing Key Laboratory of Intelligent Telecommunication Software and Multimedia, Beijing University of Posts and\nTelecommunications, Beijing 100876, China. 2China Information Technology Security Evaluation Center, Beijing 100085,\nChina. 3Institute of Information Engineering, Chinese Academy of Sciences, Beijing 100088, China.\n\nReceived: 16 March 2020 Accepted: 25 December 2020\n\nReferences\n1. Y. Bengio, R. Ducharme, P. Vincent, et al., A neural probabilistic language model. J. Mach. Learn. Res. 3, 1137–1155\n\n(2003)\n2. Kim Y. Convolutional neural networks for sentence classification arXiv preprint arXiv:1408.5882, 2014.\n3. R. Johnson, T. Zhang, Deep pyramid convolutional neural networks for text categorization [C]//Proceedings of the 55th\n\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (2017), pp. 562–570\n4. Otter D W, Medina J R, Kalita J K. A survey of the usages of deep learning in natural language processing arXiv preprint\n\narXiv:1807.10854, 2018.\n5. R. Jozefowicz, W. Zaremba, I. Sutskever, An empirical exploration of recurrent network architectures [C]//International\n\nconference on machine learning (2015), pp. 2342–2350\n6. Liu P, Qiu X, Huang X. Recurrent neural network for text classification with multi-task learning arXiv preprint arXiv:1605.\n\n05101, 2016.\n7. Peters M E, Neumann M, Iyyer M, et al. Deep contextualized word representations. arXiv preprint arXiv:1802.05365, 2018.\n8. A. Vaswani, N. Shazeer, N. Parmar, et al., Attention is all you need [C]//Advances in neural information processing systems\n\n(2017), pp. 5998–6008\n9. Devlin J, Chang M W, Lee K, et al. BERT: pre-training of deep bidirectional transformers for language understanding\n\narXiv preprint arXiv:1810.04805, 2018.\n10. L. Wu et al., MLLDA: multi-level LDA for modelling users on content curation social networks. Neurocomputing 236, 73–\n\n81 (2017)\n11. L. Wu et al., Modeling the evolution of users’ preferences and social links in social networking services. IEEE Transact.\n\nKnowledge. Data. Eng. 29.6, 1240–1253 (2017)\n12. M. Malli, N. Said, A. Fadlallah, A new model for rating users’ profiles in online social networks. Comput. Information. Sci.\n\n10.2, 39–51 (2017)\n13. W. Chen et al., Development and application of big data platform for garlic industry chain. Comput. Mater. Continua 58.\n\n1, 229 (2019)\n14. M. Ning et al., GA-BP air quality evaluation method based on fuzzy theory. Comput. Mater. Continua 58.1, 215–227 (2019)\n15. Yin, Libo, et al. Relation extraction for massive news texts. Tech Science Press, CMC,60, no.1(2019), pp.275-285.\n16. Sun S, Cheng Y, Gan Z, et al. Patient knowledge distillation for BERT model compression arXiv preprint arXiv:1908.09355, 2019.\n17. Yalniz I Z, Jégou H, Chen K, et al. Billion-scale semi-supervised learning for image classification. arXiv preprint arXiv:1905.\n\n00546, 2019.\n18. Yaqiong Qiao, Xiangyang Luo, Chenliang Li, et al. Heterogeneous graph-based joint representation learning for users\n\nand POIs in location-based social network, Inf. Process. Manag., 2020, 57, 102151-1~102151-17\n19. Jinwei Wang, Hao Wang, Jian Li, Xiangyang Luo, Yun-Qing Shi, Sunil Kr. Jha, Detecting double JPEG compressed color\n\nimages with the same quantization matrix in spherical coordinates, IEEE Trans. on CSVT, doi: 10.1109/TCSVT.2019.\n2922309.\n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 12 of 12\n\n\n\nhttps://github.com/cuihaoliang/User-portraits-of-social-e-commerce\n\n\tAbstract\n\tIntroduction\n\tRelated work\n\tNatural language processing\n\tUser analysis of social networks\n\n\tData collection\n\tOverall structure\n\tSecurity container\n\tBackground server\n\n\tKey processes\n\tSocial software process initialization\n\tSocial software process execution\n\tLocal processing of social information\n\tBackground processing of social information\n\n\n\tMethods\n\tFeature classification and TF-IDF clustering\n\tFeature classification\n\tTF-IDF clustering\n\n\tClassification scheme based on BERT\n\tData label\n\tClassification scheme\n\n\n\tResults and discussion\n\tTF-IDF clustering scheme\n\tClassification scheme based on BERT\n\n\tConclusion\n\tAbbreviations\n\tAcknowledgements\n\tAuthors’ contributions\n\tFunding\n\tAvailability of data and materials\n\tCompeting interests\n\tAuthor details\n\tReferences\n\tPublisher’s Note\n\n",
      "metadata_storage_path": "aHR0cHM6Ly90cmFpbmluZ2NhdGFsb2dzdG9yYWdlLmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlci9zMTM2NDAtMDIwLTAwNTQ1LXoucGRm0",
      "metadata_content_type": "application/pdf",
      "metadata_author": "Haoliang Cui",
      "metadata_title": "A classification method for social information of sellers on social network",
      "people": [
        "Haoliang Cui1,",
        "Shuai Shao2",
        "Shaozhang Niu1,",
        "Chengjie Shi3",
        "Lingyu Zhou1",
        "Bengio",
        "Kim",
        "Liu",
        "Peters",
        "Devlin",
        "Wu",
        "Bayes",
        "Malli",
        "Chen",
        "Ning",
        "Yin",
        "Binder",
        "Jieba",
        "thon",
        "CLSIRATDET",
        "ECHOIKON",
        "Tok10",
        "Haoliang Cui",
        "Shuai Shao",
        "Y. Bengio",
        "R. Ducharme",
        "P. Vincent",
        "J. Mach",
        "Kim Y.",
        "R. Johnson",
        "T. Zhang",
        "Otter D W",
        "Medina J R",
        "Kalita J K",
        "R. Jozefowicz",
        "W. Zaremba",
        "I. Sutskever",
        "Liu P",
        "Qiu X",
        "Huang X",
        "Peters M E",
        "Neumann M",
        "Iyyer M",
        "A. Vaswani",
        "N. Shazeer",
        "N. Parmar",
        "Devlin J",
        "Chang M W",
        "Lee K",
        "L. Wu",
        "M. Malli",
        "N. Said",
        "A. Fadlallah",
        "W. Chen",
        "M. Ning",
        "Libo",
        "Sun S",
        "Cheng Y",
        "Gan Z",
        "Yalniz I Z",
        "Jégou H",
        "Chen K",
        "Yaqiong Qiao",
        "Xiangyang Luo",
        "Chenliang Li",
        "Jinwei Wang",
        "Hao Wang",
        "Jian Li",
        "Yun-Qing Shi",
        "Sunil Kr. Jha"
      ],
      "organizations": [
        "Security Evaluation Center",
        "BERT",
        "Taobao",
        "EURASIP Journal",
        "EURASIP",
        "Tencent AI Lab",
        "CNN",
        "RNN",
        "ELMO",
        "curation social network",
        "CCSN",
        "ule",
        "Intelligent Space",
        "AMS",
        "PMS",
        "Binder IPC",
        "SCRM",
        "activity manager service",
        "process",
        "JD",
        "JD.COM",
        "TF",
        "IDF",
        "TF-IDF",
        "Scikit-Learn",
        "TFIDF",
        "Unicode",
        "MLM",
        "Google",
        "DPCNN",
        "National Natural Science Foundation of",
        "Key Laboratory of Intelligent Telecommunication Software and Multimedia",
        "Beijing University of Posts",
        "Information Technology Security Evaluation Center",
        "of Information Engineering",
        "Chinese Academy of Sciences",
        "Association for Computational Linguistics",
        "arXiv",
        "IEEE Transact",
        "Tech Science Press",
        "IEEE",
        "Springer Nature"
      ],
      "locations": [
        "Beijing",
        "China",
        "space",
        "house"
      ],
      "keyphrases": [
        "2China Information Technology Security Evaluation Center",
        "Creative Commons Attribution 4.0 International License",
        "social network seller classification scheme",
        "other third party material",
        "2019 China social e-commerce industry",
        "mobile payment technology",
        "Creative Commons licence",
        "automated assistance capabilities",
        "social network apps",
        "social network applications",
        "original author(s",
        "RESEARCH Open Access",
        "different social software",
        "NLP classification model",
        "deep learning model",
        "Video Processing Cui",
        "social network use",
        "social information",
        "author information",
        "other means",
        "2021 Open Access",
        "systematic classification",
        "text information",
        "Haoliang Cui",
        "mobile phones",
        "assistance process",
        "Machine learning",
        "social relations",
        "social interaction",
        "The Author",
        "classification method",
        "accurate classification",
        "model training",
        "Shuai Shao2",
        "Shaozhang Niu",
        "Chengjie Shi3",
        "Lingyu Zhou1",
        "Full list",
        "hot topic",
        "recent years",
        "transaction money",
        "main activities",
        "operating environment",
        "final experiment",
        "continuous improvement",
        "one kind",
        "commodity trading",
        "rapid development",
        "ment report",
        "Internet society",
        "market size",
        "large scale",
        "high growth",
        "online retail",
        "trading activities",
        "same time",
        "uniform registration",
        "standardized terms",
        "product description",
        "great difficulty",
        "appropriate credit",
        "credit line",
        "intended use",
        "statutory regulation",
        "permitted use",
        "copyright holder",
        "EURASIP Journal",
        "User model",
        "38,970 sellers’ information",
        "user portrait",
        "commerce platforms",
        "orcid.org",
        "Correspondence",
        "Beijing",
        "article",
        "Abstract",
        "number",
        "users",
        "traditional",
        "merchandise",
        "server",
        "data",
        "picture",
        "help",
        "OCR",
        "accuracy",
        "Keywords",
        "1 Introduction",
        "employees",
        "percent",
        "billion",
        "Taobao",
        "content",
        "purchase",
        "sale",
        "goods",
        "products",
        "paper",
        "sharing",
        "adaptation",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "images",
        "permission",
        "licenses",
        "dropout full connection layer",
        "four benchmark text classifications",
        "content curation social network",
        "neural network language model",
        "various benchmark tests",
        "Tencent AI Lab",
        "long short-term memory",
        "Most existing studies",
        "tional neural network",
        "recurrent neural network",
        "one convolution layer",
        "one row vector",
        "Natural language processing",
        "different semantic environments",
        "end classification recognition",
        "multi- classification task",
        "sharing information mechanism",
        "long-distance text dependency",
        "bidirectional splicing method",
        "social content data",
        "e-commerce business classification",
        "social network analysis",
        "multimedia text data",
        "long sequence training",
        "large-scale text corpus",
        "text classification research",
        "NLP correlation algorithm",
        "double-layer two-way LSTM",
        "softmax layer",
        "user-generated content",
        "social data",
        "network structure",
        "semantic information",
        "social networks",
        "long-term research",
        "Video Processing",
        "social links",
        "one word",
        "sequential data",
        "2.2 User analysis",
        "Related work",
        "present stage",
        "good results",
        "control units",
        "original basis",
        "widespread phenomenon",
        "guage modeling",
        "feature-based form",
        "train- ing",
        "downstream tasks",
        "context words",
        "feature extractor",
        "feature fusion",
        "important part",
        "unified framework",
        "picture data",
        "word embedding",
        "Word vectors",
        "classification accuracy",
        "OCR algorithm",
        "gradient disappearance",
        "output gate",
        "RNN algorithm",
        "38,970 sellers",
        "addition",
        "order",
        "NNLM",
        "Bengio",
        "Researchers",
        "classifier",
        "Kim",
        "CNN",
        "study",
        "limitation",
        "fore",
        "capacity",
        "kind",
        "focus",
        "academia",
        "variation",
        "problem",
        "explosion",
        "Liu",
        "al.",
        "polysemy",
        "Peters",
        "dings",
        "ELMO",
        "impact",
        "grammatical",
        "ability",
        "features",
        "Transformer",
        "Devlin",
        "Cui",
        "Image",
        "Page",
        "Wu",
        "CCSN",
        "new online social network user",
        "air quality evaluation model",
        "machine learning classification model",
        "support vector machine",
        "mining user-generated content",
        "garlic industry chain",
        "Garlic planting management",
        "principal component analysis",
        "automatic assistant module",
        "various business processes",
        "independent running environment",
        "BP neural network",
        "ga-bp hybrid algorithm",
        "process communication interface",
        "file rating model",
        "social con- nections",
        "social informa- tion",
        "information acquisition module",
        "information grasping module",
        "cial network structure",
        "information collection service",
        "data analysis platform",
        "big data platform",
        "complicated user data",
        "data collection scheme",
        "independent container process",
        "users’ historical preferences",
        "Intelligent space app",
        "classification results",
        "ian model",
        "latent model",
        "Android platform",
        "independent operation",
        "consumption preferences",
        "genetic algorithm",
        "3 Data collection",
        "auxiliary process",
        "Overall structure",
        "APK file",
        "social apps",
        "social e-commerce",
        "social software",
        "information sharing",
        "secure container",
        "Security container",
        "multilevel LDA",
        "potential interest",
        "text descriptions",
        "future behavior",
        "near future",
        "price control",
        "fuzzy theory",
        "two methods",
        "ory relations",
        "English news",
        "combin- ation",
        "OCR technology",
        "behavior patterns",
        "iary ability",
        "background server",
        "two parts",
        "overall architecture",
        "application layer",
        "ant capability",
        "root privileges",
        "basic principle",
        "Binder IPC",
        "image data",
        "commerce activities",
        "iary tool",
        "MLLDA",
        "time",
        "Malli",
        "large",
        "terms",
        "Chen",
        "diction",
        "storage",
        "pretreatment",
        "knowledge",
        "Yin",
        "field",
        "combination",
        "tors",
        "sentences",
        "sellers",
        "experiment",
        "Fig.",
        "OS",
        "realization",
        "load",
        "ally",
        "intercept",
        "1.1",
        "interception automatic assistance  module Information Collection Binder IPC Binder IPC Binder",
        "social customer relationship management Linux Kernel Binder Mode",
        "AMS Proxy PMS Proxy Application Layer Mode Social App Interactive",
        "Intelligent Space Service Layer Mode",
        "social information Process Boundaries User Process",
        "machine learning model processing",
        "Binder communication interface",
        "IPC Backgroud Server",
        "data acquisition scheme Cui",
        "social software process initialization",
        "Interactive interception",
        "application layer module",
        "Social software process execution",
        "Social information collection",
        "service layer module",
        "automatic auxiliary module",
        "Space App",
        "social application process",
        "other technical means",
        "overall architecture diagram",
        "activity manager service",
        "package manager service",
        "sales assistance",
        "data transmission security",
        "dynamic proxy",
        "system library API",
        "customer acquisition",
        "four key processes",
        "core processing logic",
        "group management",
        "communication process",
        "social applications",
        "social network",
        "3.2 Key processes",
        "process startup",
        "independent process",
        "system service",
        "calling logic",
        "Local processing",
        "call logic",
        "data preprocessing",
        "data training",
        "auxiliary functions",
        "Java reflection",
        "main part",
        "three parts",
        "inter- action",
        "underlying system",
        "corresponding plugins",
        "daily affairs",
        "commercial attributes",
        "local cache",
        "main function",
        "result storage",
        "Batch upload",
        "Libc hook",
        "container",
        "interfaces",
        "boundary",
        "interaction",
        "loading",
        "Internet",
        "SCRM",
        "timer",
        "HTTPS",
        "parameters",
        "real",
        "simulation",
        "support",
        "fication",
        "chapter",
        "Encrypt",
        "1.2",
        "Machine learning categorizes social information",
        "traditional feature matching scheme",
        "3.2.1 Social software process initialization",
        "machine learning modeling",
        "Key flow chart",
        "automatic auxiliary modules",
        "third-party OCR technology",
        "local security cache",
        "complete business activities",
        "information capture module",
        "social network seller",
        "simple data processing",
        "50 social text data",
        "social e-commerce user",
        "service process",
        "complete process",
        "service layer",
        "matching degree",
        "tering scheme",
        "process loading",
        "4.1 Feature classification",
        "4.1.1 Feature classification",
        "business attributes",
        "classification scheme",
        "local processing",
        "plaintext data",
        "background processing",
        "intelligent space",
        "callback function",
        "life cycle",
        "subsequent processing",
        "safe storage",
        "compression method",
        "cure communication",
        "target database",
        "TF-IDF) clustering",
        "TF-IDF clustering",
        "JD.COM",
        "language habits",
        "word segmentation",
        "manual screening",
        "classification clus",
        "different classification",
        "The Server",
        "transmission protocol",
        "components",
        "Sellers",
        "encryption",
        "next",
        "4 Methods",
        "quency",
        "analysis",
        "average",
        "11 categories",
        "50–100 keywords",
        "category",
        "basis",
        "situation",
        "threshold",
        "3.2.2",
        "other machine learning algorithms",
        "naive Bayes algorithm formula",
        "classical feature matching scheme",
        "basic word segmentation process",
        "Term frequency-inverse document frequency",
        "naive Bayes method",
        "large human intervention",
        "high misjudgment rate",
        "basic key- words",
        "word segmenta- tion",
        "one file set",
        "new hot words",
        "small optimization space",
        "vector space model",
        "text preprocessing stage",
        "word frequency matrix",
        "model optimization stage",
        "social e-commerce text",
        "highest frequency",
        "recall rate",
        "single word",
        "various situations",
        "dynamic changes",
        "weighted technique",
        "information retrieval",
        "text mining",
        "clear mapping",
        "entire solution",
        "first thing",
        "newline character",
        "simplified mode",
        "Scikit-Learn library",
        "keyword set",
        "m*n",
        "scheme model",
        "representative words",
        "stop words",
        "top20 words",
        "Category labels",
        "classification calculation",
        "classification effect",
        "same category",
        "4.1.2 TF-IDF clustering",
        "TF-IDF matrix",
        "first step",
        "next step",
        "characteristic dimension",
        "good accuracy",
        "complete dictionary",
        "TF-IDF value",
        "document vectors",
        "total weight",
        "verification",
        "simplicity",
        "rules",
        "goal",
        "importance",
        "documents",
        "corpus",
        "texts",
        "probability",
        "advantages",
        "keywords",
        "lower",
        "efficiency",
        "architecture",
        "Jieba",
        "noise",
        "pears",
        "training",
        "CountVectorizer",
        "TfidfTransformer",
        "thon",
        "categories",
        "Conditional probability matrix Model optimization",
        "category label generation method",
        "3 TF-IDF scheme framework Cui",
        "official Chinese pre-training model",
        "phone charge recharge",
        "random masked tokens",
        "bidirectional coding technology",
        "vector build matrix",
        "context prediction method",
        "shading language model",
        "masked language model",
        "large-scale Chinese corpus",
        "entire document collection",
        "class construction parameters",
        "inverse document frequency",
        "low document frequency",
        "Load training set",
        "good classification ability",
        "high word frequency",
        "category tags",
        "4.2 Classification scheme",
        "card category",
        "4.2.2 Classification scheme",
        "tf matrix",
        "Data label",
        "vector space",
        "context information",
        "Anaphase prediction",
        "encoder-decoder model",
        "GPT model",
        "classification model",
        "particular document",
        "main idea",
        "other articles",
        "Format processing",
        "Bayesian classifier",
        "high-weight TF-IDF",
        "Classified labels",
        "promo- tion",
        "pre-processing phase",
        "Unicode encoding",
        "attention mechanism",
        "long-distance dependence",
        "feature extraction",
        "bilateral contexts",
        "two-way transformers",
        "traditional, 12-layer",
        "110M parameters",
        "data set",
        "two-way training",
        "one-way training",
        "term frequency",
        "commerce data",
        "Data preparation",
        "stop word",
        "Text preprocessing",
        "Text articiple",
        "common words",
        "important words",
        "long documents",
        "j � idf",
        "total number",
        "phrase",
        "normalization",
        "portance",
        "D|",
        "files",
        "dj",
        "Filter",
        "directory",
        "characteristics",
        "38,970 items",
        "17 categories",
        "3c",
        "dress",
        "food",
        "house",
        "beauty",
        "makeup",
        "jewelry",
        "medicine",
        "health",
        "finance",
        "cigarettes",
        "others",
        "emojis",
        "numbers",
        "spaces",
        "RNN",
        "performance",
        "MLM",
        "encoders",
        "QA",
        "NLI",
        "Google",
        "hidden",
        "fine-tuning",
        "38,970 pieces",
        "4.2.1",
        "fine-tuning model structure diagram Cui",
        "AMD Ryzen R5-4600H CPU",
        "windows10 64bit operating system",
        "Text message token serialization",
        "discussion 5.1 TF-IDF clustering scheme",
        "experimental schematic diagram",
        "direct word segmentation",
        "word frequency statistics",
        "final hidden state",
        "ECHOIKON Tokcls Tok1",
        "official recommended values",
        "feature matching scheme",
        "Text classification fine-tuning",
        "full connection layer",
        "natural language processing",
        "machine learning scheme",
        "text information token",
        "additional 9500 text data",
        "word segmentation process",
        "social information data",
        "first token",
        "5.2 Classification scheme",
        "text length",
        "TF-IDF model",
        "deep learning",
        "learning rate",
        "training set",
        "16G memory",
        "default construction",
        "genetic algo",
        "statistical estimation",
        "classifica- tion",
        "big gap",
        "three reasons",
        "later texts",
        "upgraded version",
        "intermediate function",
        "next chapter",
        "sentence vector",
        "various labels",
        "imum length",
        "super parameter",
        "training epochs",
        "recognition rate",
        "same preprocessing",
        "test set",
        "commodity terms",
        "experimental results",
        "TF-IDF-based model",
        "verification set",
        "highest value",
        "reference value",
        "average accuracy",
        "accuracy rate",
        "running time",
        "classification problem",
        "large number",
        "rithm optimization",
        "algorithm",
        "5 Results",
        "ratio",
        "23,382 pieces",
        "15,588 pieces",
        "computer",
        "100 rounds",
        "28 s",
        "Experiments",
        "extent",
        "input",
        "previous",
        "correlation",
        "words",
        "method",
        "preprocessed",
        "Figs.",
        "sequence",
        "actual",
        "CLSIRATDET",
        "Tok10",
        "batch_size",
        "train_epochs",
        "Table",
        "Funding National Natural Science Foundation",
        "Deep pyramid convolutional neural networks",
        "Table 2 Text information classification results",
        "perceptual image hashing schemes",
        "neural probabilistic language model",
        "social network Results Value",
        "knowledge distillation technology",
        "social net- work",
        "social e-commerce market",
        "social e-commerce environment",
        "Bidirectional Encoder Representations",
        "1Beijing Key Laboratory",
        "Intelligent Telecommunication Software",
        "recurrent network architectures",
        "social e-commerce classification",
        "Optical character recognition",
        "Kalita J K.",
        "standard product names",
        "Otter D W",
        "standard description text",
        "large-scale data marking",
        "strong data support",
        "Medina J R",
        "large-scale unlabeled data",
        "model recognition rate",
        "product information",
        "Information Engineering",
        "training results",
        "text description",
        "text data",
        "text-based classification",
        "sentence classification",
        "text categorization",
        "J. Mach.",
        "W. Zaremba",
        "Corresponding table",
        "R. Ducharme",
        "R. Johnson",
        "R. Jozefowicz",
        "test data",
        "colloquial words",
        "existing model",
        "operational performance",
        "labor cost",
        "time cost",
        "full use",
        "supervised learning",
        "generalization ability",
        "process- ing",
        "emerging products",
        "model landing",
        "Shuai Shao",
        "Award Number",
        "Competing interests",
        "Author details",
        "Chinese Academy",
        "Y. Bengio",
        "P. Vincent",
        "Kim Y",
        "T. Zhang",
        "Annual Meeting",
        "Computational Linguistics",
        "Long Papers",
        "I. Sutskever",
        "empirical exploration",
        "high correlation",
        "Epoch eval_accuracy",
        "Authors’ contributions",
        "final manuscript",
        "Beijing University",
        "commodities",
        "industry",
        "Bobo",
        "Botox",
        "scene",
        "6 Conclusion",
        "problems",
        "view",
        "Loss",
        "Abbreviations",
        "Transformers",
        "DPCNN",
        "Acknowledgements",
        "experiments",
        "suggestions",
        "structure",
        "Availability",
        "materials",
        "github",
        "cuihaoliang",
        "User-portraits",
        "social-e-commerce",
        "Multimedia",
        "Posts",
        "Telecommunications",
        "3Institute",
        "Sciences",
        "16 March",
        "References",
        "Res.",
        "Proceedings",
        "55th",
        "Association",
        "Volume",
        "survey",
        "usages",
        "International",
        "25",
        "GA-BP air quality evaluation method",
        "double JPEG compressed color images",
        "Heterogeneous graph-based joint representation learning",
        "Data label Classification scheme Results",
        "social information Methods Feature classification",
        "Overall structure Security container",
        "Deep contextualized word representations",
        "Social software process initialization",
        "content curation social networks",
        "neural information processing systems",
        "Recurrent neural network",
        "deep bidirectional transformers",
        "social networking services",
        "online social networks",
        "location-based social network",
        "massive news texts",
        "Tech Science Press",
        "Billion-scale semi-supervised learning",
        "Sunil Kr. Jha",
        "same quantization matrix",
        "Authors’ contributions Funding",
        "Author details References",
        "Yalniz I Z",
        "Jégou H",
        "Peters M E",
        "Patient knowledge distillation",
        "Chang M W",
        "rating users’ profiles",
        "TF-IDF clustering scheme",
        "Inf. Process",
        "machine learning",
        "multi-task learning",
        "text classification",
        "Devlin J",
        "language understanding",
        "W. Chen",
        "Gan Z",
        "Data collection",
        "image classification",
        "Neumann M",
        "Iyyer M",
        "M. Malli",
        "M. Ning",
        "Liu P",
        "Qiu X",
        "Huang X",
        "A. Vaswani",
        "N. Shazeer",
        "N. Parmar",
        "Lee K",
        "L. Wu",
        "multi-level LDA",
        "IEEE Transact",
        "N. Said",
        "A. Fadlallah",
        "new model",
        "Relation extraction",
        "Sun S",
        "Cheng Y",
        "model compression",
        "Chen K",
        "Yaqiong Qiao",
        "Xiangyang Luo",
        "Chenliang Li",
        "Jinwei Wang",
        "Hao Wang",
        "Jian Li",
        "Yun-Qing Shi",
        "spherical coordinates",
        "Springer Nature",
        "jurisdictional claims",
        "institutional affiliations",
        "User analysis",
        "Key processes",
        "users’ preferences",
        "conference",
        "Attention",
        "Neurocomputing",
        "evolution",
        "Eng.",
        "Sci.",
        "Development",
        "application",
        "Mater",
        "Libo",
        "CMC",
        "POIs",
        "Manag.",
        "CSVT",
        "Publisher",
        "Note",
        "regard",
        "maps",
        "Introduction",
        "discussion",
        "Conclusion",
        "14"
      ],
      "masked_text": "\nRESEARCH Open Access\n\nA classification method for social\ninformation of sellers on social network\n************1, ************ , *************1, ************* and ************\n\n* Correspondence: shaoshuaib@163.\ncom\n*****************************\n**************************, Beijing\n******, China\nFull list of author information is\navailable at the end of the article\n\nAbstract\n\nSocial e-commerce has been a hot topic in recent years, with the number of users\nincreasing year by year and the transaction money exploding. Unlike traditional e-\ncommerce, the main activities of social e-commerce are on social network apps. To\nclassify sellers by the merchandise, this article designs and implements a social\nnetwork seller classification scheme. We develop an app, which runs on the mobile\nphones of the sellers and provides the operating environment and automated\nassistance capabilities of social network applications. The app can collect social\ninformation published by the ******* during the assistance process, uploads to the\n****** to perform model training on the data. We collect 38,970 *******’ information,\nextract the text information in the picture with the help of OCR, and establish a\ndeep learning model based on BERT to classify the merchandise of *******. In the\nfinal experiment, we achieve an accuracy of more than 90%, which shows that the\nmodel can accurately classify ******* on a social network.\n\nKeywords: User model, Machine learning, Social e-commerce\n\n1 Introduction\nWith the continuous improvement of social network and mobile payment technology,\n\none kind of commodity trading based on social relations called social e-commerce is in\n\nrapid development. According to the **** China social e-commerce industry develop-\n\nment report released by the Internet society of China, the number of employees of so-\n\ncial e-commerce in China is expected to reach 48.01 million in ****, up by 58.3\n\npercent year on year, and the market size is expected to reach 2060.58 billion yuan, up\n\nby 63.2% year on year. Social e-commerce has become a large scale, and the high\n\ngrowth cannot be ignored. Different from e-commerce platforms such as ******, so-\n\ncial e-commerce is at the end of online retail. It carries out trading activities through\n\nsocial software and uses social interaction, user generated content and other means to\n\nassist the purchase and sale of goods. At the same time, sellers on social network use\n\ndifferent social software without uniform registration, have no systematic classification\n\nof products for sale, and there are no standardized terms for product description.\n\nThese bring great difficulty to the accurate classification of **** portrait. This paper\n\nproposes a method based on the NLP classification model, which can realize accurate\n\n© The Author(s). **** Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the\noriginal ******(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or\nother third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit\nline to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from the ****************. To view a\ncopy of this licence, visit ********************************************\n\n******* Journal on Image\nand Video Processing\n\n*** et al. ******* Journal on Image and Video Processing          (****) ****:4 \n******************************************\n\n*******************************************************************************\n************************************\nmailto:******************\nmailto:******************\n*******************************************\n\n\nbusiness classification of social e-commerce based on social information of social e-\n\ncommerce. This method analyzes 38,970 ******* on social networks and establishes a\n\ndeep learning model based on BERT to accurately classify the merchandise of *******.\n\nIn addition, we introduced the OCR algorithm to extract the text information in the\n\npicture and superimposed it on the social content data, which effectively improved the\n\nclassification accuracy. The final experiment shows that the measured accuracy is more\n\nthan 90%.\n\n2 Related work\n2.1 Natural language processing\n\nIn order to analyze e-commerce business classification based on social data of sellers on a\n\nsocial network, the text needs to be analyzed based on the NLP correlation algorithm.\n\nThe rapid development of NLP at the present stage is due to the neural network language\n\nmodel (NNLM) ****** et al. [1] proposed in ****. Researchers have been trying to realize\n\nthe end-to-end classification recognition by using a neural network as a classifier in the\n\ntext classification research based on word embedding. *** first introduces the convolu-\n\ntional neural network (CNN) into the study of text classification. The network structure is\n\na dropout full connection layer and a softmax layer connected after one convolution layer\n\n[2]. Although this algorithm achieves good results in various benchmark tests, it cannot\n\nobtain long-distance text dependency due to the limitation of network structure. There-\n\nfore, ************** proposed DPCNN, which further enhanced the extraction capacity\n\nof long-distance text dependency by deepening *** [3].\n\nSocial content data includes multimedia text data and picture data. With the help of\n\nOCR, we extract the text in the picture and convert the picture data into text data. Text\n\nis a kind of sequential data, and the classification of it by recurrent neural network\n\n(***) has been the focus of long-term research in academia [4]. As a variation of\n\nRNN, long short-term memory (LSTM) adds control units such as forgetting gate, in-\n\nput gate, and output gate on the original basis, which solves the problem of gradient\n\nexplosion and gradient disappearance in the long sequence training of RNN and pro-\n\nmotes the use of RNN [5]. By introducing the sharing information mechanism, ***\n\net al. further improved the accuracy of the RNN algorithm in the text multi-\n\nclassification task and achieved good results in four benchmark text classifications [6].\n\nHowever, Word vectors cannot be constructed in Word embedding to solve the\n\nproblem of polysemy. Even though different semantic environments are considered\n\nduring training, the result of training is still one word corresponding to one row vector.\n\nConsidering the widespread phenomenon of polysemy, ****** et al. propose embed-\n\ndings from language model (ELMO) to address the impact of polysemy on natural lan-\n\nguage modeling [7]. **** uses a feature-based form of pre-training. First, two-way\n\nLSTM is used to pre-train the corpus, and then word embedding resulting from train-\n\ning is adjusted by double-layer two-way LSTM when processing downstream tasks to\n\nadd more grammatical and semantic information according to the context words.\n\nThe ability of ELMO to extract features is limited for choosing LSTM as the feature\n\nextractor instead of Transformer [8], and ****’s bidirectional splicing method is also\n\nweak in feature fusion. Therefore, ****** et al. propose the BERT model, taking Trans-\n\nformer as a feature extractor to pre-train large-scale text corpus [9].\n\n*** et al. ******* Journal on Image and Video Processing          (****) ****:4 Page 2 of 12\n\n\n\n2.2 User analysis of social networks\n\nUser analysis is an important part of social network analysis. Most existing studies use\n\nuser-generated content or social links between ***** to simulate *****. ** et al. mod-\n\neled users on the content curation social network (CCSN) in the unified framework by\n\nmining user-generated content and social links [10]. They proposed a potential *****-\n\nian model, multilevel LDA (MLLDA), that could represent users of potential interest\n\nfound in social links formed by text descriptions contributed by users and information\n\nsharing. In ****, ** et al. proposed a latent model [11], trying to explain how the so-\n\ncial network structure and users’ historical preferences change over time affect each\n\nuser’s future behavior and predict each user’s consumption preferences and social con-\n\nnections in the near future. ***** et al. proposed a new online social network user pro-\n\nfile rating model [12], which solved the problem of large and complicated user data. In\n\nterms of data analysis platform, **** et al. [13] developed a big data platform for the\n\nstudy of the garlic industry chain. Garlic planting management, price control, and pre-\n\ndiction were realized through data collection, storage, and pretreatment. **** et al.\n\n[14] designed a ga-bp hybrid algorithm based on the fuzzy theory and constructed an\n\nair quality evaluation model by combining the knowledge of BP neural network, genetic\n\nalgorithm, and fuzzy theory. *** et al. [15] studied two methods of extracting supervis-\n\nory relations and applied them to the field of English news. One is the combination of\n\nsupport vector machine and principal component analysis, and the other is the combin-\n\nation of support vector machine and CNN, which can extract high-quality feature vec-\n\ntors from sentences of support vector machine. In the social apps, the data we obtain is\n\nmostly image data, so we introduced the OCR technology to identify text information\n\nin images.\n\n3 Data collection\nIn order to analyze the behavior patterns of social e-commerce, we developed an auxil-\n\niary tool for social e-commerce. In this tool, sellers on a social network are provided\n\nwith the independent running environment of social software and the automatic auxil-\n\niary ability, and the information acquisition module of the auxiliary process is used to\n\ncollect the social information published by sellers on a social network, which is\n\nuploaded to the background server for model training. We provided this tool to nearly\n\n10,000 sellers on a social network who participated in the experiment to obtain their\n\nsocial information in their e-commerce activities.\n\n3.1 Overall structure\n\nThe whole data collection scheme is mainly composed of two parts: intelligent space\n\napp and background server. The overall architecture is shown in Fig. 1. Intelligent\n\nspace app is deployed in the mobile phones of sellers on a social network and imple-\n\nmented based on the application layer of the Android platform, providing sellers on a\n\nsocial network with a secure container for the independent operation of social software.\n\nThe app contains the automatic assistant module, which provides the automatic assist-\n\nant capability of various business processes for ******, and collects the social informa-\n\ntion in the auxiliary process through the information grasping module. The collected\n\ninformation is cached and uploaded locally through the information collection service.\n\n*** et al. ******* Journal on Image and Video Processing          (****) ****:4 Page 3 of 12\n\n\n\nThe background server is responsible for receiving the collected data uploaded by the\n\nintelligent space, preprocessing the data first, and then classifying the social e-\n\ncommerce through the data based on the machine learning classification model, and fi-\n\nnally storing the classification results.\n\n3.1.1 Security container\n\nThe security container is designed to allow social software to run independently with-\n\nout modifying the OS or gaining root privileges. The basic principle of its realization is\n\nto create an independent container process; load APK file of social software dynamic-\n\nally; monitor and intercept process communication interface such as Binder IPC\n\nthrough Libc hook, Java reflection, dynamic proxy, and other technical means; and col-\n\nlect social information through an automatic assistant module. The main part of the\n\ncontainer is composed of an application layer module and a service layer module.\n\nThe application layer module is responsible for the process startup and execution of\n\nsocial software, and its main functions include three parts.\n\n******* Interactive interception The application layer module intercepts the inter-\n\naction between the application process and the underlying system in the container and\n\nmodifies the calling logic. By hook or dynamic proxy of system library API and Binder\n\ncommunication interface, the application layer module blocks all interfaces that interact\n\nwith the system during the execution of social software and controls the process\n\nboundary of interaction between social applications and system services.\n\n******* Social information collection The loading of the automatic auxiliary module\n\nby social software is realized when initializing the process of social application.\n\nThe application layer module injects the corresponding plugins in the automatic\n\nassistant module into the social application process. The automatic assistance mod-\n\nule provides a number of e-commerce auxiliary functions for sellers on a social\n\nnetwork, including customer acquisition, social customer relationship management\n\nLinux Kernel\n\n***********\n\nIntelligent Space\n\nService Layer Mode\nAMS Proxy PMS Proxy\n\nApplication Layer Mode\nSocial App\n\nInteractive \ninterception\n\nautomatic \nassistance  \nmodule\n\nInformation\nCollection \n\nBinder IPC\n\nBinder IPC\n\n****** \n*************\n\n Server\n\nInternet\n\nFig. 1 The overall architecture diagram of the data acquisition scheme\n\n*** et al. ******* Journal on Image and Video Processing          (****) ****:4 Page 4 of 12\n\n\n\n(SCRM), group management, sales assistance, and ***** affairs. ******* on social\n\nnetworks publish social information with commercial attributes through auxiliary\n\nfunctions, then the automatic auxiliary module will automatically collect the social\n\ninformation and send it to the information collection service for processing.\n\n******* Local processing of social information When the information collection ser-\n\nvice receives the social information collected by the automatic auxiliary module, the\n\ndata will be compressed and encrypted in the local cache. The service then uploads the\n\ncollected data to the background server periodically through the timer, and HTTPS is\n\nused to ensure data transmission security.\n\nThe main function of the service layer module is to take over the call logic modified\n\nby the application layer module by simulating the system service modify the parameters\n\nin the communication process and finally call the real system service. The service layer\n\nmodule exists in the container as an independent process. It focuses on the simulation\n\nof activity manager service (AMS) and package manager service (PMS) and realizes the\n\nsupport of system services in the process of launching and running social software.\n\n3.1.2 Background server\n\nThe background server mainly realizes the machine learning model processing of the\n\ncollected social data, including the functions of data preprocessing, data training, classi-\n\nfication, and result storage. The core processing logic will be described in chapter 5.\n\n3.2 Key processes\n\nThere are four key processes in the process of social information collection and pro-\n\ncessing. They are social software process initialization, social software process\n\nIntelligent \nSpace App \nlaunched\n\nProcess Boundaries\n\nUser Process\n\nSocial software process \ninitialization\n\nlaunching social \nsoftware\n\ninject automatic \nauxiliary\n\nSocial software process \nexecution\n\nRun the plug-in\n\nCollect social \ninformation\n\nProcess Boundaries\n\nUser Process\n\nLocal processing of \nsocial information\n\nBatch upload \nprocessed social \n\ninformation\n\nEncrypt, compress \nand store social \n\ninformation\n\nInternet\n\nInformation collecting \nservice process\n\nBackground processing \nof social information\n\nReceiving social \ninformation\n\nPreprocessing social \ninformation\n\nThe Server\n\nMachine learning \ncategorizes social \n\ninformation\n\nStore the \nclassification results\n\nFig. 2 Key flow chart of data acquisition scheme\n\n*** et al. ******* Journal on Image and Video Processing          (****) ****:4 Page 5 of 12\n\n\n\nexecution, local processing of social information, and background processing of social\n\ninformation. The complete process is shown in Fig. 2.\n\n3.2.1 Social software process initialization\n\nWhen launching social software, the intelligent space will first intercept the callback\n\nfunction of the life cycle of all its components, then realize the process loading of the\n\nautomatic auxiliary module during the process initialization.\n\n3.2.2 Social software process execution\n\nThe process execution is completed by the application layer module and service layer\n\nmodule together. Sellers on a social network use automatic auxiliary modules to\n\ncomplete business activities, trigger information capture module to collect social infor-\n\nmation, and send it to the information collection service for subsequent processing.\n\n3.2.3 Local processing of social information\n\nThe local processing of social information is mainly completed by the information col-\n\nlection service. In order to ensure the safe storage and transmission of the collected so-\n\ncial information, the information collection service first adopts the encryption and\n\ncompression method to realize the local security cache and then adopts the HTTPS se-\n\ncure communication and transmission protocol to upload the data.\n\n3.2.4 Background processing of social information\n\nThe background processing of social information is completed by the background ser-\n\nver. The server first receives the social information uploaded by the intelligent space,\n\nnext decrypts and decompresses the social information, cleans the plaintext data, uses\n\nthird-party OCR technology to identify text information in images, and adds it to the\n\n****’s social information after simple data processing. Then, the classification of sellers\n\non a social network is realized through the data based on machine learning modeling.\n\nFinally, the classification results are stored in the target database.\n\n4 Methods\nTo classify the business attributes of social e-commerce based on the information of\n\nsellers on a social network, traditional feature matching scheme and classification clus-\n\ntering scheme based on machine learning can be used to establish the model. In this\n\nchapter, we introduce the scheme based on term frequency-inverse document fre-\n\nquency (TF-IDF) clustering and the classification scheme based on BERT.\n\n4.1 Feature classification and TF-IDF clustering\n\n4.1.1 Feature classification\n\nWe randomly select 5000 sellers on a social network from the data collected by the\n\nbackground server and extracted the text data of their social information for analysis.\n\nEach social e-commerce **** contains an average of 50 social text data. Based on the\n\ncontent, we manually classify social e-commerce into 11 categories. With the help of e-\n\ncommerce platforms like ******, 50–100 keywords are sorted out for each category,\n\nand these keywords are screened and expanded according to the language habits of\n\n*** et al. ******* Journal on Image and Video Processing          (****) ****:4 Page 6 of 12\n\n\n\n******* on a social network. On this basis, we collect all the social information of each\n\nsocial network seller, cut and remove word segmentation, and match the results with\n\nthe keywords of the selected 11 categories. The number of keywords that are matched\n\nis counted as the matching degree. According to the situation of different classification,\n\nthe threshold of matching degree is determined by manual screening of some results,\n\nand then all social e-commerce is classified according to the threshold. After\n\noptimization and verification, the accuracy of the classical feature matching scheme fi-\n\nnally reached 40%. However, due to the simplicity of the rules of the feature matching\n\nscheme, the small optimization space, the high misjudgment rate of the scheme, and\n\nthe large human intervention in the basic word segmentation process, it is difficult to\n\ncover various situations of social e-commerce due to the limitation of these basic key-\n\nwords, thus making it insensitive to the dynamic changes of new hot words of social e-\n\ncommerce.\n\n4.1.2 TF-IDF clustering\n\nTo achieve the goal of accurate classification of social e-commerce, we designed a\n\nscheme based on TF-IDF clustering. Term frequency-inverse document frequency (TF-\n\nIDF) is a commonly used weighted technique for information retrieval and text mining\n\nto evaluate the importance of a single word to a document in a set of documents or a\n\ncorpus. In this scheme, the social information of each social e-commerce user is\n\nmapped as one file set of TF-IDF, and all texts of all sellers on a social network are\n\nmapped as the whole corpus. The words with the highest frequency used by each social\n\ne-commerce user are the most representative words in this document and become key-\n\nwords. Category labels can be generated to calculate the probability that a document\n\nbelongs to a certain category using the naive ***** algorithm formula. The advantages\n\nof TF-IDF clustering to achieve the classification of sellers on the social network in-\n\nclude the following: (1) clear mapping; (2) emphasize the weight of keywords and lower\n\nthe weight of non-keywords; (3) compared with other machine learning algorithms, the\n\ncharacteristic dimension of the model is greatly reduced to avoid the dimension disas-\n\nter; and (4) while improving the efficiency of classification calculation, ensure that the\n\nclassification effect has a good accuracy and recall rate. The architecture of the entire\n\nsolution is shown in Fig. 3.\n\nIn the text preprocessing stage, the first thing to do is to format the social informa-\n\ntion, mainly including deleting the space, deleting the newline character, merging the\n\nsocial e-commerce text, and so on, and finally getting the text to be processed for word\n\nsegmentation. In this scheme, we choose *****’s simplified mode for word segmenta-\n\ntion, then filter out the noise by filtering the stop words (e.g., yes, ah, etc.).\n\nIn the stage of establishing the vector space model, the first step is to load the train-\n\ning set and take the pre-processed social information of each social e-commerce user\n\nas a document. The next step is to generate a dictionary, by adding every word that ap-\n\npears in the training set to it, using the complete dictionary to calculate the TF-IDF\n\nvalue of each document. In this scheme, CountVectorizer and TfidfTransformer in Py-\n\nthon’s Scikit-Learn library are used. CountVectorizer is used to convert words in the\n\ntext into word frequency matrix, TfidfTransformer is used to count the TF-IDF value\n\nof each word in each document, and the top20 words in each document are taken as\n\n*** et al. ******* Journal on Image and Video Processing          (****) ****:4 Page 7 of 12\n\n\n\nkeywords of ******* on a social network. After this step, the keywords with a large TF-\n\nIDF value in each document are the most representative words in the document, which\n\nbecome the keyword set of the social e-commerce ****. Finally, the naive ***** method\n\nis used to generate the category label, and the document vectors belonging to the same\n\ncategory in the TF-IDF matrix are added to form a matrix of m*n, where m represents\n\nthe number of categories and n represents the number of documents. The weight of\n\neach word is divided by the total weight of all words of the class, to calculate the prob-\n\nability that a document belongs to a certain class.\n\nIn the model optimization stage, we optimize the whole scheme model by adjusting\n\nthe stop word set, adjusting parameters (including CountVectorizer, TfidfTransformer\n\nclass construction parameters), and adjusting the category label generation method.\n\nThe main idea of TFIDF is if a word or phrase appears in an article with a high fre-\n\nquency of TF, and rarely appears in other articles, it is considered that the word or\n\nphrase has a good classification ability and is suitable for classification. ***** is actu-\n\nally: TF * IDF, TF is term frequency and IDF is inverse document frequency.\n\nIn a given document, word frequency refers to the frequency of a given word in the\n\ndocument. This number is a normalization of the number of words to prevent it from\n\nbeing biased towards long documents. For the word ti in a particular document, its im-\n\nportance can be expressed as:\n\ntf i; j ¼\nj D j\n\nj j : ti∈d j\n� � j\n\namong them:\n\n|D|: The total number of files in the corpus\n\n∣{j : ti ∈ dj}∣: The number of documents containing the term ti (i.e., the number of\n\ndocuments in ni, j ≠ 0). If the term is not in the corpus, it will cause the dividend to be\n\nzero, so it is generally used 1 + ∣ {j : ti ∈ dj}∣.\n\nand then:\n\n Social e-\ncommerce data\n\nData preparation\n\nFormat processing\n\nFilter stop words\n\nText preprocessing\n\nGenerate directory\n\nBuild the vector space and \n******\n\nGenerate category tags\n\nBayesian classifier\n\nText articiple\n\nLoad training set \n\nBuild tf matrix \n\nbuild vector\n\nbuild matrix \n\nConditional probability \nmatrix\n\nModel optimization\n\nFig. 3 TF-IDF scheme framework\n\n*** et al. ******* Journal on Image and Video Processing          (****) ****:4 Page 8 of 12\n\n\n\ntfidf i; j ¼ tf i; j � idf i\n\nA high word frequency in a particular document and a low document frequency of\n\nthe word in the entire document collection can produce a high-weight TF-IDF. There-\n\nfore, TF-IDF tends to filter out common words and keep important words.\n\n4.2 Classification scheme based on BERT\n\n4.2.1 Data label\n\nWe manually classify and mark the data of sellers on a social network according to the\n\ncharacteristics of the products. Classified labels include 38,970 items and 17 categories\n\nof data, including 3c, dress, food, car, house, beauty, makeup, training, jewelry, promo-\n\ntion, medicine and health, phone charge recharge, finance, card category, cigarettes, es-\n\nsays, and others. The pre-processing phase removes emojis, numbers, and spaces from\n\nthe text through Unicode encoding.\n\n4.2.2 Classification scheme\n\nIn the BERT model, Transformer, as an encoder-decoder model based on attention\n\nmechanism, solves the problem that RNN cannot deal with long-distance dependence\n\nand the model cannot be parallel, improving the performance of the model without re-\n\nducing the accuracy. At the same time, **** introduced the shading language model\n\n(MLM, masked language model) and context prediction method, further enhance the\n\ntwo-way training of the ability of feature extraction and text. *** uses Transformer\n\nencoders and bilateral contexts to predict random masked tokens to pre-train two-way\n\ntransformers. This makes BERT different from the GPT model, which can only conduct\n\none-way training and can better extract context information through feature fusion.\n\nAnaphase prediction is more embodied in QA and NLI. Therefore, we choose the\n\nBERT model based on the bidirectional coding technology of pre-training and attention\n\nmechanism to classify ******* on a social network.\n\nWe chose the official Chinese pre-training model of ****** as the pre-training model\n\nof the experiment: BERT-Base which is Chinese simplified and traditional, 12-layer,\n\n768-hidden, 12-head, **** parameters [16]. This pre-training model is obtained by\n\n******’s unsupervised pre-training on a large-scale Chinese corpus. On this basis, we\n\nwill carry out fine-tuning to realize the classification model of sellers on a social net-\n\nwork. When dividing the data set, we divided 38,970 pieces of data into training set\n\nand verification set according to the ratio of 6:4, that is, 23,382 pieces of training set\n\nand 15,588 pieces of verification set.\n\n5 Results and discussion\n5.1 TF-IDF clustering scheme\n\nThe computer used in the experiment is configured with AMD Ryzen R5-***** CPU,\n\n16G memory, and windows10 64bit operating system. First, the default construction\n\nparameters are used, and the average accuracy of each classification is 45.7%. Next, the\n\nparameters are adjusted through a genetic algorithm, and 100 rounds of genetic algo-\n\nrithm optimization are performed, then the average accuracy reached the highest value\n\nof 52.5%. In the process of genetic algorithm, statistical estimation of algorithm time is\n\n*** et al. ******* Journal on Image and Video Processing          (****) ****:4 Page 9 of 12\n\n\n\nalso carried out. On average, on this data set, the running time of each round of the\n\nTF-IDF model is about 28 s.\n\nExperiments show that the accuracy of the TF-IDF clustering scheme has been\n\nimproved after optimization, and it has a certain reference value for the classifica-\n\ntion of sellers on a social network, but there is still a big gap from the accurate\n\nclassification. We found three reasons after analyzing the experimental results. (1)\n\nCompared to the feature matching scheme, the TF-IDF-based model is improved\n\nto some extent. However, the input of the model is still the result of direct word\n\nsegmentation, and more information is lost in the word segmentation process, such\n\nas the semantic information of previous and later texts and the repetition fre-\n\nquency of corpus, which are relatively important in the process of natural language\n\nprocessing. (2) The classification problem of ******* on a social network is compli-\n\ncated. This model does not analyze the correlation between words and is essen-\n\ntially an upgraded version of word frequency statistics, which makes it difficult to\n\nimprove the accuracy after reaching a certain value. (3) For the optimization of the\n\nmodel, only the parameters of the intermediate function are adjusted, and the\n\nmethod is not upgraded. Therefore, the machine learning scheme based on TF-IDF\n\nclustering cannot solve the problem of accurate classification of sellers on a social\n\nnetwork. In the next chapter, we will introduce a scheme based on deep learning\n\nto achieve the goal of classifying ******* on a social network.\n\n5.2 Classification scheme based on BERT\n\nText classification fine-tuning is to serialize the preprocessed text information\n\ntoken and input BERT, and select the final hidden state of the first token [CLS] as\n\na sentence vector to output to the full connection layer, and then output the prob-\n\nability of obtaining various labels corresponding to the text through the softmax\n\nlayer. The experimental schematic diagram is shown in Figs. 4 and 5. The max-\n\nimum length of the sequence (ma_seq_length) is set to 256 according to the actual\n\ntext length of the social information data set of the sellers on a social network and\n\nFig. 4 Text message token serialization\n\nFig. 5 Text classification BERT fine-tuning model structure diagram\n\n*** et al. ******* Journal on Image and Video Processing          (****) ****:4 Page 10 of 12\n\n ********** (E) ******************** . Tok10 \n\n\n\nthe batch_size and learning rate adopt the official recommended values of 32 and\n\n2e−5. In addition, we also adjust the super parameter num_train_epochs and in-\n\ncrease the number of training epochs (num_train_epochs) from 3 to 9 to improve\n\nthe recognition rate of the model (Table 1). The results are shown in Table 2.\n\nWe select an additional 9500 text data of sellers on social networks and test the\n\nmodel after the same preprocessing. The accuracy rate is 90.5%, which is lower than\n\nthat of the verification set (96.2%). The reason may be that the data of the test set con-\n\ntains a large number of commodity terms not included in the corpus and training set,\n\nand the text description of these commodities is too colloquial. ******* on a social net-\n\nwork often use colloquial words in the industry to replace the standard product names\n\nwhen releasing product information, such as “Bobo” instead of “Botox,” which to some\n\nextent limits the accuracy of text-based classification in the social e-commerce market\n\nscene.\n\n6 Conclusion\nThe classification model proposes in this paper achieves an accuracy of 90.5% in the\n\ntest data. However, there are still some problems such as non-standard description text.\n\nA corpus with a high correlation with a social e-commerce environment will be estab-\n\nlished in order to further improve the accuracy of social e-commerce classification. At\n\nthe same time, we will use the knowledge distillation technology to compress and refine\n\nthe existing model, so as to improve the model recognition rate while simplifying the\n\nmodel and improving the operational performance [16]. In addition, in view of the high\n\nlabor cost and time cost of large-scale data marking, the next step will be trying to\n\nmake full use of semi-supervised learning to train unlabeled data and labeled completed\n\ndata [17]. The full use of large-scale unlabeled data is conducive to further improving\n\nthe accuracy and generalization ability of the model, as well as the analysis and process-\n\ning of emerging products, providing strong data support for the model landing. Since\n\nthe image data have also been studied to profiling the users in a social network [18]\n\nand perceptual image hashing schemes are proposed [19], we will improve our model\n\nso that the image and text data are combined for analysis.\n\nTable 1 Corresponding table of epoch and accuracy\n\nEpoch eval_accuracy (%)\n\n3 95.84\n\n6 96.05\n\n9 96.2\n\nThe training results are shown in Table 2, and the recognition rate is 96.2%\n\nTable 2 Text information classification results of sellers on social network\n\nResults Value\n\neval_accuracy 96.2%\n\neval_loss 0.25033528\n\nglobal_step 6024\n\nLoss 0.25023073\n\n*** et al. ******* Journal on Image and Video Processing          (****) ****:4 Page 11 of 12\n\n\n\nAbbreviations\nBERT: Bidirectional Encoder Representations from Transformers; DPCNN: Deep pyramid convolutional neural networks;\nOCR: Optical character recognition\n\nAcknowledgements\nNot applicable\n\nAuthors’ contributions\n************ designed the scheme and carried out the experiments. ********** gave suggestions on the structure of\nthe manuscript and participated in modifying the manuscript. All ******* read and approved the final manuscript.\n\nFunding\n******************************************** (Award Number ********, U1536121)\n\nAvailability of data and materials\n******************************************************************\n\nCompeting interests\nThe ******* declare that they have no competing interests.\n\nAuthor details\n********************************************************************************, *******************************\nTelecommunications, Beijing ******* China. 2China ****************************************** Center, **************,\nChina. *************************************, ***************************, Beijing ******, China.\n\nReceived: ************* Accepted: ****************\n\nReferences\n1. *********, ***********, **********, et al., A neural probabilistic language model. *******. Learn. Res. 3, 1137–1155\n\n(****)\n2. ****** Convolutional neural networks for sentence classification arXiv preprint arXiv:1408.5882, ****.\n3. **********, ********, Deep pyramid convolutional neural networks for text categorization [C]//Proceedings of the 55th\n\n****** Meeting of the ***************************************** (Volume 1: Long Papers) (****), pp. 562–570\n4. *********, **********, **********. A survey of the usages of deep learning in natural language processing arXiv preprint\n\narXiv:1807.10854, ****.\n5. *************, **********, ************, An empirical exploration of recurrent network architectures [C]//International\n\nconference on machine learning (****), pp. 2342–2350\n6. *****, *****, *******. Recurrent neural network for text classification with multi-task learning arXiv preprint arXiv:****.\n\n05101, ****.\n7. **********, *********, *******, et al. Deep contextualized word representations. arXiv preprint arXiv:1802.05365, ****.\n8. **********, **********, *********, et al., Attention is all you need [C]//Advances in neural information processing systems\n\n(****), pp. 5998–6008\n9. ********, *********, *****, et al. BERT: pre-training of deep bidirectional transformers for language understanding\n\narXiv preprint arXiv:1810.04805, ****.\n10. ***** et al., MLLDA: multi-level LDA for modelling ***** on content curation social networks. Neurocomputing 236, 73–\n\n81 (****)\n11. ***** et al., Modeling the evolution of *****’ preferences and social links in social networking services. **** Transact.\n\nKnowledge. Data. Eng. 29.6, 1240–1253 (****)\n12. ********, *******, ************, A new model for rating *****’ profiles in online social networks. Comput. Information. Sci.\n\n10.2, 39–51 (****)\n13. ******* et al., Development and application of big data platform for garlic industry chain. Comput. Mater. Continua 58.\n\n1, 229 (****)\n14. ******* et al., GA-BP air quality evaluation method based on fuzzy theory. Comput. Mater. Continua 58.1, 215–227 (****)\n15. ***, ****, et al. Relation extraction for massive news texts. ******************, CMC,60, no.1(****), pp.275-285.\n16. *****, *******, *****, et al. Patient knowledge distillation for BERT model compression arXiv preprint arXiv:1908.09355, ****.\n17. **********, *******, ******, et al. Billion-scale semi-supervised learning for image classification. arXiv preprint arXiv:****.\n\n00546, ****.\n18. ************, *************, ************, et al. Heterogeneous graph-based joint representation learning for *****\n\nand POIs in location-based social network, Inf. Process. Manag., ****, 57, ******-1~******-17\n19. ***********, ********, *******, *************, ************, *************, Detecting double JPEG compressed color\n\nimages with the same quantization matrix in spherical coordinates, **** Trans. on CSVT, doi: 10.1109/TCSVT.****.\n*******.\n\nPublisher’s Note\n*************** remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\n*** et al. ******* Journal on Image and Video Processing          (****) ****:4 Page 12 of 12\n\n Published online: *************** \n\n******************************************************************\n\n\tAbstract\n\tIntroduction\n\tRelated work\n\tNatural language processing\n\tUser analysis of social networks\n\n\tData collection\n\tOverall structure\n\tSecurity container\n\tBackground server\n\n\tKey processes\n\tSocial software process initialization\n\tSocial software process execution\n\tLocal processing of social information\n\tBackground processing of social information\n\n\n\tMethods\n\tFeature classification and TF-IDF clustering\n\tFeature classification\n\tTF-IDF clustering\n\n\tClassification scheme based on BERT\n\tData label\n\tClassification scheme\n\n\n\tResults and discussion\n\tTF-IDF clustering scheme\n\tClassification scheme based on BERT\n\n\tConclusion\n\tAbbreviations\n\tAcknowledgements\n\tAuthors’ contributions\n\tFunding\n\tAvailability of data and materials\n\tCompeting interests\n\tAuthor details\n\tReferences\n\tPublisher’s Note\n\n",
      "merged_content": "\nRESEARCH Open Access\n\nA classification method for social\ninformation of sellers on social network\nHaoliang Cui1, Shuai Shao2* , Shaozhang Niu1, Chengjie Shi3 and Lingyu Zhou1\n\n* Correspondence: shaoshuaib@163.\ncom\n2China Information Technology\nSecurity Evaluation Center, Beijing\n100085, China\nFull list of author information is\navailable at the end of the article\n\nAbstract\n\nSocial e-commerce has been a hot topic in recent years, with the number of users\nincreasing year by year and the transaction money exploding. Unlike traditional e-\ncommerce, the main activities of social e-commerce are on social network apps. To\nclassify sellers by the merchandise, this article designs and implements a social\nnetwork seller classification scheme. We develop an app, which runs on the mobile\nphones of the sellers and provides the operating environment and automated\nassistance capabilities of social network applications. The app can collect social\ninformation published by the sellers during the assistance process, uploads to the\nserver to perform model training on the data. We collect 38,970 sellers’ information,\nextract the text information in the picture with the help of OCR, and establish a\ndeep learning model based on BERT to classify the merchandise of sellers. In the\nfinal experiment, we achieve an accuracy of more than 90%, which shows that the\nmodel can accurately classify sellers on a social network.\n\nKeywords: User model, Machine learning, Social e-commerce\n\n1 Introduction\nWith the continuous improvement of social network and mobile payment technology,\n\none kind of commodity trading based on social relations called social e-commerce is in\n\nrapid development. According to the 2019 China social e-commerce industry develop-\n\nment report released by the Internet society of China, the number of employees of so-\n\ncial e-commerce in China is expected to reach 48.01 million in 2019, up by 58.3\n\npercent year on year, and the market size is expected to reach 2060.58 billion yuan, up\n\nby 63.2% year on year. Social e-commerce has become a large scale, and the high\n\ngrowth cannot be ignored. Different from e-commerce platforms such as Taobao, so-\n\ncial e-commerce is at the end of online retail. It carries out trading activities through\n\nsocial software and uses social interaction, user generated content and other means to\n\nassist the purchase and sale of goods. At the same time, sellers on social network use\n\ndifferent social software without uniform registration, have no systematic classification\n\nof products for sale, and there are no standardized terms for product description.\n\nThese bring great difficulty to the accurate classification of user portrait. This paper\n\nproposes a method based on the NLP classification model, which can realize accurate\n\n© The Author(s). 2021 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the\noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or\nother third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit\nline to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a\ncopy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\n\nEURASIP Journal on Image\nand Video Processing\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 \nhttps://doi.org/10.1186/s13640-020-00545-z\n\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13640-020-00545-z&domain=pdf\nhttp://orcid.org/0000-0001-9638-0201\nmailto:shaoshuaib@163.com\nmailto:shaoshuaib@163.com\nhttp://creativecommons.org/licenses/by/4.0/\n\n\nbusiness classification of social e-commerce based on social information of social e-\n\ncommerce. This method analyzes 38,970 sellers on social networks and establishes a\n\ndeep learning model based on BERT to accurately classify the merchandise of sellers.\n\nIn addition, we introduced the OCR algorithm to extract the text information in the\n\npicture and superimposed it on the social content data, which effectively improved the\n\nclassification accuracy. The final experiment shows that the measured accuracy is more\n\nthan 90%.\n\n2 Related work\n2.1 Natural language processing\n\nIn order to analyze e-commerce business classification based on social data of sellers on a\n\nsocial network, the text needs to be analyzed based on the NLP correlation algorithm.\n\nThe rapid development of NLP at the present stage is due to the neural network language\n\nmodel (NNLM) Bengio et al. [1] proposed in 2003. Researchers have been trying to realize\n\nthe end-to-end classification recognition by using a neural network as a classifier in the\n\ntext classification research based on word embedding. Kim first introduces the convolu-\n\ntional neural network (CNN) into the study of text classification. The network structure is\n\na dropout full connection layer and a softmax layer connected after one convolution layer\n\n[2]. Although this algorithm achieves good results in various benchmark tests, it cannot\n\nobtain long-distance text dependency due to the limitation of network structure. There-\n\nfore, Tencent AI Lab proposed DPCNN, which further enhanced the extraction capacity\n\nof long-distance text dependency by deepening CNN [3].\n\nSocial content data includes multimedia text data and picture data. With the help of\n\nOCR, we extract the text in the picture and convert the picture data into text data. Text\n\nis a kind of sequential data, and the classification of it by recurrent neural network\n\n(RNN) has been the focus of long-term research in academia [4]. As a variation of\n\nRNN, long short-term memory (LSTM) adds control units such as forgetting gate, in-\n\nput gate, and output gate on the original basis, which solves the problem of gradient\n\nexplosion and gradient disappearance in the long sequence training of RNN and pro-\n\nmotes the use of RNN [5]. By introducing the sharing information mechanism, Liu\n\net al. further improved the accuracy of the RNN algorithm in the text multi-\n\nclassification task and achieved good results in four benchmark text classifications [6].\n\nHowever, Word vectors cannot be constructed in Word embedding to solve the\n\nproblem of polysemy. Even though different semantic environments are considered\n\nduring training, the result of training is still one word corresponding to one row vector.\n\nConsidering the widespread phenomenon of polysemy, Peters et al. propose embed-\n\ndings from language model (ELMO) to address the impact of polysemy on natural lan-\n\nguage modeling [7]. ELMO uses a feature-based form of pre-training. First, two-way\n\nLSTM is used to pre-train the corpus, and then word embedding resulting from train-\n\ning is adjusted by double-layer two-way LSTM when processing downstream tasks to\n\nadd more grammatical and semantic information according to the context words.\n\nThe ability of ELMO to extract features is limited for choosing LSTM as the feature\n\nextractor instead of Transformer [8], and ELMO’s bidirectional splicing method is also\n\nweak in feature fusion. Therefore, Devlin et al. propose the BERT model, taking Trans-\n\nformer as a feature extractor to pre-train large-scale text corpus [9].\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 2 of 12\n\n\n\n2.2 User analysis of social networks\n\nUser analysis is an important part of social network analysis. Most existing studies use\n\nuser-generated content or social links between users to simulate users. Wu et al. mod-\n\neled users on the content curation social network (CCSN) in the unified framework by\n\nmining user-generated content and social links [10]. They proposed a potential Bayes-\n\nian model, multilevel LDA (MLLDA), that could represent users of potential interest\n\nfound in social links formed by text descriptions contributed by users and information\n\nsharing. In 2017, Wu et al. proposed a latent model [11], trying to explain how the so-\n\ncial network structure and users’ historical preferences change over time affect each\n\nuser’s future behavior and predict each user’s consumption preferences and social con-\n\nnections in the near future. Malli et al. proposed a new online social network user pro-\n\nfile rating model [12], which solved the problem of large and complicated user data. In\n\nterms of data analysis platform, Chen et al. [13] developed a big data platform for the\n\nstudy of the garlic industry chain. Garlic planting management, price control, and pre-\n\ndiction were realized through data collection, storage, and pretreatment. Ning et al.\n\n[14] designed a ga-bp hybrid algorithm based on the fuzzy theory and constructed an\n\nair quality evaluation model by combining the knowledge of BP neural network, genetic\n\nalgorithm, and fuzzy theory. Yin et al. [15] studied two methods of extracting supervis-\n\nory relations and applied them to the field of English news. One is the combination of\n\nsupport vector machine and principal component analysis, and the other is the combin-\n\nation of support vector machine and CNN, which can extract high-quality feature vec-\n\ntors from sentences of support vector machine. In the social apps, the data we obtain is\n\nmostly image data, so we introduced the OCR technology to identify text information\n\nin images.\n\n3 Data collection\nIn order to analyze the behavior patterns of social e-commerce, we developed an auxil-\n\niary tool for social e-commerce. In this tool, sellers on a social network are provided\n\nwith the independent running environment of social software and the automatic auxil-\n\niary ability, and the information acquisition module of the auxiliary process is used to\n\ncollect the social information published by sellers on a social network, which is\n\nuploaded to the background server for model training. We provided this tool to nearly\n\n10,000 sellers on a social network who participated in the experiment to obtain their\n\nsocial information in their e-commerce activities.\n\n3.1 Overall structure\n\nThe whole data collection scheme is mainly composed of two parts: intelligent space\n\napp and background server. The overall architecture is shown in Fig. 1. Intelligent\n\nspace app is deployed in the mobile phones of sellers on a social network and imple-\n\nmented based on the application layer of the Android platform, providing sellers on a\n\nsocial network with a secure container for the independent operation of social software.\n\nThe app contains the automatic assistant module, which provides the automatic assist-\n\nant capability of various business processes for seller, and collects the social informa-\n\ntion in the auxiliary process through the information grasping module. The collected\n\ninformation is cached and uploaded locally through the information collection service.\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 3 of 12\n\n\n\nThe background server is responsible for receiving the collected data uploaded by the\n\nintelligent space, preprocessing the data first, and then classifying the social e-\n\ncommerce through the data based on the machine learning classification model, and fi-\n\nnally storing the classification results.\n\n3.1.1 Security container\n\nThe security container is designed to allow social software to run independently with-\n\nout modifying the OS or gaining root privileges. The basic principle of its realization is\n\nto create an independent container process; load APK file of social software dynamic-\n\nally; monitor and intercept process communication interface such as Binder IPC\n\nthrough Libc hook, Java reflection, dynamic proxy, and other technical means; and col-\n\nlect social information through an automatic assistant module. The main part of the\n\ncontainer is composed of an application layer module and a service layer module.\n\nThe application layer module is responsible for the process startup and execution of\n\nsocial software, and its main functions include three parts.\n\n3.1.1.1 Interactive interception The application layer module intercepts the inter-\n\naction between the application process and the underlying system in the container and\n\nmodifies the calling logic. By hook or dynamic proxy of system library API and Binder\n\ncommunication interface, the application layer module blocks all interfaces that interact\n\nwith the system during the execution of social software and controls the process\n\nboundary of interaction between social applications and system services.\n\n3.1.1.2 Social information collection The loading of the automatic auxiliary module\n\nby social software is realized when initializing the process of social application.\n\nThe application layer module injects the corresponding plugins in the automatic\n\nassistant module into the social application process. The automatic assistance mod-\n\nule provides a number of e-commerce auxiliary functions for sellers on a social\n\nnetwork, including customer acquisition, social customer relationship management\n\nLinux Kernel\n\nBinder Mode\n\nIntelligent Space\n\nService Layer Mode\nAMS Proxy PMS Proxy\n\nApplication Layer Mode\nSocial App\n\nInteractive \ninterception\n\nautomatic \nassistance  \nmodule\n\nInformation\nCollection \n\nBinder IPC\n\nBinder IPC\n\nBinder \nIPC Backgroud\n\n Server\n\nInternet\n\nFig. 1 The overall architecture diagram of the data acquisition scheme\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 4 of 12\n\n\n\n(SCRM), group management, sales assistance, and daily affairs. Sellers on social\n\nnetworks publish social information with commercial attributes through auxiliary\n\nfunctions, then the automatic auxiliary module will automatically collect the social\n\ninformation and send it to the information collection service for processing.\n\n3.1.1.3 Local processing of social information When the information collection ser-\n\nvice receives the social information collected by the automatic auxiliary module, the\n\ndata will be compressed and encrypted in the local cache. The service then uploads the\n\ncollected data to the background server periodically through the timer, and HTTPS is\n\nused to ensure data transmission security.\n\nThe main function of the service layer module is to take over the call logic modified\n\nby the application layer module by simulating the system service modify the parameters\n\nin the communication process and finally call the real system service. The service layer\n\nmodule exists in the container as an independent process. It focuses on the simulation\n\nof activity manager service (AMS) and package manager service (PMS) and realizes the\n\nsupport of system services in the process of launching and running social software.\n\n3.1.2 Background server\n\nThe background server mainly realizes the machine learning model processing of the\n\ncollected social data, including the functions of data preprocessing, data training, classi-\n\nfication, and result storage. The core processing logic will be described in chapter 5.\n\n3.2 Key processes\n\nThere are four key processes in the process of social information collection and pro-\n\ncessing. They are social software process initialization, social software process\n\nIntelligent \nSpace App \nlaunched\n\nProcess Boundaries\n\nUser Process\n\nSocial software process \ninitialization\n\nlaunching social \nsoftware\n\ninject automatic \nauxiliary\n\nSocial software process \nexecution\n\nRun the plug-in\n\nCollect social \ninformation\n\nProcess Boundaries\n\nUser Process\n\nLocal processing of \nsocial information\n\nBatch upload \nprocessed social \n\ninformation\n\nEncrypt, compress \nand store social \n\ninformation\n\nInternet\n\nInformation collecting \nservice process\n\nBackground processing \nof social information\n\nReceiving social \ninformation\n\nPreprocessing social \ninformation\n\nThe Server\n\nMachine learning \ncategorizes social \n\ninformation\n\nStore the \nclassification results\n\nFig. 2 Key flow chart of data acquisition scheme\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 5 of 12\n\n\n\nexecution, local processing of social information, and background processing of social\n\ninformation. The complete process is shown in Fig. 2.\n\n3.2.1 Social software process initialization\n\nWhen launching social software, the intelligent space will first intercept the callback\n\nfunction of the life cycle of all its components, then realize the process loading of the\n\nautomatic auxiliary module during the process initialization.\n\n3.2.2 Social software process execution\n\nThe process execution is completed by the application layer module and service layer\n\nmodule together. Sellers on a social network use automatic auxiliary modules to\n\ncomplete business activities, trigger information capture module to collect social infor-\n\nmation, and send it to the information collection service for subsequent processing.\n\n3.2.3 Local processing of social information\n\nThe local processing of social information is mainly completed by the information col-\n\nlection service. In order to ensure the safe storage and transmission of the collected so-\n\ncial information, the information collection service first adopts the encryption and\n\ncompression method to realize the local security cache and then adopts the HTTPS se-\n\ncure communication and transmission protocol to upload the data.\n\n3.2.4 Background processing of social information\n\nThe background processing of social information is completed by the background ser-\n\nver. The server first receives the social information uploaded by the intelligent space,\n\nnext decrypts and decompresses the social information, cleans the plaintext data, uses\n\nthird-party OCR technology to identify text information in images, and adds it to the\n\nuser’s social information after simple data processing. Then, the classification of sellers\n\non a social network is realized through the data based on machine learning modeling.\n\nFinally, the classification results are stored in the target database.\n\n4 Methods\nTo classify the business attributes of social e-commerce based on the information of\n\nsellers on a social network, traditional feature matching scheme and classification clus-\n\ntering scheme based on machine learning can be used to establish the model. In this\n\nchapter, we introduce the scheme based on term frequency-inverse document fre-\n\nquency (TF-IDF) clustering and the classification scheme based on BERT.\n\n4.1 Feature classification and TF-IDF clustering\n\n4.1.1 Feature classification\n\nWe randomly select 5000 sellers on a social network from the data collected by the\n\nbackground server and extracted the text data of their social information for analysis.\n\nEach social e-commerce user contains an average of 50 social text data. Based on the\n\ncontent, we manually classify social e-commerce into 11 categories. With the help of e-\n\ncommerce platforms like JD.COM, 50–100 keywords are sorted out for each category,\n\nand these keywords are screened and expanded according to the language habits of\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 6 of 12\n\n\n\nsellers on a social network. On this basis, we collect all the social information of each\n\nsocial network seller, cut and remove word segmentation, and match the results with\n\nthe keywords of the selected 11 categories. The number of keywords that are matched\n\nis counted as the matching degree. According to the situation of different classification,\n\nthe threshold of matching degree is determined by manual screening of some results,\n\nand then all social e-commerce is classified according to the threshold. After\n\noptimization and verification, the accuracy of the classical feature matching scheme fi-\n\nnally reached 40%. However, due to the simplicity of the rules of the feature matching\n\nscheme, the small optimization space, the high misjudgment rate of the scheme, and\n\nthe large human intervention in the basic word segmentation process, it is difficult to\n\ncover various situations of social e-commerce due to the limitation of these basic key-\n\nwords, thus making it insensitive to the dynamic changes of new hot words of social e-\n\ncommerce.\n\n4.1.2 TF-IDF clustering\n\nTo achieve the goal of accurate classification of social e-commerce, we designed a\n\nscheme based on TF-IDF clustering. Term frequency-inverse document frequency (TF-\n\nIDF) is a commonly used weighted technique for information retrieval and text mining\n\nto evaluate the importance of a single word to a document in a set of documents or a\n\ncorpus. In this scheme, the social information of each social e-commerce user is\n\nmapped as one file set of TF-IDF, and all texts of all sellers on a social network are\n\nmapped as the whole corpus. The words with the highest frequency used by each social\n\ne-commerce user are the most representative words in this document and become key-\n\nwords. Category labels can be generated to calculate the probability that a document\n\nbelongs to a certain category using the naive Bayes algorithm formula. The advantages\n\nof TF-IDF clustering to achieve the classification of sellers on the social network in-\n\nclude the following: (1) clear mapping; (2) emphasize the weight of keywords and lower\n\nthe weight of non-keywords; (3) compared with other machine learning algorithms, the\n\ncharacteristic dimension of the model is greatly reduced to avoid the dimension disas-\n\nter; and (4) while improving the efficiency of classification calculation, ensure that the\n\nclassification effect has a good accuracy and recall rate. The architecture of the entire\n\nsolution is shown in Fig. 3.\n\nIn the text preprocessing stage, the first thing to do is to format the social informa-\n\ntion, mainly including deleting the space, deleting the newline character, merging the\n\nsocial e-commerce text, and so on, and finally getting the text to be processed for word\n\nsegmentation. In this scheme, we choose Jieba’s simplified mode for word segmenta-\n\ntion, then filter out the noise by filtering the stop words (e.g., yes, ah, etc.).\n\nIn the stage of establishing the vector space model, the first step is to load the train-\n\ning set and take the pre-processed social information of each social e-commerce user\n\nas a document. The next step is to generate a dictionary, by adding every word that ap-\n\npears in the training set to it, using the complete dictionary to calculate the TF-IDF\n\nvalue of each document. In this scheme, CountVectorizer and TfidfTransformer in Py-\n\nthon’s Scikit-Learn library are used. CountVectorizer is used to convert words in the\n\ntext into word frequency matrix, TfidfTransformer is used to count the TF-IDF value\n\nof each word in each document, and the top20 words in each document are taken as\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 7 of 12\n\n\n\nkeywords of sellers on a social network. After this step, the keywords with a large TF-\n\nIDF value in each document are the most representative words in the document, which\n\nbecome the keyword set of the social e-commerce user. Finally, the naive Bayes method\n\nis used to generate the category label, and the document vectors belonging to the same\n\ncategory in the TF-IDF matrix are added to form a matrix of m*n, where m represents\n\nthe number of categories and n represents the number of documents. The weight of\n\neach word is divided by the total weight of all words of the class, to calculate the prob-\n\nability that a document belongs to a certain class.\n\nIn the model optimization stage, we optimize the whole scheme model by adjusting\n\nthe stop word set, adjusting parameters (including CountVectorizer, TfidfTransformer\n\nclass construction parameters), and adjusting the category label generation method.\n\nThe main idea of TFIDF is if a word or phrase appears in an article with a high fre-\n\nquency of TF, and rarely appears in other articles, it is considered that the word or\n\nphrase has a good classification ability and is suitable for classification. TFIDF is actu-\n\nally: TF * IDF, TF is term frequency and IDF is inverse document frequency.\n\nIn a given document, word frequency refers to the frequency of a given word in the\n\ndocument. This number is a normalization of the number of words to prevent it from\n\nbeing biased towards long documents. For the word ti in a particular document, its im-\n\nportance can be expressed as:\n\ntf i; j ¼\nj D j\n\nj j : ti∈d j\n� � j\n\namong them:\n\n|D|: The total number of files in the corpus\n\n∣{j : ti ∈ dj}∣: The number of documents containing the term ti (i.e., the number of\n\ndocuments in ni, j ≠ 0). If the term is not in the corpus, it will cause the dividend to be\n\nzero, so it is generally used 1 + ∣ {j : ti ∈ dj}∣.\n\nand then:\n\n Social e-\ncommerce data\n\nData preparation\n\nFormat processing\n\nFilter stop words\n\nText preprocessing\n\nGenerate directory\n\nBuild the vector space and \nTF-IDF\n\nGenerate category tags\n\nBayesian classifier\n\nText articiple\n\nLoad training set \n\nBuild tf matrix \n\nbuild vector\n\nbuild matrix \n\nConditional probability \nmatrix\n\nModel optimization\n\nFig. 3 TF-IDF scheme framework\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 8 of 12\n\n\n\ntfidf i; j ¼ tf i; j � idf i\n\nA high word frequency in a particular document and a low document frequency of\n\nthe word in the entire document collection can produce a high-weight TF-IDF. There-\n\nfore, TF-IDF tends to filter out common words and keep important words.\n\n4.2 Classification scheme based on BERT\n\n4.2.1 Data label\n\nWe manually classify and mark the data of sellers on a social network according to the\n\ncharacteristics of the products. Classified labels include 38,970 items and 17 categories\n\nof data, including 3c, dress, food, car, house, beauty, makeup, training, jewelry, promo-\n\ntion, medicine and health, phone charge recharge, finance, card category, cigarettes, es-\n\nsays, and others. The pre-processing phase removes emojis, numbers, and spaces from\n\nthe text through Unicode encoding.\n\n4.2.2 Classification scheme\n\nIn the BERT model, Transformer, as an encoder-decoder model based on attention\n\nmechanism, solves the problem that RNN cannot deal with long-distance dependence\n\nand the model cannot be parallel, improving the performance of the model without re-\n\nducing the accuracy. At the same time, BERT introduced the shading language model\n\n(MLM, masked language model) and context prediction method, further enhance the\n\ntwo-way training of the ability of feature extraction and text. MLM uses Transformer\n\nencoders and bilateral contexts to predict random masked tokens to pre-train two-way\n\ntransformers. This makes BERT different from the GPT model, which can only conduct\n\none-way training and can better extract context information through feature fusion.\n\nAnaphase prediction is more embodied in QA and NLI. Therefore, we choose the\n\nBERT model based on the bidirectional coding technology of pre-training and attention\n\nmechanism to classify sellers on a social network.\n\nWe chose the official Chinese pre-training model of Google as the pre-training model\n\nof the experiment: BERT-Base which is Chinese simplified and traditional, 12-layer,\n\n768-hidden, 12-head, 110M parameters [16]. This pre-training model is obtained by\n\nGoogle’s unsupervised pre-training on a large-scale Chinese corpus. On this basis, we\n\nwill carry out fine-tuning to realize the classification model of sellers on a social net-\n\nwork. When dividing the data set, we divided 38,970 pieces of data into training set\n\nand verification set according to the ratio of 6:4, that is, 23,382 pieces of training set\n\nand 15,588 pieces of verification set.\n\n5 Results and discussion\n5.1 TF-IDF clustering scheme\n\nThe computer used in the experiment is configured with AMD Ryzen R5-4600H CPU,\n\n16G memory, and windows10 64bit operating system. First, the default construction\n\nparameters are used, and the average accuracy of each classification is 45.7%. Next, the\n\nparameters are adjusted through a genetic algorithm, and 100 rounds of genetic algo-\n\nrithm optimization are performed, then the average accuracy reached the highest value\n\nof 52.5%. In the process of genetic algorithm, statistical estimation of algorithm time is\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 9 of 12\n\n\n\nalso carried out. On average, on this data set, the running time of each round of the\n\nTF-IDF model is about 28 s.\n\nExperiments show that the accuracy of the TF-IDF clustering scheme has been\n\nimproved after optimization, and it has a certain reference value for the classifica-\n\ntion of sellers on a social network, but there is still a big gap from the accurate\n\nclassification. We found three reasons after analyzing the experimental results. (1)\n\nCompared to the feature matching scheme, the TF-IDF-based model is improved\n\nto some extent. However, the input of the model is still the result of direct word\n\nsegmentation, and more information is lost in the word segmentation process, such\n\nas the semantic information of previous and later texts and the repetition fre-\n\nquency of corpus, which are relatively important in the process of natural language\n\nprocessing. (2) The classification problem of sellers on a social network is compli-\n\ncated. This model does not analyze the correlation between words and is essen-\n\ntially an upgraded version of word frequency statistics, which makes it difficult to\n\nimprove the accuracy after reaching a certain value. (3) For the optimization of the\n\nmodel, only the parameters of the intermediate function are adjusted, and the\n\nmethod is not upgraded. Therefore, the machine learning scheme based on TF-IDF\n\nclustering cannot solve the problem of accurate classification of sellers on a social\n\nnetwork. In the next chapter, we will introduce a scheme based on deep learning\n\nto achieve the goal of classifying sellers on a social network.\n\n5.2 Classification scheme based on BERT\n\nText classification fine-tuning is to serialize the preprocessed text information\n\ntoken and input BERT, and select the final hidden state of the first token [CLS] as\n\na sentence vector to output to the full connection layer, and then output the prob-\n\nability of obtaining various labels corresponding to the text through the softmax\n\nlayer. The experimental schematic diagram is shown in Figs. 4 and 5. The max-\n\nimum length of the sequence (ma_seq_length) is set to 256 according to the actual\n\ntext length of the social information data set of the sellers on a social network and\n\nFig. 4 Text message token serialization\n\nFig. 5 Text classification BERT fine-tuning model structure diagram\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 10 of 12\n\n CLSIRATDET (E) ECHOIKON Tokcls Tok1 . Tok10 \n\n\n\nthe batch_size and learning rate adopt the official recommended values of 32 and\n\n2e−5. In addition, we also adjust the super parameter num_train_epochs and in-\n\ncrease the number of training epochs (num_train_epochs) from 3 to 9 to improve\n\nthe recognition rate of the model (Table 1). The results are shown in Table 2.\n\nWe select an additional 9500 text data of sellers on social networks and test the\n\nmodel after the same preprocessing. The accuracy rate is 90.5%, which is lower than\n\nthat of the verification set (96.2%). The reason may be that the data of the test set con-\n\ntains a large number of commodity terms not included in the corpus and training set,\n\nand the text description of these commodities is too colloquial. Sellers on a social net-\n\nwork often use colloquial words in the industry to replace the standard product names\n\nwhen releasing product information, such as “Bobo” instead of “Botox,” which to some\n\nextent limits the accuracy of text-based classification in the social e-commerce market\n\nscene.\n\n6 Conclusion\nThe classification model proposes in this paper achieves an accuracy of 90.5% in the\n\ntest data. However, there are still some problems such as non-standard description text.\n\nA corpus with a high correlation with a social e-commerce environment will be estab-\n\nlished in order to further improve the accuracy of social e-commerce classification. At\n\nthe same time, we will use the knowledge distillation technology to compress and refine\n\nthe existing model, so as to improve the model recognition rate while simplifying the\n\nmodel and improving the operational performance [16]. In addition, in view of the high\n\nlabor cost and time cost of large-scale data marking, the next step will be trying to\n\nmake full use of semi-supervised learning to train unlabeled data and labeled completed\n\ndata [17]. The full use of large-scale unlabeled data is conducive to further improving\n\nthe accuracy and generalization ability of the model, as well as the analysis and process-\n\ning of emerging products, providing strong data support for the model landing. Since\n\nthe image data have also been studied to profiling the users in a social network [18]\n\nand perceptual image hashing schemes are proposed [19], we will improve our model\n\nso that the image and text data are combined for analysis.\n\nTable 1 Corresponding table of epoch and accuracy\n\nEpoch eval_accuracy (%)\n\n3 95.84\n\n6 96.05\n\n9 96.2\n\nThe training results are shown in Table 2, and the recognition rate is 96.2%\n\nTable 2 Text information classification results of sellers on social network\n\nResults Value\n\neval_accuracy 96.2%\n\neval_loss 0.25033528\n\nglobal_step 6024\n\nLoss 0.25023073\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 11 of 12\n\n\n\nAbbreviations\nBERT: Bidirectional Encoder Representations from Transformers; DPCNN: Deep pyramid convolutional neural networks;\nOCR: Optical character recognition\n\nAcknowledgements\nNot applicable\n\nAuthors’ contributions\nHaoliang Cui designed the scheme and carried out the experiments. Shuai Shao gave suggestions on the structure of\nthe manuscript and participated in modifying the manuscript. All authors read and approved the final manuscript.\n\nFunding\nNational Natural Science Foundation of China (Award Number 61370195, U1536121)\n\nAvailability of data and materials\nhttps://github.com/cuihaoliang/User-portraits-of-social-e-commerce\n\nCompeting interests\nThe authors declare that they have no competing interests.\n\nAuthor details\n1Beijing Key Laboratory of Intelligent Telecommunication Software and Multimedia, Beijing University of Posts and\nTelecommunications, Beijing 100876, China. 2China Information Technology Security Evaluation Center, Beijing 100085,\nChina. 3Institute of Information Engineering, Chinese Academy of Sciences, Beijing 100088, China.\n\nReceived: 16 March 2020 Accepted: 25 December 2020\n\nReferences\n1. Y. Bengio, R. Ducharme, P. Vincent, et al., A neural probabilistic language model. J. Mach. Learn. Res. 3, 1137–1155\n\n(2003)\n2. Kim Y. Convolutional neural networks for sentence classification arXiv preprint arXiv:1408.5882, 2014.\n3. R. Johnson, T. Zhang, Deep pyramid convolutional neural networks for text categorization [C]//Proceedings of the 55th\n\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (2017), pp. 562–570\n4. Otter D W, Medina J R, Kalita J K. A survey of the usages of deep learning in natural language processing arXiv preprint\n\narXiv:1807.10854, 2018.\n5. R. Jozefowicz, W. Zaremba, I. Sutskever, An empirical exploration of recurrent network architectures [C]//International\n\nconference on machine learning (2015), pp. 2342–2350\n6. Liu P, Qiu X, Huang X. Recurrent neural network for text classification with multi-task learning arXiv preprint arXiv:1605.\n\n05101, 2016.\n7. Peters M E, Neumann M, Iyyer M, et al. Deep contextualized word representations. arXiv preprint arXiv:1802.05365, 2018.\n8. A. Vaswani, N. Shazeer, N. Parmar, et al., Attention is all you need [C]//Advances in neural information processing systems\n\n(2017), pp. 5998–6008\n9. Devlin J, Chang M W, Lee K, et al. BERT: pre-training of deep bidirectional transformers for language understanding\n\narXiv preprint arXiv:1810.04805, 2018.\n10. L. Wu et al., MLLDA: multi-level LDA for modelling users on content curation social networks. Neurocomputing 236, 73–\n\n81 (2017)\n11. L. Wu et al., Modeling the evolution of users’ preferences and social links in social networking services. IEEE Transact.\n\nKnowledge. Data. Eng. 29.6, 1240–1253 (2017)\n12. M. Malli, N. Said, A. Fadlallah, A new model for rating users’ profiles in online social networks. Comput. Information. Sci.\n\n10.2, 39–51 (2017)\n13. W. Chen et al., Development and application of big data platform for garlic industry chain. Comput. Mater. Continua 58.\n\n1, 229 (2019)\n14. M. Ning et al., GA-BP air quality evaluation method based on fuzzy theory. Comput. Mater. Continua 58.1, 215–227 (2019)\n15. Yin, Libo, et al. Relation extraction for massive news texts. Tech Science Press, CMC,60, no.1(2019), pp.275-285.\n16. Sun S, Cheng Y, Gan Z, et al. Patient knowledge distillation for BERT model compression arXiv preprint arXiv:1908.09355, 2019.\n17. Yalniz I Z, Jégou H, Chen K, et al. Billion-scale semi-supervised learning for image classification. arXiv preprint arXiv:1905.\n\n00546, 2019.\n18. Yaqiong Qiao, Xiangyang Luo, Chenliang Li, et al. Heterogeneous graph-based joint representation learning for users\n\nand POIs in location-based social network, Inf. Process. Manag., 2020, 57, 102151-1~102151-17\n19. Jinwei Wang, Hao Wang, Jian Li, Xiangyang Luo, Yun-Qing Shi, Sunil Kr. Jha, Detecting double JPEG compressed color\n\nimages with the same quantization matrix in spherical coordinates, IEEE Trans. on CSVT, doi: 10.1109/TCSVT.2019.\n2922309.\n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 12 of 12\n\n Published online: 14 January 2021 \n\nhttps://github.com/cuihaoliang/User-portraits-of-social-e-commerce\n\n\tAbstract\n\tIntroduction\n\tRelated work\n\tNatural language processing\n\tUser analysis of social networks\n\n\tData collection\n\tOverall structure\n\tSecurity container\n\tBackground server\n\n\tKey processes\n\tSocial software process initialization\n\tSocial software process execution\n\tLocal processing of social information\n\tBackground processing of social information\n\n\n\tMethods\n\tFeature classification and TF-IDF clustering\n\tFeature classification\n\tTF-IDF clustering\n\n\tClassification scheme based on BERT\n\tData label\n\tClassification scheme\n\n\n\tResults and discussion\n\tTF-IDF clustering scheme\n\tClassification scheme based on BERT\n\n\tConclusion\n\tAbbreviations\n\tAcknowledgements\n\tAuthors’ contributions\n\tFunding\n\tAvailability of data and materials\n\tCompeting interests\n\tAuthor details\n\tReferences\n\tPublisher’s Note\n\n",
      "text": [
        "CLSIRATDET (E) ECHOIKON Tokcls Tok1 . Tok10",
        "Published online: 14 January 2021"
      ],
      "layoutText": [
        "{\"language\":\"en\",\"text\":\"CLSIRATDET (E) ECHOIKON Tokcls Tok1 . Tok10\",\"lines\":[{\"boundingBox\":[{\"x\":99,\"y\":15},{\"x\":893,\"y\":14},{\"x\":893,\"y\":54},{\"x\":99,\"y\":55}],\"text\":\"CLSIRATDET (E) ECHOIKON\"},{\"boundingBox\":[{\"x\":49,\"y\":112},{\"x\":171,\"y\":112},{\"x\":172,\"y\":141},{\"x\":49,\"y\":142}],\"text\":\"Tokcls\"},{\"boundingBox\":[{\"x\":282,\"y\":113},{\"x\":372,\"y\":113},{\"x\":373,\"y\":142},{\"x\":282,\"y\":143}],\"text\":\"Tok1\"},{\"boundingBox\":[{\"x\":473,\"y\":110},{\"x\":490,\"y\":109},{\"x\":492,\"y\":129},{\"x\":474,\"y\":131}],\"text\":\".\"},{\"boundingBox\":[{\"x\":615,\"y\":112},{\"x\":730,\"y\":112},{\"x\":730,\"y\":142},{\"x\":615,\"y\":141}],\"text\":\"Tok10\"}],\"words\":[{\"boundingBox\":[{\"x\":99,\"y\":16},{\"x\":435,\"y\":15},{\"x\":434,\"y\":56},{\"x\":99,\"y\":55}],\"text\":\"CLSIRATDET\"},{\"boundingBox\":[{\"x\":478,\"y\":15},{\"x\":573,\"y\":15},{\"x\":572,\"y\":56},{\"x\":478,\"y\":56}],\"text\":\"(E)\"},{\"boundingBox\":[{\"x\":597,\"y\":15},{\"x\":892,\"y\":15},{\"x\":892,\"y\":55},{\"x\":597,\"y\":56}],\"text\":\"ECHOIKON\"},{\"boundingBox\":[{\"x\":51,\"y\":113},{\"x\":168,\"y\":113},{\"x\":167,\"y\":142},{\"x\":51,\"y\":143}],\"text\":\"Tokcls\"},{\"boundingBox\":[{\"x\":284,\"y\":115},{\"x\":372,\"y\":114},{\"x\":372,\"y\":142},{\"x\":284,\"y\":144}],\"text\":\"Tok1\"},{\"boundingBox\":[{\"x\":473,\"y\":110},{\"x\":485,\"y\":109},{\"x\":487,\"y\":130},{\"x\":475,\"y\":131}],\"text\":\".\"},{\"boundingBox\":[{\"x\":617,\"y\":113},{\"x\":724,\"y\":113},{\"x\":724,\"y\":143},{\"x\":617,\"y\":142}],\"text\":\"Tok10\"}]}",
        "{\"language\":\"en\",\"text\":\"Published online: 14 January 2021\",\"lines\":[{\"boundingBox\":[{\"x\":5,\"y\":16},{\"x\":978,\"y\":18},{\"x\":978,\"y\":72},{\"x\":5,\"y\":69}],\"text\":\"Published online: 14 January 2021\"}],\"words\":[{\"boundingBox\":[{\"x\":5,\"y\":16},{\"x\":269,\"y\":17},{\"x\":269,\"y\":70},{\"x\":5,\"y\":69}],\"text\":\"Published\"},{\"boundingBox\":[{\"x\":291,\"y\":17},{\"x\":498,\"y\":17},{\"x\":498,\"y\":71},{\"x\":291,\"y\":71}],\"text\":\"online:\"},{\"boundingBox\":[{\"x\":510,\"y\":17},{\"x\":572,\"y\":18},{\"x\":573,\"y\":72},{\"x\":510,\"y\":71}],\"text\":\"14\"},{\"boundingBox\":[{\"x\":591,\"y\":18},{\"x\":816,\"y\":18},{\"x\":817,\"y\":73},{\"x\":591,\"y\":72}],\"text\":\"January\"},{\"boundingBox\":[{\"x\":835,\"y\":18},{\"x\":977,\"y\":19},{\"x\":977,\"y\":73},{\"x\":835,\"y\":73}],\"text\":\"2021\"}]}"
      ],
      "pii_entities": [
        {
          "text": "Haoliang Cui",
          "type": "Person",
          "subtype": null,
          "offset": 99,
          "length": 12,
          "score": 0.89
        },
        {
          "text": "Shuai Shao2",
          "type": "Person",
          "subtype": null,
          "offset": 114,
          "length": 11,
          "score": 0.94
        },
        {
          "text": "Shaozhang Niu",
          "type": "Person",
          "subtype": null,
          "offset": 129,
          "length": 13,
          "score": 0.93
        },
        {
          "text": "Chengjie Shi3",
          "type": "Person",
          "subtype": null,
          "offset": 145,
          "length": 13,
          "score": 0.93
        },
        {
          "text": "Lingyu Zhou1",
          "type": "Person",
          "subtype": null,
          "offset": 163,
          "length": 12,
          "score": 0.87
        },
        {
          "text": "2China Information Technology",
          "type": "Organization",
          "subtype": null,
          "offset": 215,
          "length": 29,
          "score": 0.55
        },
        {
          "text": "Security Evaluation Center",
          "type": "Organization",
          "subtype": null,
          "offset": 245,
          "length": 26,
          "score": 0.76
        },
        {
          "text": "100085",
          "type": "DateTime",
          "subtype": "Date",
          "offset": 281,
          "length": 6,
          "score": 0.8
        },
        {
          "text": "sellers",
          "type": "PersonType",
          "subtype": null,
          "offset": 974,
          "length": 7,
          "score": 0.78
        },
        {
          "text": "server",
          "type": "PersonType",
          "subtype": null,
          "offset": 1028,
          "length": 6,
          "score": 0.51
        },
        {
          "text": "sellers",
          "type": "PersonType",
          "subtype": null,
          "offset": 1092,
          "length": 7,
          "score": 0.8
        },
        {
          "text": "sellers",
          "type": "PersonType",
          "subtype": null,
          "offset": 1261,
          "length": 7,
          "score": 0.85
        },
        {
          "text": "sellers",
          "type": "PersonType",
          "subtype": null,
          "offset": 1387,
          "length": 7,
          "score": 0.94
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 1697,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 1895,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Taobao",
          "type": "Organization",
          "subtype": null,
          "offset": 2153,
          "length": 6,
          "score": 0.76
        },
        {
          "text": "user",
          "type": "PersonType",
          "subtype": null,
          "offset": 2671,
          "length": 4,
          "score": 0.54
        },
        {
          "text": "2021",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 2800,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "author",
          "type": "PersonType",
          "subtype": null,
          "offset": 3056,
          "length": 6,
          "score": 0.92
        },
        {
          "text": "copyright holder",
          "type": "PersonType",
          "subtype": null,
          "offset": 3555,
          "length": 16,
          "score": 0.65
        },
        {
          "text": "http://creativecommons.org/licenses/by/4.0/.",
          "type": "URL",
          "subtype": null,
          "offset": 3611,
          "length": 44,
          "score": 0.8
        },
        {
          "text": "EURASIP",
          "type": "Organization",
          "subtype": null,
          "offset": 3657,
          "length": 7,
          "score": 0.72
        },
        {
          "text": "Cui",
          "type": "Person",
          "subtype": null,
          "offset": 3704,
          "length": 3,
          "score": 0.95
        },
        {
          "text": "EURASIP",
          "type": "Organization",
          "subtype": null,
          "offset": 3715,
          "length": 7,
          "score": 0.54
        },
        {
          "text": "2021",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 3771,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "2021",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 3777,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "https://doi.org/10.1186/s13640-020-00545-z",
          "type": "URL",
          "subtype": null,
          "offset": 3785,
          "length": 42,
          "score": 0.8
        },
        {
          "text": "http://crossmark.crossref.org/dialog/?doi=10.1186/s13640-020-00545-z&domain=pdf",
          "type": "URL",
          "subtype": null,
          "offset": 3829,
          "length": 79,
          "score": 0.8
        },
        {
          "text": "http://orcid.org/0000-0001-9638-0201",
          "type": "URL",
          "subtype": null,
          "offset": 3909,
          "length": 36,
          "score": 0.8
        },
        {
          "text": "shaoshuaib@163.com",
          "type": "Email",
          "subtype": null,
          "offset": 3953,
          "length": 18,
          "score": 0.8
        },
        {
          "text": "shaoshuaib@163.com",
          "type": "Email",
          "subtype": null,
          "offset": 3979,
          "length": 18,
          "score": 0.8
        },
        {
          "text": "http://creativecommons.org/licenses/by/4.0/",
          "type": "URL",
          "subtype": null,
          "offset": 3998,
          "length": 43,
          "score": 0.8
        },
        {
          "text": "sellers",
          "type": "PersonType",
          "subtype": null,
          "offset": 4169,
          "length": 7,
          "score": 0.64
        },
        {
          "text": "sellers",
          "type": "PersonType",
          "subtype": null,
          "offset": 4291,
          "length": 7,
          "score": 0.77
        },
        {
          "text": "Bengio",
          "type": "Person",
          "subtype": null,
          "offset": 4903,
          "length": 6,
          "score": 0.93
        },
        {
          "text": "2003",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 4933,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Kim",
          "type": "Person",
          "subtype": null,
          "offset": 5126,
          "length": 3,
          "score": 0.98
        },
        {
          "text": "Tencent AI Lab",
          "type": "Organization",
          "subtype": null,
          "offset": 5530,
          "length": 14,
          "score": 0.94
        },
        {
          "text": "CNN",
          "type": "Organization",
          "subtype": null,
          "offset": 5655,
          "length": 3,
          "score": 0.95
        },
        {
          "text": "RNN",
          "type": "Organization",
          "subtype": null,
          "offset": 5931,
          "length": 3,
          "score": 0.69
        },
        {
          "text": "Liu",
          "type": "Person",
          "subtype": null,
          "offset": 6344,
          "length": 3,
          "score": 0.97
        },
        {
          "text": "Peters",
          "type": "Person",
          "subtype": null,
          "offset": 6818,
          "length": 6,
          "score": 0.96
        },
        {
          "text": "ELMO",
          "type": "Organization",
          "subtype": null,
          "offset": 6952,
          "length": 4,
          "score": 0.77
        },
        {
          "text": "ELMO",
          "type": "Organization",
          "subtype": null,
          "offset": 7389,
          "length": 4,
          "score": 0.65
        },
        {
          "text": "Devlin",
          "type": "Person",
          "subtype": null,
          "offset": 7470,
          "length": 6,
          "score": 0.97
        },
        {
          "text": "Cui",
          "type": "Person",
          "subtype": null,
          "offset": 7596,
          "length": 3,
          "score": 0.96
        },
        {
          "text": "EURASIP",
          "type": "Organization",
          "subtype": null,
          "offset": 7607,
          "length": 7,
          "score": 0.58
        },
        {
          "text": "2021",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 7663,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "2021",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 7669,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 7867,
          "length": 5,
          "score": 0.91
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 7885,
          "length": 5,
          "score": 0.9
        },
        {
          "text": "Wu",
          "type": "Person",
          "subtype": null,
          "offset": 7892,
          "length": 2,
          "score": 0.96
        },
        {
          "text": "Bayes",
          "type": "Person",
          "subtype": null,
          "offset": 8073,
          "length": 5,
          "score": 0.81
        },
        {
          "text": "2017",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 8266,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Wu",
          "type": "Person",
          "subtype": null,
          "offset": 8272,
          "length": 2,
          "score": 0.96
        },
        {
          "text": "Malli",
          "type": "Person",
          "subtype": null,
          "offset": 8547,
          "length": 5,
          "score": 0.95
        },
        {
          "text": "Chen",
          "type": "Person",
          "subtype": null,
          "offset": 8730,
          "length": 4,
          "score": 0.96
        },
        {
          "text": "Ning",
          "type": "Person",
          "subtype": null,
          "offset": 8949,
          "length": 4,
          "score": 0.96
        },
        {
          "text": "Yin",
          "type": "Person",
          "subtype": null,
          "offset": 9163,
          "length": 3,
          "score": 0.97
        },
        {
          "text": "seller",
          "type": "PersonType",
          "subtype": null,
          "offset": 10944,
          "length": 6,
          "score": 0.51
        },
        {
          "text": "Cui",
          "type": "Person",
          "subtype": null,
          "offset": 11160,
          "length": 3,
          "score": 0.97
        },
        {
          "text": "EURASIP",
          "type": "Organization",
          "subtype": null,
          "offset": 11171,
          "length": 7,
          "score": 0.58
        },
        {
          "text": "2021",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 11227,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "2021",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 11233,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "3.1.1.1",
          "type": "IPAddress",
          "subtype": null,
          "offset": 12334,
          "length": 7,
          "score": 0.8
        },
        {
          "text": "3.1.1.2",
          "type": "IPAddress",
          "subtype": null,
          "offset": 12840,
          "length": 7,
          "score": 0.8
        },
        {
          "text": "Binder Mode",
          "type": "Organization",
          "subtype": null,
          "offset": 13353,
          "length": 11,
          "score": 0.59
        },
        {
          "text": "Binder",
          "type": "Person",
          "subtype": null,
          "offset": 13568,
          "length": 6,
          "score": 0.74
        },
        {
          "text": "IPC Backgroud",
          "type": "Organization",
          "subtype": null,
          "offset": 13576,
          "length": 13,
          "score": 0.6
        },
        {
          "text": "Cui",
          "type": "Person",
          "subtype": null,
          "offset": 13682,
          "length": 3,
          "score": 0.96
        },
        {
          "text": "EURASIP",
          "type": "Organization",
          "subtype": null,
          "offset": 13693,
          "length": 7,
          "score": 0.58
        },
        {
          "text": "2021",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 13749,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "2021",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 13755,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "daily",
          "type": "DateTime",
          "subtype": "Set",
          "offset": 13826,
          "length": 5,
          "score": 0.8
        },
        {
          "text": "Sellers",
          "type": "Person",
          "subtype": null,
          "offset": 13841,
          "length": 7,
          "score": 0.51
        },
        {
          "text": "3.1.1.3",
          "type": "IPAddress",
          "subtype": null,
          "offset": 14107,
          "length": 7,
          "score": 0.8
        },
        {
          "text": "Cui",
          "type": "Person",
          "subtype": null,
          "offset": 16234,
          "length": 3,
          "score": 0.97
        },
        {
          "text": "EURASIP",
          "type": "Organization",
          "subtype": null,
          "offset": 16245,
          "length": 7,
          "score": 0.55
        },
        {
          "text": "2021",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 16301,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "2021",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 16307,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "user",
          "type": "PersonType",
          "subtype": null,
          "offset": 18012,
          "length": 4,
          "score": 0.87
        },
        {
          "text": "user",
          "type": "PersonType",
          "subtype": null,
          "offset": 18964,
          "length": 4,
          "score": 0.6
        },
        {
          "text": "JD",
          "type": "Organization",
          "subtype": null,
          "offset": 19140,
          "length": 2,
          "score": 0.52
        },
        {
          "text": "JD.COM",
          "type": "Organization",
          "subtype": null,
          "offset": 19140,
          "length": 6,
          "score": 0.9
        },
        {
          "text": "Cui",
          "type": "Person",
          "subtype": null,
          "offset": 19281,
          "length": 3,
          "score": 0.96
        },
        {
          "text": "EURASIP",
          "type": "Organization",
          "subtype": null,
          "offset": 19292,
          "length": 7,
          "score": 0.64
        },
        {
          "text": "2021",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 19348,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "2021",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 19354,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "sellers",
          "type": "PersonType",
          "subtype": null,
          "offset": 19377,
          "length": 7,
          "score": 0.85
        },
        {
          "text": "Bayes",
          "type": "Person",
          "subtype": null,
          "offset": 21270,
          "length": 5,
          "score": 0.88
        },
        {
          "text": "Jieba",
          "type": "Person",
          "subtype": null,
          "offset": 22182,
          "length": 5,
          "score": 0.83
        },
        {
          "text": "Cui",
          "type": "Person",
          "subtype": null,
          "offset": 23003,
          "length": 3,
          "score": 0.95
        },
        {
          "text": "EURASIP",
          "type": "Organization",
          "subtype": null,
          "offset": 23014,
          "length": 7,
          "score": 0.55
        },
        {
          "text": "2021",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 23070,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "2021",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 23076,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "sellers",
          "type": "PersonType",
          "subtype": null,
          "offset": 23111,
          "length": 7,
          "score": 0.92
        },
        {
          "text": "user",
          "type": "PersonType",
          "subtype": null,
          "offset": 23321,
          "length": 4,
          "score": 0.73
        },
        {
          "text": "Bayes",
          "type": "Person",
          "subtype": null,
          "offset": 23346,
          "length": 5,
          "score": 0.95
        },
        {
          "text": "TFIDF",
          "type": "Organization",
          "subtype": null,
          "offset": 24263,
          "length": 5,
          "score": 0.84
        },
        {
          "text": "TF-IDF",
          "type": "Organization",
          "subtype": null,
          "offset": 25132,
          "length": 6,
          "score": 0.65
        },
        {
          "text": "Cui",
          "type": "Person",
          "subtype": null,
          "offset": 25353,
          "length": 3,
          "score": 0.96
        },
        {
          "text": "EURASIP",
          "type": "Organization",
          "subtype": null,
          "offset": 25364,
          "length": 7,
          "score": 0.65
        },
        {
          "text": "2021",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 25420,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "2021",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 25426,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "BERT",
          "type": "Organization",
          "subtype": null,
          "offset": 26574,
          "length": 4,
          "score": 0.89
        },
        {
          "text": "MLM",
          "type": "Organization",
          "subtype": "Medical",
          "offset": 26763,
          "length": 3,
          "score": 0.68
        },
        {
          "text": "sellers",
          "type": "PersonType",
          "subtype": null,
          "offset": 27227,
          "length": 7,
          "score": 0.94
        },
        {
          "text": "Google",
          "type": "Organization",
          "subtype": null,
          "offset": 27309,
          "length": 6,
          "score": 0.95
        },
        {
          "text": "110M",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 27449,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Google",
          "type": "Organization",
          "subtype": null,
          "offset": 27511,
          "length": 6,
          "score": 0.95
        },
        {
          "text": "4600H",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 28031,
          "length": 5,
          "score": 0.8
        },
        {
          "text": "Cui",
          "type": "Person",
          "subtype": null,
          "offset": 28481,
          "length": 3,
          "score": 0.96
        },
        {
          "text": "EURASIP",
          "type": "Organization",
          "subtype": null,
          "offset": 28492,
          "length": 7,
          "score": 0.61
        },
        {
          "text": "2021",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 28548,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "2021",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 28554,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "sellers",
          "type": "PersonType",
          "subtype": null,
          "offset": 29484,
          "length": 7,
          "score": 0.77
        },
        {
          "text": "sellers",
          "type": "PersonType",
          "subtype": null,
          "offset": 30138,
          "length": 7,
          "score": 0.96
        },
        {
          "text": "Cui",
          "type": "Person",
          "subtype": null,
          "offset": 30904,
          "length": 3,
          "score": 0.94
        },
        {
          "text": "EURASIP",
          "type": "Organization",
          "subtype": null,
          "offset": 30915,
          "length": 7,
          "score": 0.69
        },
        {
          "text": "2021",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 30971,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "2021",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 30977,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "CLSIRATDET",
          "type": "Organization",
          "subtype": null,
          "offset": 31000,
          "length": 10,
          "score": 0.56
        },
        {
          "text": "ECHOIKON Tokcls Tok1",
          "type": "Organization",
          "subtype": null,
          "offset": 31015,
          "length": 20,
          "score": 0.52
        },
        {
          "text": "Sellers",
          "type": "Person",
          "subtype": null,
          "offset": 31781,
          "length": 7,
          "score": 0.57
        },
        {
          "text": "Cui",
          "type": "Person",
          "subtype": null,
          "offset": 33729,
          "length": 3,
          "score": 0.95
        },
        {
          "text": "EURASIP",
          "type": "Organization",
          "subtype": null,
          "offset": 33740,
          "length": 7,
          "score": 0.62
        },
        {
          "text": "2021",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 33796,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "2021",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 33802,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Haoliang Cui",
          "type": "Person",
          "subtype": null,
          "offset": 34046,
          "length": 12,
          "score": 0.99
        },
        {
          "text": "Shuai Shao",
          "type": "Person",
          "subtype": null,
          "offset": 34112,
          "length": 10,
          "score": 0.94
        },
        {
          "text": "authors",
          "type": "PersonType",
          "subtype": null,
          "offset": 34225,
          "length": 7,
          "score": 0.98
        },
        {
          "text": "National Natural Science Foundation of China",
          "type": "Organization",
          "subtype": null,
          "offset": 34282,
          "length": 44,
          "score": 0.93
        },
        {
          "text": "61370195",
          "type": "PhoneNumber",
          "subtype": null,
          "offset": 34341,
          "length": 8,
          "score": 0.8
        },
        {
          "text": "https://github.com/cuihaoliang/User-portraits-of-social-e-commerce",
          "type": "URL",
          "subtype": null,
          "offset": 34397,
          "length": 66,
          "score": 0.8
        },
        {
          "text": "authors",
          "type": "PersonType",
          "subtype": null,
          "offset": 34489,
          "length": 7,
          "score": 0.96
        },
        {
          "text": "1Beijing Key Laboratory of Intelligent Telecommunication Software and Multimedia",
          "type": "Organization",
          "subtype": null,
          "offset": 34560,
          "length": 80,
          "score": 0.78
        },
        {
          "text": "Beijing University of Posts and",
          "type": "Organization",
          "subtype": null,
          "offset": 34642,
          "length": 31,
          "score": 0.78
        },
        {
          "text": "100876,",
          "type": "Address",
          "subtype": null,
          "offset": 34702,
          "length": 7,
          "score": 0.83
        },
        {
          "text": "100876",
          "type": "DateTime",
          "subtype": "Date",
          "offset": 34702,
          "length": 6,
          "score": 0.8
        },
        {
          "text": "Information Technology Security Evaluation",
          "type": "Organization",
          "subtype": null,
          "offset": 34724,
          "length": 42,
          "score": 0.5
        },
        {
          "text": "Beijing 100085",
          "type": "Address",
          "subtype": null,
          "offset": 34775,
          "length": 14,
          "score": 0.78
        },
        {
          "text": "100085",
          "type": "DateTime",
          "subtype": "Date",
          "offset": 34783,
          "length": 6,
          "score": 0.8
        },
        {
          "text": "3Institute of Information Engineering",
          "type": "Organization",
          "subtype": null,
          "offset": 34798,
          "length": 37,
          "score": 0.71
        },
        {
          "text": "Chinese Academy of Sciences",
          "type": "Organization",
          "subtype": null,
          "offset": 34837,
          "length": 27,
          "score": 0.62
        },
        {
          "text": "100088",
          "type": "Address",
          "subtype": null,
          "offset": 34874,
          "length": 6,
          "score": 0.86
        },
        {
          "text": "100088",
          "type": "DateTime",
          "subtype": "Date",
          "offset": 34874,
          "length": 6,
          "score": 0.8
        },
        {
          "text": "16 March 2020",
          "type": "DateTime",
          "subtype": "Date",
          "offset": 34900,
          "length": 13,
          "score": 0.8
        },
        {
          "text": "25 December 2020",
          "type": "DateTime",
          "subtype": "Date",
          "offset": 34924,
          "length": 16,
          "score": 0.8
        },
        {
          "text": "Y. Bengio",
          "type": "Person",
          "subtype": null,
          "offset": 34956,
          "length": 9,
          "score": 0.98
        },
        {
          "text": "R. Ducharme",
          "type": "Person",
          "subtype": null,
          "offset": 34967,
          "length": 11,
          "score": 0.94
        },
        {
          "text": "P. Vincent",
          "type": "Person",
          "subtype": null,
          "offset": 34980,
          "length": 10,
          "score": 0.95
        },
        {
          "text": "J. Mach",
          "type": "Person",
          "subtype": null,
          "offset": 35039,
          "length": 7,
          "score": 0.91
        },
        {
          "text": "2003",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 35075,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Kim Y.",
          "type": "Person",
          "subtype": null,
          "offset": 35084,
          "length": 6,
          "score": 0.93
        },
        {
          "text": "2014",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 35181,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "R. Johnson",
          "type": "Person",
          "subtype": null,
          "offset": 35190,
          "length": 10,
          "score": 0.96
        },
        {
          "text": "T. Zhang",
          "type": "Person",
          "subtype": null,
          "offset": 35202,
          "length": 8,
          "score": 0.96
        },
        {
          "text": "Annual",
          "type": "DateTime",
          "subtype": "Set",
          "offset": 35309,
          "length": 6,
          "score": 0.8
        },
        {
          "text": "Association for Computational Linguistics",
          "type": "Organization",
          "subtype": null,
          "offset": 35331,
          "length": 41,
          "score": 0.53
        },
        {
          "text": "2017",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 35398,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Otter D W",
          "type": "Person",
          "subtype": null,
          "offset": 35420,
          "length": 9,
          "score": 0.96
        },
        {
          "text": "Medina J R",
          "type": "Person",
          "subtype": null,
          "offset": 35431,
          "length": 10,
          "score": 0.94
        },
        {
          "text": "Kalita J K",
          "type": "Person",
          "subtype": null,
          "offset": 35443,
          "length": 10,
          "score": 0.93
        },
        {
          "text": "2018",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 35560,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "R. Jozefowicz",
          "type": "Person",
          "subtype": null,
          "offset": 35569,
          "length": 13,
          "score": 0.98
        },
        {
          "text": "W. Zaremba",
          "type": "Person",
          "subtype": null,
          "offset": 35584,
          "length": 10,
          "score": 0.96
        },
        {
          "text": "I. Sutskever",
          "type": "Person",
          "subtype": null,
          "offset": 35596,
          "length": 12,
          "score": 0.96
        },
        {
          "text": "2015",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 35722,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Liu P",
          "type": "Person",
          "subtype": null,
          "offset": 35746,
          "length": 5,
          "score": 0.96
        },
        {
          "text": "Qiu X",
          "type": "Person",
          "subtype": null,
          "offset": 35753,
          "length": 5,
          "score": 0.96
        },
        {
          "text": "Huang X",
          "type": "Person",
          "subtype": null,
          "offset": 35760,
          "length": 7,
          "score": 0.95
        },
        {
          "text": "1605",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 35864,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "2016",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 35878,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Peters M E",
          "type": "Person",
          "subtype": null,
          "offset": 35887,
          "length": 10,
          "score": 0.94
        },
        {
          "text": "Neumann M",
          "type": "Person",
          "subtype": null,
          "offset": 35899,
          "length": 9,
          "score": 0.92
        },
        {
          "text": "Iyyer M",
          "type": "Person",
          "subtype": null,
          "offset": 35910,
          "length": 7,
          "score": 0.91
        },
        {
          "text": "2018",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 36001,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "A. Vaswani",
          "type": "Person",
          "subtype": null,
          "offset": 36010,
          "length": 10,
          "score": 0.97
        },
        {
          "text": "N. Shazeer",
          "type": "Person",
          "subtype": null,
          "offset": 36022,
          "length": 10,
          "score": 0.93
        },
        {
          "text": "N. Parmar",
          "type": "Person",
          "subtype": null,
          "offset": 36034,
          "length": 9,
          "score": 0.9
        },
        {
          "text": "2017",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 36136,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Devlin J",
          "type": "Person",
          "subtype": null,
          "offset": 36160,
          "length": 8,
          "score": 0.95
        },
        {
          "text": "Chang M W",
          "type": "Person",
          "subtype": null,
          "offset": 36170,
          "length": 9,
          "score": 0.93
        },
        {
          "text": "Lee K",
          "type": "Person",
          "subtype": null,
          "offset": 36181,
          "length": 5,
          "score": 0.93
        },
        {
          "text": "2018",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 36310,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "L. Wu",
          "type": "Person",
          "subtype": null,
          "offset": 36320,
          "length": 5,
          "score": 0.97
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 36371,
          "length": 5,
          "score": 0.71
        },
        {
          "text": "2017",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 36443,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "L. Wu",
          "type": "Person",
          "subtype": null,
          "offset": 36453,
          "length": 5,
          "score": 0.98
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 36493,
          "length": 5,
          "score": 0.77
        },
        {
          "text": "IEEE",
          "type": "Organization",
          "subtype": null,
          "offset": 36560,
          "length": 4,
          "score": 0.82
        },
        {
          "text": "2017",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 36615,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "M. Malli",
          "type": "Person",
          "subtype": null,
          "offset": 36625,
          "length": 8,
          "score": 0.96
        },
        {
          "text": "N. Said",
          "type": "Person",
          "subtype": null,
          "offset": 36635,
          "length": 7,
          "score": 0.95
        },
        {
          "text": "A. Fadlallah",
          "type": "Person",
          "subtype": null,
          "offset": 36644,
          "length": 12,
          "score": 0.9
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 36681,
          "length": 5,
          "score": 0.72
        },
        {
          "text": "2017",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 36764,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "W. Chen",
          "type": "Person",
          "subtype": null,
          "offset": 36774,
          "length": 7,
          "score": 0.97
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 36903,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "M. Ning",
          "type": "Person",
          "subtype": null,
          "offset": 36913,
          "length": 7,
          "score": 0.98
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 37027,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Yin",
          "type": "Person",
          "subtype": null,
          "offset": 37037,
          "length": 3,
          "score": 0.98
        },
        {
          "text": "Libo",
          "type": "Person",
          "subtype": null,
          "offset": 37042,
          "length": 4,
          "score": 0.92
        },
        {
          "text": "Tech Science Press",
          "type": "Organization",
          "subtype": null,
          "offset": 37099,
          "length": 18,
          "score": 0.82
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 37132,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Sun S",
          "type": "Person",
          "subtype": null,
          "offset": 37155,
          "length": 5,
          "score": 0.98
        },
        {
          "text": "Cheng Y",
          "type": "Person",
          "subtype": null,
          "offset": 37162,
          "length": 7,
          "score": 0.98
        },
        {
          "text": "Gan Z",
          "type": "Person",
          "subtype": null,
          "offset": 37171,
          "length": 5,
          "score": 0.96
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 37276,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Yalniz I Z",
          "type": "Person",
          "subtype": null,
          "offset": 37286,
          "length": 10,
          "score": 0.98
        },
        {
          "text": "Jégou H",
          "type": "Person",
          "subtype": null,
          "offset": 37298,
          "length": 7,
          "score": 0.96
        },
        {
          "text": "Chen K",
          "type": "Person",
          "subtype": null,
          "offset": 37307,
          "length": 6,
          "score": 0.96
        },
        {
          "text": "1905",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 37408,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 37422,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Yaqiong Qiao",
          "type": "Person",
          "subtype": null,
          "offset": 37432,
          "length": 12,
          "score": 0.99
        },
        {
          "text": "Xiangyang Luo",
          "type": "Person",
          "subtype": null,
          "offset": 37446,
          "length": 13,
          "score": 0.99
        },
        {
          "text": "Chenliang Li",
          "type": "Person",
          "subtype": null,
          "offset": 37461,
          "length": 12,
          "score": 0.99
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 37542,
          "length": 5,
          "score": 0.58
        },
        {
          "text": "2020",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 37614,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "102151",
          "type": "DateTime",
          "subtype": "Date",
          "offset": 37624,
          "length": 6,
          "score": 0.8
        },
        {
          "text": "102151",
          "type": "DateTime",
          "subtype": "Date",
          "offset": 37633,
          "length": 6,
          "score": 0.8
        },
        {
          "text": "Jinwei Wang",
          "type": "Person",
          "subtype": null,
          "offset": 37647,
          "length": 11,
          "score": 0.99
        },
        {
          "text": "Hao Wang",
          "type": "Person",
          "subtype": null,
          "offset": 37660,
          "length": 8,
          "score": 0.99
        },
        {
          "text": "Jian Li",
          "type": "Person",
          "subtype": null,
          "offset": 37670,
          "length": 7,
          "score": 0.99
        },
        {
          "text": "Xiangyang Luo",
          "type": "Person",
          "subtype": null,
          "offset": 37679,
          "length": 13,
          "score": 0.99
        },
        {
          "text": "Yun-Qing Shi",
          "type": "Person",
          "subtype": null,
          "offset": 37694,
          "length": 12,
          "score": 0.99
        },
        {
          "text": "Sunil Kr. Jha",
          "type": "Person",
          "subtype": null,
          "offset": 37708,
          "length": 13,
          "score": 0.95
        },
        {
          "text": "IEEE",
          "type": "Organization",
          "subtype": null,
          "offset": 37830,
          "length": 4,
          "score": 0.89
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 37870,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "2922309",
          "type": "PhoneNumber",
          "subtype": null,
          "offset": 37876,
          "length": 7,
          "score": 0.8
        },
        {
          "text": "Springer Nature",
          "type": "Organization",
          "subtype": null,
          "offset": 37903,
          "length": 15,
          "score": 0.94
        },
        {
          "text": "Cui",
          "type": "Person",
          "subtype": null,
          "offset": 38023,
          "length": 3,
          "score": 0.96
        },
        {
          "text": "EURASIP",
          "type": "Organization",
          "subtype": null,
          "offset": 38034,
          "length": 7,
          "score": 0.54
        },
        {
          "text": "2021",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 38090,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "2021",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 38096,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "14 January 2021",
          "type": "DateTime",
          "subtype": "Date",
          "offset": 38137,
          "length": 15,
          "score": 0.8
        },
        {
          "text": "https://github.com/cuihaoliang/User-portraits-of-social-e-commerce",
          "type": "URL",
          "subtype": null,
          "offset": 38155,
          "length": 66,
          "score": 0.8
        }
      ]
    },
    {
      "@search.score": 1.6963805,
      "content": "\nBig data stream analysis: a systematic \nliterature review\nTaiwo Kolajo1,2* , Olawande Daramola3  and Ayodele Adebiyi1,4 \n\nIntroduction\nAdvances in information technology have facilitated large volume, high-velocity of data, \nand the ability to store data continuously leading to several computational challenges. \nDue to the nature of big data in terms of volume, velocity, variety, variability, veracity, \nvolatility, and value [1] that are being generated recently, big data computing is a new \ntrend for future computing.\n\nBig data computing can be generally categorized into two types based on the process-\ning requirements, which are big data batch computing and big data stream computing \n\nAbstract \n\nRecently, big data streams have become ubiquitous due to the fact that a number of \napplications generate a huge amount of data at a great velocity. This made it difficult \nfor existing data mining tools, technologies, methods, and techniques to be applied \ndirectly on big data streams due to the inherent dynamic characteristics of big data. In \nthis paper, a systematic review of big data streams analysis which employed a rigorous \nand methodical approach to look at the trends of big data stream tools and technolo-\ngies as well as methods and techniques employed in analysing big data streams. It \nprovides a global view of big data stream tools and technologies and its comparisons. \nThree major databases, Scopus, ScienceDirect and EBSCO, which indexes journals and \nconferences that are promoted by entities such as IEEE, ACM, SpringerLink, and Elsevier \nwere explored as data sources. Out of the initial 2295 papers that resulted from the \nfirst search string, 47 papers were found to be relevant to our research questions after \nimplementing the inclusion and exclusion criteria. The study found that scalability, \nprivacy and load balancing issues as well as empirical analysis of big data streams and \ntechnologies are still open for further research efforts. We also found that although, sig-\nnificant research efforts have been directed to real-time analysis of big data stream not \nmuch attention has been given to the preprocessing stage of big data streams. Only a \nfew big data streaming tools and technologies can do all of the batch, streaming, and \niterative jobs; there seems to be no big data tool and technology that offers all the key \nfeatures required for now and standard benchmark dataset for big data streaming ana-\nlytics has not been widely adopted. In conclusion, it was recommended that research \nefforts should be geared towards developing scalable frameworks and algorithms that \nwill accommodate data stream computing mode, effective resource allocation strategy \nand parallelization issues to cope with the ever-growing size and complexity of data.\n\nKeywords: Big data stream analysis, Stream computing, Big data streaming tools and \ntechnologies\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nSURVEY PAPER\n\nKolajo et al. J Big Data            (2019) 6:47  \nhttps://doi.org/10.1186/s40537-019-0210-7\n\n*Correspondence:   \ntaiwo.kolajo@stu.cu.edu.ng; \ntaiwo.kolajo@fulokoja.edu.ng \n1 Department of Computer \nand Information Sciences, \nCovenant University, Ota, \nNigeria\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0001-6780-2495\nhttp://orcid.org/0000-0001-6340-078X\nhttp://orcid.org/0000-0002-3114-6315\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-019-0210-7&domain=pdf\n\n\nPage 2 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\n[2]. Big data batch processing is not sufficient when it comes to analysing real-time \napplication scenarios. Most of the data generated in a real-time data stream need real-\ntime data analysis. In addition, the output must be generated with low-latency and any \nincoming data must be reflected in the newly generated output within seconds. This \nnecessitates big data stream analysis [3].\n\nThe demand for stream processing is increasing. The reason being not only that huge \nvolume of data need to be processed but that data must be speedily processed so that \norganisations or businesses can react to changing conditions in real-time.\n\nThis paper presents a systematic review of big data stream analysis. The purpose is to \npresent an overview of research works, findings, as well as implications for research and \npractice. This is necessary to (1) provide an update about the state of research, (2) iden-\ntify areas that are well researched, (3) showcase areas that are lacking and need further \nresearch, and (4) build a common understanding of the challenges that exist for the ben-\nefit of the scientific community.\n\nThe rest of the paper is organized as follows: “Background and related work” section \nprovides information on stream computing and big data stream analysis and the key \nissues involved in it and presents a review on big data streaming analytics. In “Research \nmethod” section, the adopted research methodology is discussed, while “Result” section \npresents the findings of the study. “Discussion” section presents a detailed evaluation \nperformed on big data stream analysis, “Limitation of the review” section highlights the \nlimitations of the study, while “Conclusion and further work” concludes the paper.\n\nBackground and related work\nStream computing\n\nStream computing refers to the processing of massive amount of data generated at high-\nvelocity from multiple sources with low latency in real-time. It is a new paradigm neces-\nsitated because of new sources of data generating scenarios which include ubiquity of \nlocation services, mobile devices, and sensor pervasiveness [4]. It can be applied to the \nhigh-velocity flow of data from real-time sources such as the Internet of Things, Sensors, \nmarket data, mobile, and clickstream.\n\nThe fundamental assumption of this paradigm is that the potential value of data lies in \nits freshness. As a result, data are analysed as soon as they arrive in a stream to produce \nresult as opposed to what obtains in batch computing where data are first stored before \nthey are analysed. There is a crucial need for parallel architectures and scalable com-\nputing platforms [5]. With stream computing, organisations can analyse and respond in \nreal-time to rapidly changing data. Streaming processing frameworks include Storm, S4, \nKafka, and Spark [6–8]. The real contrasts between the batch processing and the stream \nprocessing paradigms are outlined in Table 1.\n\nIncorporating streaming data into decision-making process necessitates a program-\nming paradigm called stream computing. With stream computing, fairly static questions \ncan be evaluated on data in motion (i.e. real-time data) continuously [9].\n\nBig data stream analysis\n\nThe essence of big data streaming analytics is the need to analyse and respond to real-\ntime streaming data using continuous queries so that it is possible to continuously \n\n\n\nPage 3 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nperform analysis on the fly within the stream. Stream processing solutions must be \nable to handle a real-time, high volume of data from diverse sources putting into con-\nsideration availability, scalability and fault tolerance. Big data stream analysis involves \nassimilation of data as an infinite tuple, analysis and production of actionable results \nusually in a form of stream [10].\n\nIn a stream processor, applications are represented as data flow graph made up of \noperations and interconnected streams as depicted in Fig. 1. In a streaming analytics \nsystem, application comes in a form of continuous queries, data are ingested continu-\nously, analysed and correlated, and stream of results are generated. Streaming analytic \napplications is usually a set of operators connected by streams. Streaming analytics \nsystems must be able to identify new information, incrementally build models and \naccess whether the new incoming data deviate from model predictions [9].\n\nThe idea of streaming analytics is that each of the received data tuples is processed \nin the data processing node. Such processing includes removing duplicates, filling \nmissing data, data normalization, parsing, feature extraction, which are typically done \nin a single pass due to the high data rates of external feeds. When a new tuple arrives, \nthis node is triggered, and it expels tuples older than the time specified in the sliding \nwindow (sliding window is a typical example of windows used in stream computing \nwhich keeps only the latest tuples up to the time specified in the windows). A window \n\nTable 1 Comparison between batch processing and streaming processing [82]\n\nDimension Batch processing Streaming processing\n\nInput Data chunks Stream of new data or updates\n\nData size Known and finite Infinite or unknown in advance\n\nHardware Multiple CPUs Typical single limited amount of memory\n\nStorage Store Not store or store non-trivial portion in memory\n\nProcessing Processed in multiple rounds A single or few passes over data\n\nTime Much longer A few seconds or even milliseconds\n\nApplications Widely adopted in almost every domain Web mining, traffic monitoring, sensor networks\n\nFig. 1 Data flow graph of a stream processor. The figure shows how applications (made up of operations and \ninterconnected streams) are represented as data flow graph in a stream processor [10]\n\n\n\n\n\nPage 4 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nis referred to as a logical container for data tuples received. It defines how frequently \ndata is refreshed in the container as well as when data processing is triggered [4].\n\nKey issues in big data stream analysis\n\nBig data stream analysis is relevant when there is a need to obtain useful knowledge \nfrom current happenings in an efficient and speedy manner in order to enable organisa-\ntions to quickly react to problems, or detect new trends which can help improve their \nperformance. However, there are some challenges such as scalability, integration, fault-\ntolerance, timeliness, consistency, heterogeneity and incompleteness, load balancing, \nprivacy issues, and accuracy [3, 11–18] which arises from the nature of big data streams \nthat must be dealt with.\n\nScalability\n\nOne of the main challenges in big data streaming analysis is the issue of scalability. The \nbig data stream is experiencing exponential growth in a way much faster than computer \nresources. The processors follow Moore’s law, but the size of data is exploding. There-\nfore, research efforts should be geared towards developing scalable frameworks and \nalgorithms that will accommodate data stream computing mode, effective resource allo-\ncation strategy and parallelization issues to cope with the ever-growing size and com-\nplexity of data.\n\nIntegration\n\nBuilding a distributed system where each node has a view of the data flow, that is, every \nnode performing analysis with a small number of sources, then aggregating these views \nto build a global view is non-trivial. An integration technique should be designed to ena-\nble efficient operations across different datasets.\n\nFault‑tolerance\n\nHigh fault-tolerance is required in life-critical systems. As data is real-time and infinite \nin big data stream computing environments, a good scalable high fault-tolerance strat-\negy is required that allows an application to continue working despite component failure \nwithout interruption.\n\nTimeliness\n\nTime is of the essence for time-sensitive processes such as mitigating security threats, \nthwarting fraud, or responding to a natural disaster. There is a need for scalable architec-\ntures or platforms that will enable continuous processing of data streams which can be \nused to maximize the timeliness of data. The main challenge is implementing a distrib-\nuted architecture that will aggregate local views of data into global view with minimal \nlatency between communicating nodes.\n\nConsistency\n\nAchieving high consistency (i.e. stability) in big data stream computing environments is \nnon-trivial as it is difficult to determine which data are needed and which nodes should \nbe consistent. Hence a good system structure is required.\n\n\n\nPage 5 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nHeterogeneity and incompleteness\n\nBig data streams are heterogeneous in structure, organisations, semantics, accessi-\nbility and granularity. The challenge here is how to handle an always ever-increas-\ning data, extract meaningful content out of it, aggregate and correlate streaming \ndata from multiple sources in real-time. A competent data presentation should be \ndesigned to reflect the structure, diversity and hierarchy of the streaming data.\n\nLoad balancing\n\nA big data stream computing system is expected to be self-adaptive to data streams \nchanges and avoid load shedding. This is challenging as dedicating resources to cover \npeak loads 24/7 is impossible and load shedding is not feasible when the variance \nbetween the average load and the peak load is high. As a result, a distributing envi-\nronment that automatically streams partial data streams to a global centre when local \nresources become insufficient is required.\n\nHigh throughput\n\nDecision with respect to identifying the sub-graph that needs replication, how many \nreplicas are needed and the portion of the data stream to assign to each replica is an \nissue in big data stream computing environment. There is a need for good multiple \ninstances replication if high throughput is to be achieved.\n\nPrivacy\n\nBig data stream analytics created opportunities for analyzing a huge amount of data \nin real-time but also created a big threat to individual privacy. According to the Inter-\nnational Data Cooperation (IDC), not more than half of the entire information that \nneeds protection is effectively protected. The main challenge is proposing techniques \nfor protecting a big data stream dataset before its analysis.\n\nAccuracy\n\nOne of the main objectives of big data stream analysis is to develop effective tech-\nniques that can accurately predict future observations. However, as a result of inher-\nent characteristics of big data such as volume, velocity, variety, variability, veracity, \nvolatility, and value, big data analysis strongly constrain processing algorithms spatio-\ntemporally and hence stream-specific requirements must be taken into consideration \nto ensure high accuracy.\n\nRelated work\n\nThis section discusses some of the previous research efforts that relate to big data \nstreaming analytics.\n\nThe work of [13] presented a review of various tools, technologies and methods \nfor big data analytics by categorizing big data analytics literature according to their \nresearch focus. This paper is different in that it presents a systematic literature review \nthat focused on big data “streaming” analytics.\n\n\n\nPage 6 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nAuthors in [19] presented a systematic review of big data analytics in e-commerce. The \nstudy explored characteristics, definitions, business values, types and challenges of big \ndata analytics in the e-commerce landscape. Likewise, [20] conducted a study that is cen-\ntred on big data analytics in technology and organisational resource management specifi-\ncally focusing on reviews that present big data challenges and big data analytics methods. \nAlthough they are systematic reviews, the focus is not, particularly on big data streaming.\n\nAuthors in [21] presented the status of empirical research and application areas in big \ndata by employing a systematic mapping method. In the same vein, authors in [22] also \nconducted a survey on big data technologies and machine learning algorithms with a \nparticular focus on anomaly detection. A systematic review of literature which aims to \ndetermine the scope, application, and challenges of big data analytics in healthcare was \npresented by [23]. The work of [2] presented a review of four big data streaming tools \nand technologies. While the study conducted in this paper provided a comprehensive \nreview of not only big data streaming tools and technologies but also methods and tech-\nniques employed in analyzing big data streams. In addition, authors [2] did not provide a \nclear explanation of the methodical approach for selecting the reviewed papers.\n\nResearch method\nThe study was grounded in a systematic literature review of tools and technologies \nwith methods and techniques used in analysing big data streams by adopting [24, 25] as \nmodels.\n\nResearch question\n\nThe study tries to answer the following research questions:\n\nResearch Question 1: What are the tools and technologies employed for big data \nstream analysis?\nResearch Question 2: What methods and techniques are used in analysing big data \nstreams?\nResearch Question 3: What do these tools and technologies have in common and \ntheir differences in terms of concept, purpose and capabilities?\nResearch Question 4: What are the limitations and strengths of these tools and tech-\nnologies?\nResearch Question 5: What are the evaluation techniques or benchmarks used for \nevaluating big data streaming tools and technology?\n\nSearch string\n\nCreating a good search string requires structuring in terms of population, compari-\nson, intervention and outcome [24]. Relevant publications were identified by forming \na search string that combined keywords driven by the research questions earlier stated. \nThe searches were conducted by employing three standard database indexes, which are \nScopus, Science Direct and EBSCOhost. The search string is “big data stream analysis” \nOR “big data stream technologies” OR “big data stream framework” OR “big data stream \nalgorithms” OR “big data stream analysis tools” OR “big data stream processing” OR “big \n\n\n\nPage 7 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\ndata stream analysis reviews” OR “big data stream literature review” OR “big data stream \nanalytics”.\n\nData sources\n\nAs research becomes increasingly interdisciplinary, global and collaborative, it is expedi-\nent to select from rich and standard databases. The databases consulted are as follows:\n\n i. Scopus1: Scopus is a bibliographic database containing abstracts and citations for \nacademic journal articles launched in 2004. It covers nearly 36,377 titles from over \n11,678 publishers of which 34,346 are peer-reviewed journals, delivering a compre-\nhensive overview of the world’s research output in the scientific, technical, medi-\ncal, and social sciences (including arts and humanities). It is the largest abstract \nand citation database of peer-reviewed literature.\n\n ii. ScienceDirect2: ScienceDirect is Elsevier’s leading information solution for \nresearchers, students, teachers, information professionals and healthcare profes-\nsionals. It provides both subscription-based and open access-based to a large data-\nbase combining authoritative, full-text scientific, technical and health publications \nwith smart intuitive functionality. It covers over 14 million publications from over \n3800 journals and more than 35,000 books. The journals are grouped into four \ncategories: Life Sciences, Physical Sciences and Engineering, Health Sciences, and \nSocial Sciences and Humanities.\n\n iii. EBSCOhost3: EBSCOhost covers a wide range of bibliographic and full-text data-\nbases for researchers, providing electronic journal service available to both cor-\nporate and academic researchers. It has a total of 16,711 journals and magazine \nindexed and abstracted of which 14,914 are peer-reviewed; more than 900,000 \nhigh-quality e-books and titles and over 60,000 audiobooks from more than 1500 \nmajor academic publishers.\n\n iv. ResearchGate4: A free online professional network for scientists and researchers to \nask and answer questions, share papers and find collaborators. It covers over 100 \nmillion publications from over 11 million researchers. ResearchGate was used as \na secondary source where the authors could not access some papers due to lack of \nsubscription.\n\nData retrieval\n\nThe search was conducted in Scopus, ScienceDirect and EBSCOhost since most of \nthe high impact journals and conferences are indexed in these set of rich databases. \nBoolean ‘OR’ was used in combining the nine (9) search strings. A total of 2295 arti-\ncles from the three databases were retrieved as shown in Table 2.\n\n1 http://www.scopu s.com.\n2 http://www.scien cedir ect.com.\n3 https ://www.ebsco host.com.\n4 https ://www.resea archg ate.net.\n\nhttp://www.scopus.com\nhttp://www.sciencedirect.com\nhttps://www.ebscohost.com\nhttps://www.reseaarchgate.net\n\n\nPage 8 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nFurther refinement was performed by (i) limiting the search to journals and confer-\nence papers; (ii) selecting computer science and IT related as the subject domain; (iii) \nselecting ACM, IEEE, SpringerLink, Elsevier as sources; and year of publication to \nbetween 2004 and 2018. The year range was selected due to the fact that interest in \nbig data stream analysis actually started in 2004. At this stage, a total of 1989 papers \nwere excluded leaving a total of 315 papers (see Table  3). The result of the search \nstring was exported to PDF.\n\nBy going through the title of the papers, 111 seemingly relevant papers were extracted \nexcluding a total number of 213 that were not relevant at this stage (see Table 4).\n\nThe abstracts of 111 papers and introduction (for papers that the abstracts were not \nclear enough) were then read to have a quick overview of the paper and to ascertain \nwhether they are suitable or at variance with the research questions. The citations of \nthe papers were exported to Microsoft Excel for easy analysis. The papers were grouped \ninto three categories; “relevant”, “may be relevant” and “irrelevant”. The “relevant” papers \nwere marked with black colour, “may be relevant” and “irrelevant” with green and red \ncolours respectively. At the end of this stage, 45 papers were classified as “relevant”, 9 \npapers as “may be relevant” and 11 as “irrelevant”. Looking critically at the abstract again, \n18 papers were excluded by using the exclusion criteria leaving a total of 47 papers (see \nTable 5) which were manually reviewed in line with the research questions.\n\nInclusion criteria\n\nPapers published in journals, peer-reviewed conferences, workshops, technical and \nsymposium from 2004 and 2018 were included. In addition, the most recent papers \nwere selected in case of papers with similar investigations and results.\n\nTable 2 First search string result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 2097 65 133 2295\n\nTable 3 Second search string result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 196 27 92 315\n\nTable 4 Third Search string refinement result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 64 23 24 111\n\nTable 5 Final Selection\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 25 10 12 47\n\n\n\nPage 9 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nExclusion criteria\n\nPapers that belong to the following categories were excluded from selection as part of \nthe primary study: (i) papers written in source language other than English; (ii) papers \nwith an abstract and or introduction that does not clearly define the contributions of the \nwork; (iii) papers whose abstract do not relate to big data stream analysis.\n\nResult\nThe findings of the study are now presented with respect to the research questions that \nguided the execution of the systematic literature review.\n\nResearch Question 1: What are the tools and technologies employed for big data stream \n\nanalysis?\n\nBig data stream platforms provide functionalities and features that enable big data \nstream applications to develop, operate, deploy, and manage big data streams. Such \nplatforms must be able to pull in streams of data, process the data and stream it back \nas a single flow. Several tools and technologies have been employed to analyse big data \nstreams. In response to the growing demand for big data streaming analytics, a large \nnumber of alternative big data streaming solutions have been developed both by the \nopen source community and enterprise technology vendors. According to [26], there are \nsome factors to consider when selecting big data streaming tools and technologies in \norder to make effective data management decisions. These are briefly described below.\n\nShape of the data\n\nStreaming data sources require serialization technologies for capturing, storing and rep-\nresenting such high-velocity data. For instance, some tools and technologies allow pro-\njection of different structures across data stores, giving room for flexibility for storage \nand access of data in different ways. However, the performance of such platforms may \nnot be suitable for high-velocity data.\n\nData access\n\nThere is a need to put into consideration how the data will be accessed by users and \napplications. For instance, many NoSQL databases require specific application interfaces \nfor data access. Hence there is a need to consider the integration of some other neces-\nsary tools for data access.\n\nAvailability and consistency requirement\n\nIf a distributed system is needed, then CAP theorem states that consistency and avail-\nability cannot be both guaranteed in the presence of network partition (i.e. when there is \na break in the network). In such a scenario, consistency is often traded off for availability \nto ensure that requests can always be processed.\n\nWorkload profile required\n\nPlatform as a service deployment may be appropriate for a spike load profile platform. \nIf platform distribution can be deployed on Infrastructure as a service cloud, then this \noption may be preferred as users will need to pay only when processing. On-premise \n\n\n\nPage 10 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\ndeployment may be considered for predictable or consistent loads. But if workloads are \nmixed (i.e. consistent flows or spikes), a combination of cloud and on-premise approach \nmay be considered so as to give room for easy integration of web-based services or soft-\nware and access to critical functions on the go.\n\nLatency requirement\n\nIf a minimal delay or low latency is required, key-value stores may be considered or bet-\nter still, an in-memory solution which allows the process of large datasets in real-time is \nrequired in order to optimize the data loading procedure.\n\nThe tools and technologies for big data stream analysis can be broadly categorized into \ntwo, which are open source and proprietary solutions. These are listed in Tables 6 and 7.\n\nThe selection of big data streaming tools and technologies should be based on the impor-\ntance of each factor earlier mentioned in this section. Proprietary solutions may not be eas-\nily available because of pricing and licensing issues. While open source supports innovation \nand development at a large scale, careful selection must be made especially when choosing \na recent technology still in production due to limited maturity and lack of support from \nacademic researchers or developer communities. In addition, open source solutions may \nlead to outdating and modification challenges [27]. Moreover, the selection of whether pro-\nprietary or open source or combination of both should depend on the problem to address, \nthe understanding of the true costs, and benefits of both open and proprietary solutions.\n\nTable 6 Open source tools and technologies for big data stream analysis\n\nTools and technology Article\n\nBlockMon [83]\n\nNoSQL [4, 84–86]\n\nSpark streaming [67, 87–91]\n\nApache storm [68, 85, 86, 92–97]\n\nKafka [85, 91, 95, 96, 98]\n\nYahoo! S4 [6, 45, 87, 99]\n\nApache Samza [46, 67, 100]\n\nPhoton [67, 101]\n\nApache Aurora [67, 102]\n\nMavEStream [103]\n\nEsperTech [104, 105]\n\nRedis [106]\n\nC-SPARQL [107, 108]\n\nSAMOA [56, 78, 109]\n\nCQELS [108, 110, 111]\n\nETALIS [112]\n\nXSEQ [73]\n\nApache Kylin [113]\n\nSplunk stream [114]\n\n\n\nPage 11 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nResearch Question 2: What methods and techniques are used in analysing big data \n\nstreams?\n\nGiven the real-time nature, velocity and volume of social media streams, the clus-\ntering algorithms that are applied on streaming data must be highly scalable and \nefficient. Also, the dynamic nature of data makes it difficult to know the required or \ndesirable number of clusters in advance. This renders partitioning clustering tech-\nniques (such as k-median, k-means and k-medoid) or expectation-maximization \n(EM) algorithms-based approaches unsuitable for analysing real-time social media \ndata because they require prior knowledge of clusters in advance. In addition, due \nto concept drift inherent in social media streams, scalable graph partitioning algo-\nrithms are not also suitable because of their tendency towards balanced partitioning. \nSocial media streams must be analysed dynamically in order to provide decisions at \nany given time within a limited space and time window [28–30].\n\nDensity-based clustering algorithm (such as DenStream, OpticStream, Flock-\nStream, Exclusive and Complete Clustering) unlike partitioning algorithms does not \nrequire apriori number of clusters in advance and can detect outliers [31]. However, \nthe issue with density-based clustering algorithms is that most of them except for few \nlike HDDStream, PreDeCon-Stream and PKS-Stream (which are memory intensive) \nperform less efficiently in the face of high dimensional data and as a result are not \nsuitable for analyzing social media streams [32].\n\nThreshold-based techniques, hierarchical clustering, and incremental clustering \nor online clustering are more relevant to social media analysis. Several online thresh-\nold-based stream clustering approaches or incremental clustering approaches such as \nMarkov Random Field [33, 34], Online Spherical K-means [35], and Condensed Clusters \n[36] have been adopted. Incremental approaches are suitable for continuously generated \ndata grouping by setting a maximum similarity threshold between the incoming stream \n\nTable 7 Proprietary tools and technologies for big data stream analysis\n\nTools and technology Article\n\nCodeBlue [115]\n\nAnodot [116]\n\nCloudet [117]\n\nSentiment brand monitoring [118]\n\nNumenta [119]\n\nElastic streaming processing engine [120]\n\nMicrosoft azure stream analytics [121]\n\nIBM InfoSphere streams [8, 122]\n\nGoogle MillWheel [123]\n\nArtemis [124]\n\nWSO2 analytics [125]\n\nMicrosoft StreamInsight [126]\n\nTIBCO StreamBase [127]\n\nStriim [128]\n\nKyvos insights [129]\n\nAtScale [130, 131]\n\nLambda architecture [57]\n\n\n\nPage 12 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nand the existing clusters. Much work has been done in improving the efficiency of online \nclustering algorithms, however, little research efforts have been directed to threshold \nand fragmentation issues. Incremental algorithm threshold setting should employ adap-\ntive approach instead of relying on static values [37, 38]. Some of the methods and tech-\nniques that have been employed in analysing big data streams are outlined in Table 8.\n\nTable 8 Methods and techniques for big data stream analysis\n\nMethods and techniques Article\n\nSPADE [132]\n\nLocally supervised metric learning (LSML) [133]\n\nKTS [106]\n\nMultinomial latent dirichlet allocation [106]\n\nVoltage clustering algorithm [106]\n\nLocality sensitive hashing (LSH) [134]\n\nUser profile vector update algorithm [134]\n\nTag assignment stream clustering (TASC) [134]\n\nStreamMap [117]\n\nDensity cognition [117]\n\nQRS detection algorithm [87]\n\nForward chaining rule [110]\n\nStream [135]\n\nCluStream [136, 137]\n\nHPClustering [138]\n\nDenStream [139]\n\nD-Stream [140]\n\nACluStream [141]\n\nDCStream [142]\n\nP-Stream [143]\n\nADStream [144]\n\nContinuous query processing (CQR) [145]\n\nFPSPAN-growth [146]\n\nOutlier method for cloud computing algorithm (OMCA) [147]\n\nMulti-query optimization strategy (MQOS) [148]\n\nParallel K-means clustering [72]\n\nVisibly push down automata (VPA) [73]\n\nIncremental MI outlier detection algorithm (Inc I-MLOF) [149]\n\nAdaptive windowing based online ensemble (AWOE) [74]\n\nDynamic prime-number based security verification [84]\n\nK-anonymity, I-diversity, t-closeness [90]\n\nSingular spectrum matrix completion (SS-MC) [76]\n\nTemporal fuzzy concept analysis [96]\n\nECM-sketch [77]\n\nNearest neighbour [91]\n\nMarkov chains [91]\n\nBlock-QuickSort-AdjacentJobMatch [86]\n\nBlock-QuickSort-OverlapReplicate [86]\n\nFuzzy-CSar-AFP [150]\n\nWeighted online sequential extreme learning machine with kernels (WOS-ELMK) [22]\n\nConcept-adapting very fast decision tree (CVFDT) [151]\n\n\n\nPage 13 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nMany researchers have looked at the aspect of the real-time analysis of big data \nstreams but not much attention has been directed towards social media stream pre-\nprocessing. For instance, the social media stream is characterized by incomplete, noisy, \nslang, abbreviated words. Also, contextual meaning of social media post is essential for \nimproved event detection, sentiment analysis or any other social media analytics algo-\nrithms in terms of quality and accuracy [36, 39]. There is the need to give more atten-\ntion to the preprocessing stage of social media stream analysis in the face of incomplete, \nnoisy, slang, and abbreviated words that are pertinent to social media streams. These \nchallenges create opportunities application of new semantic technology approaches, \nwhich are more suited to social media streams [40, 41].\n\nResearch Question 3: What do big data streaming tools and technologies have in common \n\nand their differences in terms of concept, purpose, and capabilities?\n\nThe features of various tools and technologies for big data stream were compared in \norder to answer this question. An overview analysis based on 10 dimensions, which are \ndatabase support, execution model, workload, fault-tolerance, latency, throughput, reli-\nability, operating system, implementation languages and application domain or areas is \npresented in Table 9.\n\nFor organisations with existing applications that have support for SQL, MySQL, SQL \nServer, Oracle Database, for instance, may consider choosing big data streaming tools \nand technologies that have support for their existing databases. There are few big data \nstreaming tools and technology that support virtually any data format. An example of \nsuch is Infochimps Cloud.\n\nThe major big data streaming tools and technologies considered are all suitable for \nstreaming execution model, however out of 19 big data tools and technology compared \nand contrasted in this section, only 10.5% is suitable for streaming, batch, and iterative \nprocessing while 47.4% can handle jobs requiring both batch and streaming processing. \nIt is safer for a job to be executed on a single platform which can accommodate all the \ndependencies required in order to avoid interoperability constraints than combining \ntwo or more platforms or frameworks. The best fit with respect to the choice of big data \nstreaming tools and technologies will depend on the state of data to process, infrastruc-\nture preference, business use case, and kind of results interested in.\n\nVirtually all the big data streaming tools and technologies are memory intensive. This \nimplies that the main performance bottleneck at higher load conditions will be due to \nlack of memory [42]. However, research has shown that the benefit of high intensive \nmemory applications outweighs the performance loss due to long memory latency [43].\n\nFrom all the big data streaming tools and technologies reviewed, only IBMInfoS-\nphere and TIBCO StreamBase support all of the three “at-most-once” “at-least-once” \nand “exactly-once” message delivery mechanisms while others support one or two of the \nthree delivery mechanisms. “At-most-once” is the cheapest with least implementation \noverhead and highest performance because it can be done in a fire-and-forget fashion \nwithout keeping the state in the transport mechanism or at the sending end. “At-least-\nonce” delivery requires multiple attempts in order to counter transport losses which \nmeans keeping the state at the sending end and having an acknowledgement mechanism \nat the receiving end. “Exactly-once” is the most expensive and has consequently worst \n\n\n\nPage 14 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nTa\nbl\n\ne \n9 \n\nCo\nm\n\npa\nri\n\nso\nn \n\nof\n b\n\nig\n d\n\nat\na \n\nst\nre\n\nam\nin\n\ng \nto\n\nol\ns \n\nan\nd \n\nte\nch\n\nno\nlo\n\ngi\nes\n\nTo\nol\n\ns \nan\n\nd \nte\n\nch\nno\n\nlo\ngy\n\nD\nat\n\nab\nas\n\ne \nsu\n\npp\nor\n\nt\nEx\n\nec\nut\n\nio\nn \n\nm\nod\n\nel\nW\n\nor\nkl\n\noa\nd\n\nFa\nul\n\nt t\nol\n\ner\nan\n\nce\nLa\n\nte\nnc\n\ny\nTh\n\nro\nug\n\nhp\nut\n\nRe\nlia\n\nbi\nlit\n\ny\nO\n\npe\nra\n\ntin\ng \n\nsy\nst\n\nem\nIm\n\npl\nem\n\nen\nta\n\ntio\nn/\n\nsu\npp\n\nor\nte\n\nd \nla\n\nng\nua\n\nge\ns\n\nA\npp\n\nlic\nat\n\nio\nn\n\nBl\noc\n\nkM\non\n\nCa\nss\n\nan\ndr\n\na,\n M\n\non\n-\n\ngo\nD\n\nB,\n X\n\nM\nL\n\nSt\nre\n\nam\nin\n\ng\nM\n\nul\nti-\n\nsl\nic\n\ne \nm\n\nem\n-\n\nor\ny \n\nal\nlo\n\nca\ntio\n\nn \nan\n\nd \nba\n\ntc\nh \n\nal\nlo\n\nca\ntio\n\nns\n\nC\nhe\n\nck\npo\n\nin\nt, \n\nro\nllb\n\nac\nk\n\nVe\nry\n\n lo\nw\n\nH\nig\n\nh\nA\n\nt l\nea\n\nst\n o\n\nnc\ne\n\nLi\nnu\n\nx\nC\n\n +\n+\n\n11\n, P\n\nyt\nho\n\nn\nA\n\nno\nm\n\nal\ny \n\nde\nte\n\nct\nio\n\nn,\n \n\nne\ntw\n\nor\nk \n\nop\ntim\n\niz\na-\n\ntio\nn,\n\n m\nul\n\ntim\ned\n\nia\n \n\nco\nnt\n\nen\nt d\n\nel\niv\n\ner\ny,\n\n \nfin\n\nan\nci\n\nal\n m\n\nar\nke\n\nt \nan\n\nal\nys\n\nis\n, w\n\neb\n \n\nan\nal\n\nyt\nic\n\ns\n\nSp\nar\n\nk \nSt\n\nre\nam\n\nin\ng\n\nKa\nfk\n\na,\n H\n\nBa\nse\n\n, \nH\n\niv\ne \n\nFl\num\n\ne,\n \n\nH\nD\n\nF/\nS3\n\n, \nKi\n\nne\nsi\n\ns, \nTC\n\nP \nso\n\nck\net\n\ns, \nTw\n\nit-\nte\n\nr, \nSQ\n\nL\n\nBa\ntc\n\nh,\n It\n\ner\nat\n\niv\ne,\n\n \nSt\n\nre\nam\n\nin\ng\n\nC\nPU\n\n/m\nem\n\nor\ny \n\nin\nte\n\nns\niv\n\ne\nRD\n\nD\n b\n\nas\ned\n\n \nC\n\nhe\nck\n\n-p\noi\n\nnt\n-\n\nin\ng,\n\n p\nar\n\nal\nle\n\nl \nre\n\nco\nve\n\nry\n, \n\nre\npl\n\nic\nat\n\nio\nn\n\nLo\nw\n\nH\nig\n\nh\nEx\n\nac\ntly\n\n o\nnc\n\ne\nW\n\nin\ndo\n\nw\ns, \n\nm\nac\n\nO\nS,\n\n L\nin\n\nux\nSc\n\nal\na,\n\n P\nyt\n\nho\nn,\n\n \nJa\n\nva\n, R\n\nEv\nen\n\nt d\net\n\nec\ntio\n\nn,\n \n\nst\nre\n\nam\nin\n\ng \nm\n\nac\nhi\n\nne\n \n\nle\nar\n\nni\nng\n\n, f\nog\n\n c\nom\n\n-\npu\n\ntin\ng,\n\n in\nte\n\nra\nct\n\niv\ne \n\nan\nal\n\nys\nis\n\n, m\nul\n\ntim\ne-\n\ndi\na \n\nan\nal\n\nys\nis\n\n, c\nlu\n\nst\ner\n\n \nan\n\nal\nys\n\nis\n, fi\n\nlte\nrin\n\ng,\n \n\nre\n-p\n\nro\nce\n\nss\nin\n\ng,\n \n\nca\nch\n\ne \nin\n\nva\nlid\n\nat\nio\n\nn\n\nA\npa\n\nch\ne \n\nSt\nor\n\nm\nSp\n\nou\nt, \n\nH\nBa\n\nse\n, \n\nH\niv\n\ne,\n S\n\nQ\nL,\n\n \nCa\n\nss\nan\n\ndr\na,\n\n \nM\n\nem\nca\n\nch\ned\n\nSt\nre\n\nam\nin\n\ng\nC\n\nPU\n/m\n\nem\nor\n\ny \nin\n\nte\nns\n\niv\ne\n\nRe\npl\n\nic\nat\n\nio\nn,\n\n \nch\n\nec\nkp\n\noi\nnt\n\n, \nda\n\nta\n re\n\nco\nve\n\nry\n, \n\nU\nps\n\ntr\nea\n\nm\n \n\nba\nck\n\nup\n, \n\nre\nco\n\nrd\n-le\n\nve\nl \n\nac\nkn\n\now\nle\n\ndg\ne-\n\nm\nen\n\nt, \nst\n\nat\nel\n\nes\ns \n\nm\nan\n\nag\nem\n\nen\nt\n\nVe\nry\n\n lo\nw\n\nLo\nw\n\nA\nt l\n\nea\nst\n\n o\nnc\n\ne\nW\n\nin\ndo\n\nw\ns, \n\nm\nac\n\nO\nS,\n\n L\nin\n\nux\nC\n\nlo\nju\n\nre\n, J\n\nav\na,\n\n S\nca\n\nla\n, \n\nC\nlo\n\nju\nre\n\n, n\non\n\n-J\nVM\n\n \nla\n\nng\nua\n\nge\ns\n\nIn\nte\n\nrn\net\n\n o\nf t\n\nhi\nng\n\ns, \nst\n\nre\nam\n\nin\ng \n\nm\nac\n\nhi\nne\n\n \nle\n\nar\nni\n\nng\n, m\n\nul\ntim\n\ne-\ndi\n\na \nan\n\nal\nys\n\nis\n\nYa\nho\n\no!\n S\n\n4\nM\n\nyS\nQ\n\nL,\n N\n\noS\nQ\n\nL,\n \n\nRi\nch\n\n D\nat\n\na \nFo\n\nrm\nat\n\nSt\nre\n\nam\nin\n\ng\nC\n\nPU\n/m\n\nem\nor\n\ny \nin\n\nte\nns\n\niv\ne\n\nRe\npl\n\nic\nat\n\nio\nn,\n\n \nch\n\nec\nkp\n\noi\nnt\n\n, \nda\n\nta\n re\n\nco\nve\n\nry\n\nLo\nw\n\nLo\nw\n\nEx\nac\n\ntly\n o\n\nnc\ne\n\nLi\nnu\n\nx\nJa\n\nva\n, P\n\nyt\nho\n\nn,\n C\n+\n+\n\n, \nPe\n\nrl\nO\n\nnl\nin\n\ne \nan\n\nal\nyt\n\nic\ns, \n\nm\non\n\nito\nrin\n\ng,\n fr\n\nau\nd \n\nde\nte\n\nct\nio\n\nn,\n fi\n\nna\nnc\n\nia\nl \n\nda\nta\n\n p\nro\n\nce\nss\n\nin\ng,\n\n \nw\n\neb\n p\n\ner\nso\n\nna\nliz\n\na-\ntio\n\nn \nan\n\nd \nse\n\nss\nio\n\nn \nm\n\nod\nel\n\nlin\ng\n\n\n\nPage 15 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nTa\nbl\n\ne \n9 \n\n(c\non\n\nti\nnu\n\ned\n)\n\nTo\nol\n\ns \nan\n\nd \nte\n\nch\nno\n\nlo\ngy\n\nD\nat\n\nab\nas\n\ne \nsu\n\npp\nor\n\nt\nEx\n\nec\nut\n\nio\nn \n\nm\nod\n\nel\nW\n\nor\nkl\n\noa\nd\n\nFa\nul\n\nt t\nol\n\ner\nan\n\nce\nLa\n\nte\nnc\n\ny\nTh\n\nro\nug\n\nhp\nut\n\nRe\nlia\n\nbi\nlit\n\ny\nO\n\npe\nra\n\ntin\ng \n\nsy\nst\n\nem\nIm\n\npl\nem\n\nen\nta\n\ntio\nn/\n\nsu\npp\n\nor\nte\n\nd \nla\n\nng\nua\n\nge\ns\n\nA\npp\n\nlic\nat\n\nio\nn\n\nA\npa\n\nch\ne \n\nSa\nm\n\nza\nKa\n\nfk\na,\n\n H\nD\n\nFS\n, \n\nKi\nne\n\nsi\ns, \n\nSt\nre\n\nam\n \n\nco\nns\n\num\ner\n\n, K\ney\n\n-\nva\n\nlu\ne \n\nst\nor\n\nes\n\nSt\nre\n\nam\nin\n\ng,\n b\n\nat\nch\n\n \npr\n\noc\nes\n\nsi\nng\n\nM\nem\n\nor\ny \n\nin\nte\n\nn-\nsi\n\nve\nC\n\nhe\nck\n\npo\nin\n\nt\nVe\n\nry\n lo\n\nw\nH\n\nig\nh\n\nA\nt l\n\nea\nst\n\n o\nnc\n\ne\nLi\n\nnu\nx,\n\n W\nin\n\ndo\nw\n\ns\nJa\n\nva\n, S\n\nca\nla\n\n, J\nVM\n\n \nla\n\nng\nua\n\nge\ns\n\nFi\nlte\n\nrin\ng,\n\n re\n-p\n\nro\n-\n\nce\nss\n\nin\ng,\n\n c\nac\n\nhe\n \n\nin\nva\n\nlid\nat\n\nio\nn\n\nA\npa\n\nch\ne \n\nFl\nin\n\nk\nKa\n\nfk\na,\n\n F\nlu\n\nm\ne,\n\n \nH\n\nD\nF/\n\nS3\n, \n\nKi\nne\n\nsi\ns, \n\nTC\nP \n\nso\nck\n\net\ns, \n\nTw\nit-\n\nte\nr, \n\nCa\nss\n\nan\ndr\n\na,\n \n\nRe\ndi\n\ns, \nM\n\non\n-\n\ngo\nD\n\nB,\n H\n\nBa\nse\n\n, \nSQ\n\nL\n\nSt\nre\n\nam\nin\n\ng,\n \n\nba\ntc\n\nh,\n it\n\ner\nat\n\niv\ne,\n\n \nin\n\nte\nra\n\nct\niv\n\ne\n\nM\nem\n\nor\ny \n\nin\nte\n\nn-\nsi\n\nve\nSt\n\nre\nam\n\n re\npl\n\nay\n \n\nan\nd \n\nm\nar\n\nke\nr-\n\nch\nec\n\nkp\noi\n\nnt\n\nVe\nry\n\n lo\nw\n\nH\nig\n\nh\nEx\n\nac\ntly\n\n o\nnc\n\ne\nLi\n\nnu\nx,\n\n M\nac\n\nO\nS,\n\n \nW\n\nin\ndo\n\nw\ns\n\nJa\nva\n\n, S\nca\n\nla\n, P\n\nyt\nho\n\nn\nO\n\npt\nim\n\niz\nat\n\nio\nn \n\nof\n \n\ne-\nco\n\nm\nm\n\ner\nce\n\n \nse\n\nar\nch\n\n re\nsu\n\nlt,\n \n\nne\ntw\n\nor\nk/\n\nse\nns\n\nor\n \n\nm\non\n\nito\nrin\n\ng \nan\n\nd \ner\n\nro\nr d\n\net\nec\n\ntio\nn,\n\n \nET\n\nL \nfo\n\nr b\nus\n\nin\nes\n\ns \nin\n\nte\nlli\n\nge\nnc\n\ne \nin\n\nfra\n-\n\nst\nru\n\nct\nur\n\ne,\n m\n\nac\nhi\n\nne\n \n\nle\nar\n\nni\nng\n\nA\npa\n\nch\ne \n\nA\nur\n\nor\na\n\nH\n2,\n\n Ja\nva\n\n m\nap\n\ns, \nM\n\nyB\nat\n\nis\n, \n\nM\nyS\n\nQ\nL,\n\n P\nos\n\nt-\ngr\n\neS\nQ\n\nL\n\nSt\nre\n\nam\nin\n\ng\nM\n\nem\nor\n\ny \nan\n\nd \ndi\n\nsk\n s\n\npa\nce\n\nPe\nrio\n\ndi\nc \n\nre\nco\n\nv-\ner\n\ny \nch\n\nec\nkp\n\noi\nnt\n\n \nan\n\nd \nro\n\nllb\nac\n\nk\n\nLo\nw\n\nH\nig\n\nh\nA\n\nt l\nea\n\nst\n o\n\nnc\ne\n\nLi\nnu\n\nx\nPy\n\nth\non\n\nM\non\n\nito\nrin\n\ng \nap\n\npl\nic\n\na-\ntio\n\nns\n s\n\nuc\nh \n\nas\n \n\nfin\nan\n\nci\nal\n\n a\nna\n\nly\nsi\n\ns \nan\n\nd \nm\n\nili\nta\n\nry\n a\n\npp\nli-\n\nca\ntio\n\nns\n\nRe\ndi\n\ns\nKe\n\ny-\nva\n\nlu\ne \n\nst\nor\n\nes\n, \n\nra\nbi\n\ntm\nq,\n\n M\non\n\n-\ngo\n\nD\nB\n\nSt\nre\n\nam\nin\n\ng\nIn\n\n-m\nem\n\nor\ny \n\nbu\nt \n\npe\nrs\n\nis\nte\n\nnt\n o\n\nn-\ndi\n\nsk\n d\n\nat\nab\n\nas\ne\n\nRe\npl\n\nic\na \n\nm\nig\n\nra\n-\n\ntio\nn,\n\n S\nen\n\ntin\nel\n\nLo\nw\n\nH\nig\n\nh\nA\n\nt l\nea\n\nst\n o\n\nnc\ne\n\nU\nbu\n\nnt\nu,\n\n L\nin\n\nux\n, \n\nO\nSX\n\nC\n, C\n\n#,\n Ja\n\nva\n, P\n\nH\nP, \n\nPy\nth\n\non\nW\n\neb\n a\n\nna\nly\n\nsi\ns, \n\nca\nch\n\ne,\n \n\nm\nes\n\nsa\nge\n\n q\nue\n\nue\ns\n\nC\n-S\n\nPA\nRQ\n\nL\nRD\n\nF, \nSQ\n\nLJ\n, \n\nN\noS\n\nQ\nL,\n\n H\nD\n\nF\nBa\n\ntc\nh,\n\n s\ntr\n\nea\nm\n\nin\ng\n\nLo\nw\n\n m\nem\n\nor\ny \n\nus\nag\n\ne\nA\n\nda\npt\n\nat\nio\n\nn\nVe\n\nry\n lo\n\nw\nH\n\nig\nh\n\nCu\nm\n\nul\nat\n\niv\ne\n\nW\nin\n\ndo\nw\n\ns, \nLi\n\nnu\nx,\n\n M\nac\n\nO\nS,\n\n \nA\n\nnd\nro\n\nid\n\nJa\nva\n\n, A\npa\n\nch\ne \n\nJe\nna\n\n \nlib\n\nra\nrie\n\ns\nRe\n\nal\n-t\n\nim\ne \n\nre\nas\n\non\nin\n\ng \nov\n\ner\n s\n\nen\nso\n\nr d\nat\n\na,\n \n\nso\nci\n\nal\n s\n\nem\nan\n\ntic\n \n\nda\nta\n\n, u\nrb\n\nan\n c\n\nom\n-\n\npu\ntin\n\ng\n\nSA\nM\n\nO\nA\n\nH\nBa\n\nse\n, H\n\niv\ne,\n\n C\nas\n\n-\nsa\n\nnd\nra\n\nSt\nre\n\nam\nin\n\ng\nLo\n\nw\n m\n\nem\nor\n\ny \nus\n\nag\ne\n\nU\nps\n\ntr\nea\n\nm\n \n\nba\nck\n\nup\nLo\n\nw\nH\n\nig\nh\n\nEx\nac\n\ntly\n o\n\nnc\ne\n\nLi\nnu\n\nx\nJa\n\nva\nC\n\nla\nss\n\nifi\nca\n\ntio\nn,\n\n c\nlu\n\nst\ner\n\n-\nin\n\ng,\n s\n\npa\nm\n\n d\net\n\nec\n-\n\ntio\nn,\n\n re\ngr\n\nes\nsi\n\non\n, \n\nfre\nqu\n\nen\nt p\n\nat\nte\n\nrn\n \n\nm\nin\n\nin\ng\n\n\n\nPage 16 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nTa\nbl\n\ne \n9 \n\n(c\non\n\nti\nnu\n\ned\n)\n\nTo\nol\n\ns \nan\n\nd \nte\n\nch\nno\n\nlo\ngy\n\nD\nat\n\nab\nas\n\ne \nsu\n\npp\nor\n\nt\nEx\n\nec\nut\n\nio\nn \n\nm\nod\n\nel\nW\n\nor\nkl\n\noa\nd\n\nFa\nul\n\nt t\nol\n\ner\nan\n\nce\nLa\n\nte\nnc\n\ny\nTh\n\nro\nug\n\nhp\nut\n\nRe\nlia\n\nbi\nlit\n\ny\nO\n\npe\nra\n\ntin\ng \n\nsy\nst\n\nem\nIm\n\npl\nem\n\nen\nta\n\ntio\nn/\n\nsu\npp\n\nor\nte\n\nd \nla\n\nng\nua\n\nge\ns\n\nA\npp\n\nlic\nat\n\nio\nn\n\nCQ\nEL\n\nS\nRD\n\nF, \nSQ\n\nLJ\n, \n\nN\noS\n\nQ\nL,\n\n H\nD\n\nF\nBa\n\ntc\nh,\n\n s\ntr\n\nea\nm\n\nin\ng\n\nIn\n-m\n\nem\nor\n\ny\nA\n\nda\npt\n\nat\nio\n\nn\nLo\n\nw\nH\n\nig\nh\n\nCu\nm\n\nul\nat\n\niv\ne\n\nW\nin\n\ndo\nw\n\ns, \nLi\n\nnu\nx,\n\n M\nac\n\nO\nS,\n\n \nA\n\nnd\nro\n\nid\n\nJa\nva\n\nRe\nal\n\n-t\nim\n\ne \nre\n\nas\non\n\nin\ng \n\nov\ner\n\n s\nen\n\nso\nr d\n\nat\na,\n\n \nso\n\nci\nal\n\n s\nem\n\nan\ntic\n\n \nda\n\nta\n, u\n\nrb\nan\n\n c\nom\n\n-\npu\n\ntin\ng\n\nET\nA\n\nLI\nS\n\nRD\nF\n\nSt\nre\n\nam\nin\n\ng\nBi\n\nna\nriz\n\nat\nio\n\nn\nA\n\nda\npt\n\nat\nio\n\nn\nLo\n\nw\nLo\n\nw\nCu\n\nm\nul\n\nat\niv\n\ne\nW\n\nin\ndo\n\nw\ns, \n\nLi\nnu\n\nx,\n M\n\nac\nO\n\nS,\n \n\nA\nnd\n\nro\nid\n\nPr\nol\n\nog\n, J\n\nav\na,\n\n C\n, \n\nSP\nA\n\nRQ\nL,\n\n C\n#,\n\n \nET\n\nA\nLI\n\nS \nLa\n\nng\nua\n\nge\n \n\nfo\nr E\n\nve\nnt\n\ns \n(E\n\nLE\n)\n\nEv\nen\n\nt d\net\n\nec\ntio\n\nn,\n \n\nre\nas\n\non\nin\n\ng \nov\n\ner\n \n\nst\nre\n\nam\nin\n\ng \nev\n\nen\nts\n\nXS\nEQ\n\nXM\nL\n\nBa\ntc\n\nh,\n s\n\ntr\nea\n\nm\nin\n\ng\nIn\n\n-m\nem\n\nor\ny \n\nw\nith\n\n \nbu\n\nffe\nrin\n\ng\nch\n\nec\nkp\n\noi\nnt\n\nLo\nw\n\nH\nig\n\nh\nA\n\nt l\nea\n\nst\n o\n\nnc\ne\n\nW\nin\n\ndo\nw\n\ns, \nLi\n\nnu\nx\n\nJa\nva\n\n, A\npa\n\nch\ne \n\nXe\nrc\n\nes\nBi\n\nol\nog\n\nic\nal\n\n d\nat\n\na,\n s\n\noc\nia\n\nl \nne\n\ntw\nor\n\nks\n, u\n\nse\nr \n\nbe\nha\n\nvi\nou\n\nr, \nfin\n\nan\nci\n\nal\n \n\nda\nta\n\n a\nna\n\nly\nsi\n\ns, \nfil\n\nte\nrin\n\ng\n\nIB\nM\n\n In\nfo\n\nSp\nhe\n\nre\n \n\nst\nre\n\nam\ns\n\nPi\ng,\n\n H\niv\n\ne,\n Ja\n\nql\n, \n\nH\nBa\n\nse\n F\n\nlu\nm\n\ne,\n \n\nLu\nce\n\nne\n, A\n\nvr\no,\n\n \nZo\n\noK\nee\n\npe\nr, \n\nO\noz\n\nie\n, O\n\nra\ncl\n\ne \nD\n\nat\nab\n\nas\ne,\n\n \nD\n\nB2\n, N\n\net\nez\n\nza\n, \n\nM\nyS\n\nQ\nL,\n\n A\nst\n\ner\n, \n\nIn\nfo\n\nrm\nix\n\n.\n\nSt\nre\n\nam\nin\n\ng\nCa\n\npt\nur\n\ne \nda\n\nta\nba\n\nse\n \n\nw\nor\n\nkl\noa\n\nds\n a\n\nnd\n \n\nre\npl\n\nay\n th\n\nem\n in\n\n \na \n\nte\nst\n\n d\nat\n\nab\nas\n\ne \nen\n\nvi\nro\n\nnm\nen\n\nt\n\nA\nut\n\nom\nat\n\nic\n \n\nre\nco\n\nve\nry\n\nLo\nw\n\nH\nig\n\nh\nEx\n\nac\ntly\n\n o\nnc\n\ne,\n \n\nA\nt l\n\nea\nst\n\n \non\n\nce\n, A\n\nt \nm\n\nos\nt o\n\nnc\ne\n\nLi\nnu\n\nx,\n C\n\nen\ntO\n\nS\nC\n\n +\n+\n\nJa\nva\n\nSP\nL\n\nSp\nac\n\ne \nw\n\nea\nth\n\ner\n p\n\nre\n-\n\ndi\nct\n\nio\nn,\n\n p\nhy\n\nsi\nol\n\nog\ni-\n\nca\nl d\n\nat\na \n\nst\nre\n\nam\ns \n\nan\nal\n\nys\nis\n\n, t\nra\n\nffi\nc \n\nm\nan\n\nag\nem\n\nen\nt, \n\nre\nal\n\n-\ntim\n\ne \npr\n\ned\nic\n\ntio\nns\n\n, \nev\n\nen\nt d\n\net\nec\n\ntio\nn,\n\n \nvi\n\nsu\nal\n\nis\nat\n\nio\nn\n\nG\noo\n\ngl\ne \n\nM\nill\n\n-\nW\n\nhe\nel\n\nBi\ngT\n\nab\nle\n\n, S\npa\n\nn-\nne\n\nr\nSt\n\nre\nam\n\nin\ng\n\nIn\n-m\n\nem\nor\n\ny \nan\n\nd \nbl\n\noo\nm\n\n fi\nlte\n\nrin\ng\n\nU\nnc\n\noo\nrd\n\nin\nat\n\ned\n \n\npe\nrio\n\ndi\nc,\n\n \nch\n\nec\nkp\n\noi\nnt\n\n, \nup\n\nst\nre\n\nam\n \n\nba\nck\n\nup\n\nLo\nw\n\nH\nig\n\nh\nEx\n\nac\ntly\n\n o\nnc\n\ne\nLi\n\nnu\nx\n\nVi\nrt\n\nua\nlly\n\n a\nny\n\n \npr\n\nog\nra\n\nm\nm\n\nin\ng \n\nla\nng\n\nua\nge\n\nA\nno\n\nm\nal\n\ny \nde\n\nte\nct\n\nio\nn,\n\n \nhe\n\nal\nth\n\n m\non\n\nito\nrin\n\ng,\n \n\nim\nag\n\ne \npr\n\noc\nes\n\nsi\nng\n\n, \nne\n\ntw\nor\n\nk \nsw\n\nitc\nh \n\nm\nan\n\nag\nem\n\nen\nt\n\n\n\nPage 17 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nTa\nbl\n\ne \n9 \n\n(c\non\n\nti\nnu\n\ned\n)\n\nTo\nol\n\ns \nan\n\nd \nte\n\nch\nno\n\nlo\ngy\n\nD\nat\n\nab\nas\n\ne \nsu\n\npp\nor\n\nt\nEx\n\nec\nut\n\nio\nn \n\nm\nod\n\nel\nW\n\nor\nkl\n\noa\nd\n\nFa\nul\n\nt t\nol\n\ner\nan\n\nce\nLa\n\nte\nnc\n\ny\nTh\n\nro\nug\n\nhp\nut\n\nRe\nlia\n\nbi\nlit\n\ny\nO\n\npe\nra\n\ntin\ng \n\nsy\nst\n\nem\nIm\n\npl\nem\n\nen\nta\n\ntio\nn/\n\nsu\npp\n\nor\nte\n\nd \nla\n\nng\nua\n\nge\ns\n\nA\npp\n\nlic\nat\n\nio\nn\n\nIn\nfo\n\nch\nim\n\nps\n \n\ncl\nou\n\nd\nSQ\n\nL,\n N\n\noS\nQ\n\nL,\n \n\nH\niv\n\ne,\n P\n\nig\n \n\nW\nuk\n\non\ng,\n\n \nH\n\nad\noo\n\np,\n \n\nRD\nBM\n\nS,\n V\n\nirt\nu-\n\nal\nly\n\n a\nny\n\n d\nat\n\na \nfo\n\nrm\nat\n\nBa\ntc\n\nh,\n s\n\ntr\nea\n\nm\nin\n\ng\nIn\n\n-m\nem\n\nor\ny\n\nU\nps\n\ntr\nea\n\nm\n \n\nba\nck\n\nup\nLo\n\nw\nH\n\nig\nh\n\nEx\nac\n\ntly\n o\n\nnc\ne\n\nLi\nnu\n\nx\nJa\n\nva\nD\n\nis\nas\n\nte\nr d\n\nis\nco\n\nve\nry\n\n, \nte\n\nxt\n a\n\nna\nly\n\nsi\ns, \n\nco\nm\n\n-\npl\n\nex\n e\n\nve\nnt\n\n p\nro\n\nce\nss\n\n-\nin\n\ng,\n v\n\nis\nua\n\nlis\nat\n\nio\nn\n\nM\nic\n\nro\nso\n\nft\n \n\nSt\nre\n\nam\nIn\n\nsi\ngh\n\nt\nSQ\n\nL \nSe\n\nrv\ner\n\nSt\nre\n\nam\nin\n\ng\nIn\n\n-m\nem\n\nor\ny\n\nRe\npl\n\nic\nat\n\nio\nn,\n\n \nch\n\nec\nkp\n\noi\nnt\n\n, \nda\n\nta\n re\n\nco\nve\n\nry\n\nVe\nry\n\n lo\nw\n\nH\nig\n\nh\nEx\n\nac\ntly\n\n o\nnc\n\ne\nW\n\nin\ndo\n\nw\ns\n\n.N\nET\n\n, C\n#,\n\n L\nIN\n\nQ\n, R\n\nx\nM\n\nan\nuf\n\nac\ntu\n\nrin\ng \n\npr\noc\n\nes\ns \n\nm\non\n\nito\nr-\n\nin\ng \n\nan\nd \n\nco\nnt\n\nro\nl, \n\nfin\nan\n\nci\nal\n\n d\nat\n\na \nan\n\nal\nys\n\nis\n, o\n\npe\nra\n\n-\ntio\n\nn \nan\n\nal\nyt\n\nic\ns, \n\nw\neb\n\n \nan\n\nal\nyt\n\nic\ns, \n\nev\nen\n\nt \npa\n\ntt\ner\n\nn \nde\n\nte\nct\n\nio\nn\n\nTI\nBC\n\nO\n S\n\ntr\nea\n\nm\n-\n\nBa\nse\n\nO\nra\n\ncl\ne \n\nda\nta\n\nba\nse\n\n, \nSQ\n\nL \nSe\n\nrv\ner\n\n, \nIm\n\npa\nla\n\nBa\ntc\n\nh,\n S\n\ntr\nea\n\nm\nin\n\ng\nIn\n\n-m\nem\n\nor\ny\n\nSy\nnc\n\nhr\non\n\niz\nat\n\nio\nn,\n\n \nre\n\npl\nic\n\nat\nio\n\nn,\n \n\nro\nllb\n\nac\nk\n\nVe\nry\n\n lo\nw\n\nH\nig\n\nh\nA\n\nt l\nea\n\nst\n o\n\nnc\ne/\n\nat\n m\n\nos\nt \n\non\nce\n\n/\nex\n\nac\ntly\n\n o\nnc\n\ne\n\nW\nin\n\ndo\nw\n\ns, \nM\n\nac\nO\n\nS,\n L\n\nin\nux\n\nR,\n Ja\n\nva\nM\n\nis\nsi\n\non\n c\n\nrit\nic\n\nal\n \n\nan\nal\n\nys\nis\n\n, I\noT\n\n a\nna\n\nly\n-\n\nsi\ns, \n\ncl\nic\n\nk-\nst\n\nre\nam\n\n \nan\n\nal\nys\n\nis\n, p\n\nre\ndi\n\nct\niv\n\ne \nan\n\nal\nyt\n\nic\ns, \n\nw\nor\n\nkfl\now\n\n \nop\n\ntim\niz\n\nat\nio\n\nn,\n ri\n\nsk\n \n\nav\noi\n\nda\nnc\n\ne\n\nLa\nm\n\nbd\na \n\nA\nrc\n\nhi\n-\n\nte\nct\n\nur\ne\n\nRD\nBM\n\nS,\n C\n\nas\nsa\n\nn-\ndr\n\na,\n K\n\naf\nka\n\n, D\nat\n\na \nW\n\nar\neh\n\nou\nse\n\ns, \nKi\n\nne\nsi\n\ns \nD\n\nat\na \n\nSt\nre\n\nam\n, H\n\nD\nFS\n\n, \nH\n\nBa\nse\n\nBa\ntc\n\nh,\n S\n\ntr\nea\n\nm\nin\n\ng\nIn\n\n-m\nem\n\nor\ny/\n\ndi\nsk\n\n \nda\n\nta\nba\n\nse\nRe\n\npl\nic\n\nat\nio\n\nn,\n \n\nch\nec\n\nkp\noi\n\nnt\nLo\n\nw\nLo\n\nw\nEx\n\nac\ntly\n\n o\nnc\n\ne\nU\n\nbu\nnt\n\nu,\n W\n\nin\n-\n\ndo\nw\n\ns, \nLi\n\nnu\nx\n\nJa\nva\n\n, C\n#,\n\n P\nyt\n\nho\nn,\n\n \nPi\n\ng \nLa\n\ntin\nIo\n\nT \nan\n\nal\nys\n\nis\n, t\n\nra\nck\n\nin\ng \n\nre\nal\n\n-t\nim\n\ne \nup\n\nda\nte\n\ns, \nfin\n\nan\nci\n\nal\n ri\n\nsk\n m\n\nan\n-\n\nag\nem\n\nen\nt, \n\ncl\nic\n\nk-\nst\n\nre\nam\n\n a\nna\n\nly\nsi\n\ns\n\n\n\nPage 18 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nperformance because, in addition to “at-least-once” delivery mechanism, it requires the \nstate to be kept at the receiving end in order to filter duplicate deliveries. In other words, \n“at-most-once” delivery mechanism implies that the message may be lost while “at-\nleast-once” delivery ensures that messages are not lost and “exactly-once” implies that \nmessage can neither be lost nor duplicated. “Exactly-once” is suitable for many critical \nsystems where duplicate messages are unacceptable.\n\nResearch Question 4: What are the limitations and strengths of big data streaming tools \n\nand technologies?\n\nObservations from the literature reveal that specific big data streaming technology may \nnot provide the full set of features that are required. It is rare to find specific big data \ntechnology that combines key features such as scalability, integration, fault-tolerance, \ntimeliness, consistency, heterogeneity and incompleteness management, and load bal-\nancing. For instance, Spark streaming [16] and Sonora [44] are excellent and efficient \nfor checkpointing but the operator space available to user codes are limited. S4 does not \nguarantee 100% fault-tolerant persistent state [45]. Storm does not guarantee the order-\ning of messages due to its “at-least-once” mechanism for record delivery [46, 47]. Strict \ntransaction ordering is required by Trident to operate [48]. While streaming SQL pro-\nvide simple and succinct solutions to many streaming problems, the complex application \nlogic (such as matrix multiplication) and intuitive state abstractions are expressed with \nthe operational flow of an imperative language rather than a declarative language such as \nSQL [49–51].\n\nMoreover, BlockMon uses batches and cache locality optimization techniques for \nmemory allocation efficiency and data speed up access. However, deadlock may occur \nif data streams are enqueued with a higher rate than that of the block consumption [52]. \nApache Samza solves batch latency processing problems but requires an added layer for \nflow control [53]. Flink is suitable for heavy stream processing and batch-oriented tasks \nalthough it has scaling limitations [46]. Redis’ in-memory data store makes it extremely \nfast although this implies that available memory size determines the size of the Redis \ndata store [54]. While C-SPARQL and CQELS are excellent for combining static and \nstreaming data, they are not suitable when scalability is required [55]. SAMOA is suit-\nable for machine learning paradigm as it focuses on speed/real-time analytics, scales \nhorizontally and is loosely coupled with its underlying distributed computation platform \n[56]. With Lambda architecture, a real-time layer can complement the batch processing \none thereby reducing maintenance overhead and risk for errors as a result of duplicate \ncode bases. In addition, Lambda architecture handles reprocessing, which is one of the \nkey challenges in stream processing. Two main problems with Lambda architecture are \ncode maintenance in two complex distributed systems that need to produce the same \nresult and high operational complexity [57, 58].\n\nSummarily, there exists various tools and technologies for implementing big data \nstreams and there seems to be no big data streaming tool and technology that offers all \nthe key features required for now. While each tool and technology may have its strengths \nand weaknesses, the choice depends on the objective of the research and data availa-\nbility. A decision in favour of the wrong technology may result in increased overhead \ncost and time. The decision should take into consideration empirical analysis along with \n\n\n\nPage 19 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nsystem requirements. In addition, research efforts should also be directed to how to \nimprove on existing big data streaming tools and technologies to provide key features \nsuch as scalability, integration, fault-tolerance, timeliness, consistency, heterogeneity \nand incompleteness management, and load balancing.\n\nResearch Question 5: What are the evaluation techniques or benchmarks that are used \n\nfor evaluating big data streaming tools and technologies?\n\nThe diversity of big data poses a challenge when it comes to developing big data bench-\nmarks that will be suitable for all workload cases. One cannot stick to one big data \nbenchmark because it has been observed that using only one benchmark on differ-\nent data sets do not give the same result. This implies that benchmark testing should \nbe application specific. Subsequently, in evaluating big data system, the identification \nof workload for an application domain is a prerequisite [59]. Most of the existing big \ndata benchmarks are designed to evaluate a specific type of systems or architectures. For \ninstance, HiBench [60] is suitable for benchmarking Hadoop, Spark and streaming work-\nloads, GridMix [61] and PigMix [62] are for MapReduce Hadoop systems. BigBench [63, \n64] is suitable for benchmarking Teradata Aster DBMS, MapReduce systems, Redshift \ndatabase, Hive, Spark and Impala. Presently, BigDataBench [65, 66] seems to be the only \nbig data benchmark that can evaluate a hybrid of different big data systems.\n\nSo far, many researchers have evaluated their work by making use of synthetic and \nreal-life data. Standard benchmark dataset for big data streaming analytics has not been \nwidely adopted. However, few of the researchers that used standardized benchmarking \nare briefly discussed below. The work of [67] was tested with two benchmarks; Word \nCount and Grep. The result showed that the proposed algorithm can effectively handle \nunstable input and the delay of the total event can be limited to an expected range.\n\nThe tool developed by [68] was tested on both car dataset and Wikinews5 dataset in \ncomparison with sequential processing. It was discovered that their tool (pipeline imple-\nmentation) performed better and faster.\n\nKrawczyk and Wozniak used several benchmark datasets which include Breast-Wis-\nconsin, Pima, Yeast3, Voting records, CYP2C19 isoform, RBF for estimating weights for \nthe new incoming data stream with their proposed method against other standard meth-\nods. They also analysed time and memory requirements. Experimental investigation \nresult proved that the proposed method can achieve better [69].\n\nA benchmark evaluation using an English movie review dataset collected from Rotten \nTomatoes website (a de facto benchmark for analysing sentiment applications) was con-\nducted by [70], the result showed that sentiment analysis engine (SAE) proposed by the \nauthors outperformed the bag of words approach.\n\nAuthors’ suite of ideas in [71] outperformed state-of-the-art searching technique \ncalled EBSM. The work of [72] used various datasets such as KDD-Cup 99, Forest Cover \ntype, Household power consumption, etc. They compared their algorithm—parallel \nK-means clustering with k-means and k-means++, the result showed that their algo-\nrithm performed better in terms of speed.\n\n5 http://en.wikin ews.org.\n\nhttp://en.wikinews.org\n\n\nPage 20 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nMozafari et al. in [73] benchmarked their system, XSeq against other general-purpose \nXML engines. The system outperformed other complex event processing engines by two \norders of magnitude improvement.\n\nAuthors in [74] evaluated their work in terms of time, accuracy and memory using \nForest cover type, Poker hand, and electricity datasets. They compared their method, \nadaptive windowing based online ensemble (AWOE) with other standard methods such \nas accuracy updated ensemble (AUE), online accuracy updated ensemble (OAUE), accu-\nracy weighted ensemble (AWE), dynamic weighted majority (DWM) and Lev Bagging \n(Lev). Their proposed approach outperformed other methods in three perspectives \nwhich include suitability in terms of different type of drifts, better resolved appropriate \nsize of block, and efficiency.\n\nThe evaluation performed by [75] using FACup and Super Tuesday datasets showed \nthat their method, which is a hybrid of topic extraction methods (i.e. a combination of \nfeature pivot and document pivot) has high efficiency and accuracy with respect to recall \nand precision.\n\nEvaluating the performance of low-rank reconstruction and prediction scheme, spe-\ncifically, singular spectrum matrix completion (SS-MC) proposed by [76], SensorScope \nGrand St-Bernard dataset6 and Intel Berkeley Research Lab dataset7 were used. The \nauthors compared their proposed method with three state-of-the-art methods; KNN-\nimputation, RegEM and ADMM version of MC and discovered that their method \noutperformed the other methods in terms of pure reconstruction as well as in the \ndemanding case of simultaneous recovery and prediction.\n\nThe authors in [77] evaluated their work using World Cup 1998 and CAIDA \nAnonymized Internet Traces 2011 datasets. When their method, ECM-Sketch (a sketch \nsynopsis that allows effective summarization of streaming data over both time-based \nand count-based sliding windows) was compared with three state-of-the-art algorithms \n(Sketch variants); ECM-RW, ECM-DW, and ECM-EH, variants using randomized waves, \ndeterministic waves and exponential histograms respectively, their method reduce \nmemory and computational requirements by at least one order of magnitude with a very \nsmall loss in accuracy.\n\nThe work of [78] centred on benchmarking real-time vehicle data streaming models \nfor a smart city using a simulator that emulates the data produced by a given amount of \nsimultaneous drivers. Experiment with the simulator shows that streaming processing \nengine such as Apache Kafka could serve as a replacement to custom-made streaming \nservers to achieve low latency and higher scalability together with cost reduction.\n\nA benchmark among Kyvos Insight, Impala and Spark conducted by [79] shows that \nKyvos Insight performed analytical queries with much lower latencies when there is a \nlarge number of concurrent users due to pre-aggregation and incremental code building \n[80].\n\nAuthors in [81] proposed that in addition to execution time and resource utilization, \nmicroarchitecture-level and energy consumption are key to fully understanding the \nbehaviour of big data frameworks.\n\n6 http://lcav.epfl.ch.page-86035 -en.html.\n7 http://db.csail .mit.edu/labda ta/labda ta.html.\n\nhttp://lcav.epfl.ch.page-86035-en.html\nhttp://db.csail.mit.edu/labdata/labdata.html\n\n\nPage 21 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nIn addition, to strengthen the confidence of big data research evaluation or result, \napplication of empirical methods (i.e. tested or evaluated concept or technology for \nevidence-based result) should be highly encouraged. The current status of empirical \nresearch in big data stream analysis is still at an infant stage. The maturity of a research \nfield is directly proportional to the number of publications with empirical result [20, 21]. \nAccording to [21] that conducted a systematic literature mapping to verify the current \nstatus of empirical research in big data, it was found out that only 151 out of 1778 stud-\nies contained empirical result. As a result, more research efforts should be directed to \nempirical research in order to raise the level of confidence of big data research outputs \nthan it is at present.\n\nMoreover, only a few big data benchmarks are suitable for different workloads at pre-\nsent. Research efforts should be geared towards advancing benchmarks that are suitable \nfor evaluating different big data systems. This would go a long way to reduce cost and \ninteroperability issue.\n\nDiscussion\nFrom the analysis, it was observed that there has been a wave of interest in big data \nstream analysis since 2013. The number of papers produced in 2012 was doubled in \n2013. In the same vein, more than double of the papers in 2013 were produced in 2014. \nThere was a relative surge in 2017 having a total of 98 paper while the year 2018 received \n156 papers (see Tables 9, 10 and Fig. 2). The percentage of papers analyzed from journals \nwas 50%; that of conferences was 41% while that of workshop/technical/symposium was \n9% as depicted in Fig. 3. Figure 4 presented the frequency of research efforts from differ-\nent geographical locations with researchers from China taking the lead.   \n\nThe selection of big data streaming tools and technologies should be based on the \nimportance of each of the factors such as the shape of the data, data access, availabil-\nity and consistent requirements, workload profile required, and latency requirement. \nCareful selection with respect to open source technology must be made especially when \nchoosing a recent technology still in production. Moreover, the problem to address, the \nunderstanding of the true costs, and benefits of both open and proprietary solutions are \nalso vital when making a selection.\n\nA lot of research efforts have been directed to big data stream analysis but social media \nstream preprocessing is still an open issue. Due to inherent characteristics of social \nmedia stream which include incomplete, noisy, slang, abbreviated words, social media \nstreams present a challenge to big data streams analytics algorithms. There is the need \nto give more attention to the preprocessing stage of social media stream analysis in the \nface of incomplete, noisy, slang, and abbreviated words that are pertinent to social media \nstreams in order to improve big data streams analytics result.\n\nOut of 19 big data streaming tools and technologies compared, 100% support stream-\ning, 47.4% can do both batch and streaming processing while only 10.5% support stream-\ning, batch and iterative processing. Depending on the state of the data to be processed, \ninfrastructure preference, business use case, and kind of results that is of interest, choos-\ning a single big data streaming technology platform that supports all the system require-\nments minimizes the effect of interoperability constraints.\n\n\n\nPage 22 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nFrom all the big data streaming tools and technologies reviewed, only IBMInfoS-\nphere and TIBCO StreamBase support all of the three “at-most-once”, “at-least-once”, \nand “exactly-once” message delivery mechanisms while others support one or two of \nthe three delivery mechanisms. Having all the three delivery mechanisms give room for \nflexibility.\n\nIt is rare to find a specific big data technology that combines key features such as scal-\nability, integration, fault-tolerance, timeliness, consistency, heterogeneity and incom-\npleteness management, and load balancing. There seems to be no big data streaming \ntool and technology that offers all the key features required for now. This calls for more \nresearch efforts that are directed to building more robust big data streaming tools and \ntechnologies.\n\nFew big data benchmarks are suitable for a hybrid of big data systems at present and \nstandard benchmark datasets for big data streaming analytics have not been widely \nadopted. Hence, research efforts should be geared towards advancing benchmarks that \nare suitable for evaluating different big data systems.\n\nLimitation of the review\nWhile authors explored Scopus, ScienceDirect and EBSCO databases which index high \nimpact journals and conference papers from IEEE, ACM, SpringerLink, and Elsevier to \nidentify all possible relevant articles, it is possible that some other relevant articles from \nother databases such as Web of Science could have been missed.\n\nThe analysis and synthesis are based on interpretation of selected articles by the \nresearch team. The authors attempted to avoid this by cross-checking papers to deal \nwith bias though that cannot completely rule out the possibility of errors. In addition, \nthe authors implemented the inclusion and exclusion criteria in the selection of articles \nand only relevant articles written in the English Language were selected. Building on the \nunderpinning of the findings of the research, while a lot of research has been done with \nrespect to tools and technologies as well as methods and techniques employed in big \ndata streaming analytics, method of evaluation or benchmarks of the technologies of \nvarious workloads for big data streaming analytics have not received much attention. As \nit could be gathered from the literature reviewed that most of the researchers evaluated \ntheir work using either synthetic or real-life datasets.\n\nConclusion and further work\nAs a result of challenges and opportunities presented by the Information Technology \nrevolution, big data streaming analytics has emerged as the new frontier of competition \nand innovation. Organisations who seize the opportunity of big data streaming analytics \nare provided with insights for robust decision making in real-time thereby making them \nto have an edge over their competitors.\n\nIn this paper, the authors have tried to present a holistic view of big data streaming \nanalytics by conducting a comprehensive literature review to understand and identify \nthe tools and technologies, methods and techniques, benchmarks or methods of evalu-\nation employed, and key issues in big data stream analysis to showcase the signpost of \nfuture research directions.\n\n\n\nPage 23 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nTa\nbl\n\ne \n10\n\n D\nis\n\ntr\nib\n\nut\nio\n\nn \nof\n\n p\nap\n\ner\ns \n\nov\ner\n\n th\ne \n\nst\nud\n\nie\nd \n\nye\nar\n\ns\n\nYe\nar\n\n20\n04\n\n20\n05\n\n20\n06\n\n20\n07\n\n20\n08\n\n20\n09\n\n20\n10\n\n20\n11\n\n20\n12\n\n20\n13\n\n20\n14\n\n20\n15\n\n20\n16\n\n20\n17\n\n20\n18\n\nTo\nta\n\nl\n\nPa\npe\n\nr\n2\n\n1\n2\n\n3\n5\n\n2\n5\n\n4\n5\n\n10\n22\n\n28\n38\n\n98\n15\n\n6\n38\n\n1\n\n\n\nPage 24 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nAlthough a lot of research efforts have been directed towards big data at rest (i.e. \nbig data batch processing), there has been increased interest in analysing big data \nin ",
      "metadata_storage_path": "aHR0cHM6Ly90cmFpbmluZ2NhdGFsb2dzdG9yYWdlLmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlci9zNDA1MzctMDE5LTAyMTAtNy5wZGY1",
      "metadata_content_type": "application/pdf",
      "metadata_author": "Taiwo Kolajo ",
      "metadata_title": "Big data stream analysis: a systematic literature review",
      "people": [
        "Taiwo Kolajo",
        "Olawande Daramola3",
        "Ayodele Adebiyi",
        "Kolajo",
        "taiwo",
        "Kafka",
        "Spark",
        "Moore",
        "BlockMon",
        "Redis",
        "SAMOA",
        "ETALIS",
        "Anodot",
        "Cloudet",
        "Artemis",
        "Kyvos",
        "AtScale",
        "lia",
        "lic",
        "B",
        "Li",
        "tim",
        "tly",
        "ju",
        "Fo",
        "Pe",
        "liz",
        "K",
        "Py",
        "Ke",
        "LJ",
        "Pi",
        "Lu",
        "Zo",
        "oz",
        "ez",
        "lly",
        "lis",
        "Sy",
        "R",
        "D",
        "Storm"
      ],
      "organizations": [
        "ScienceDirect",
        "IEEE",
        "ACM",
        "SpringerLink",
        "Elsevier",
        "Commons",
        "Department",
        "Covenant University",
        "NYMEX",
        "Data Cooperation",
        "IDC",
        "Science",
        "EBSCOhost",
        "ResearchGate",
        "s.com",
        "ect.com",
        "host.com",
        "ate.net",
        "Microsoft",
        "Apache Samza",
        "MavEStream",
        "EsperTech",
        "CQELS",
        "XSEQ",
        "DenStream",
        "OpticStream",
        "Flock",
        "CodeBlue",
        "IBM",
        "TIBCO",
        "KTS",
        "Tag",
        "clustering",
        "CluStream",
        "ACluStream",
        "DCStream",
        "ADStream",
        "FPSPAN",
        "Markov",
        "Infochimps Cloud",
        "once",
        "gy",
        "hp",
        "ic",
        "fk",
        "SQ",
        "PU",
        "dg",
        "ag",
        "ito",
        "FS",
        "ap",
        "uc",
        "PA",
        "RQ",
        "SA",
        "ifi",
        "CQ",
        "rc",
        "IB",
        "ffi",
        "itc",
        "BM",
        "irt",
        "rv",
        "Trident",
        "BlockMon",
        "Redis"
      ],
      "locations": [
        "Ota",
        "Nigeria",
        "stream",
        "Splunk stream",
        "Striim",
        "SPADE",
        "rio",
        "SP",
        "uk",
        "IN"
      ],
      "keyphrases": [
        "Creative Commons Attribution 4.0 International License",
        "effective resource allocation strategy",
        "existing data mining tools",
        "big data stream tools",
        "data stream computing mode",
        "big data streaming tools",
        "Big data stream analysis",
        "big data stream computing",
        "big data streams analysis",
        "Creative Commons license",
        "big data batch computing",
        "big data computing",
        "big data tool",
        "J Big Data",
        "several computational challenges",
        "process- ing requirements",
        "inherent dynamic characteristics",
        "Three major databases",
        "first search string",
        "standard benchmark dataset",
        "load balancing issues",
        "nificant research efforts",
        "technologies Open Access",
        "SURVEY PAPER Kolajo",
        "empirical analysis",
        "real-time analysis",
        "future computing",
        "data sources",
        "parallelization issues",
        "research questions",
        "literature review",
        "Olawande Daramola3",
        "Ayodele Adebiyi",
        "two types",
        "huge amount",
        "methodical approach",
        "global view",
        "exclusion criteria",
        "preprocessing stage",
        "iterative jobs",
        "scalable frameworks",
        "growing size",
        "iveco mmons",
        "unrestricted use",
        "appropriate credit",
        "original author",
        "Information Sciences",
        "Covenant University",
        "Full list",
        "author information",
        "Taiwo Kolajo",
        "orcid.org",
        "information technology",
        "large volume",
        "great velocity",
        "systematic review",
        "initial 2295 papers",
        "47 papers",
        "Introduction",
        "Advances",
        "high-velocity",
        "ability",
        "nature",
        "terms",
        "variety",
        "veracity",
        "volatility",
        "value",
        "new",
        "trend",
        "Abstract",
        "fact",
        "number",
        "applications",
        "methods",
        "techniques",
        "rigorous",
        "comparisons",
        "Scopus",
        "ScienceDirect",
        "EBSCO",
        "journals",
        "conferences",
        "entities",
        "IEEE",
        "ACM",
        "SpringerLink",
        "Elsevier",
        "inclusion",
        "study",
        "privacy",
        "attention",
        "key",
        "features",
        "lytics",
        "conclusion",
        "algorithms",
        "complexity",
        "article",
        "distribution",
        "reproduction",
        "medium",
        "changes",
        "Correspondence",
        "fulokoja",
        "1 Department",
        "Computer",
        "Ota",
        "Nigeria",
        "Page",
        "30Kolajo",
        "big data streaming analytics",
        "big data stream analysis",
        "Big data batch processing",
        "Streaming processing frameworks",
        "stream processing paradigms",
        "Stream processing solutions",
        "data flow graph",
        "real-time application scenarios",
        "real-time, high volume",
        "time data analysis",
        "real-time data stream",
        "batch computing",
        "stream computing",
        "stream processor",
        "high-velocity flow",
        "incoming data",
        "data generating",
        "market data",
        "real-time sources",
        "changing conditions",
        "common understanding",
        "scientific community",
        "key issues",
        "detailed evaluation",
        "massive amount",
        "high- velocity",
        "multiple sources",
        "low latency",
        "new sources",
        "location services",
        "mobile devices",
        "sensor pervasiveness",
        "fundamental assumption",
        "potential value",
        "parallel architectures",
        "decision-making process",
        "static questions",
        "continuous queries",
        "diverse sources",
        "sideration availability",
        "fault tolerance",
        "infinite tuple",
        "actionable results",
        "interconnected streams",
        "related work",
        "research works",
        "Research method",
        "crucial need",
        "real contrasts",
        "ming paradigm",
        "Discussion” section",
        "Result” section",
        "addition",
        "output",
        "low-latency",
        "seconds",
        "demand",
        "reason",
        "huge",
        "organisations",
        "businesses",
        "paper",
        "purpose",
        "overview",
        "findings",
        "implications",
        "practice",
        "update",
        "state",
        "areas",
        "challenges",
        "rest",
        "Background",
        "information",
        "Limitation",
        "Conclusion",
        "Internet",
        "Things",
        "Sensors",
        "freshness",
        "platforms",
        "Storm",
        "Kafka",
        "Spark",
        "Table",
        "motion",
        "essence",
        "fly",
        "scalability",
        "assimilation",
        "production",
        "operations",
        "Fig.",
        "Dimension Batch processing Streaming processing Input Data chunks",
        "big data stream computing environments",
        "big data streaming analysis",
        "domain Web mining",
        "streaming analytics system",
        "big data streams",
        "Hardware Multiple CPUs",
        "Data flow graph",
        "high data rates",
        "data processing node",
        "single limited amount",
        "memory Storage Store",
        "new incoming data",
        "Such processing",
        "new data",
        "operator stream",
        "missing data",
        "data normalization",
        "multiple rounds",
        "distributed system",
        "High fault-tolerance",
        "life-critical systems",
        "data tuples",
        "Data size",
        "new information",
        "new tuple",
        "new trends",
        "model predictions",
        "feature extraction",
        "single pass",
        "external feeds",
        "typical example",
        "non-trivial portion",
        "traffic monitoring",
        "sensor networks",
        "Key issues",
        "useful knowledge",
        "current happenings",
        "speedy manner",
        "organisa- tions",
        "load balancing",
        "privacy issues",
        "exponential growth",
        "computer resources",
        "research efforts",
        "effective resource",
        "cation strategy",
        "small number",
        "different datasets",
        "component failure",
        "time-sensitive processes",
        "security threats",
        "data Time",
        "latest tuples",
        "analytic applications",
        "sliding window",
        "milliseconds Applications",
        "logical container",
        "main challenges",
        "Fault‑tolerance",
        "integration technique",
        "efficient operations",
        "results",
        "operators",
        "models",
        "access",
        "idea",
        "duplicates",
        "parsing",
        "windows",
        "Table 1",
        "Comparison",
        "updates",
        "advance",
        "passes",
        "figure",
        "NYMEX",
        "need",
        "order",
        "problems",
        "performance",
        "timeliness",
        "consistency",
        "heterogeneity",
        "incompleteness",
        "accuracy",
        "The",
        "way",
        "processors",
        "Moore",
        "law",
        "fore",
        "views",
        "interruption",
        "big data stream computing system",
        "organisational resource management specifi",
        "big data stream dataset",
        "Big data stream analytics",
        "good multiple instances replication",
        "big data analytics literature",
        "big data analytics methods",
        "good system structure",
        "distributing envi- ronment",
        "effective tech- niques",
        "Big data streams",
        "competent data presentation",
        "national Data Cooperation",
        "big data analysis",
        "data streams changes",
        "partial data streams",
        "previous research efforts",
        "big data streaming",
        "big data challenges",
        "systematic literature review",
        "High throughput Decision",
        "big threat",
        "streaming analytics",
        "streaming” analytics",
        "streaming data",
        "natural disaster",
        "continuous processing",
        "local views",
        "meaningful content",
        "Load balancing",
        "load shedding",
        "peak loads",
        "average load",
        "global centre",
        "entire information",
        "needs protection",
        "main objectives",
        "future observations",
        "processing algorithms",
        "stream-specific requirements",
        "various tools",
        "research focus",
        "business values",
        "high consistency",
        "high accuracy",
        "main challenge",
        "communicating nodes",
        "individual privacy",
        "ent characteristics",
        "The study",
        "reviews",
        "fraud",
        "tures",
        "architecture",
        "minimal",
        "latency",
        "stability",
        "Heterogeneity",
        "semantics",
        "granularity",
        "correlate",
        "real-time",
        "diversity",
        "hierarchy",
        "resources",
        "variance",
        "result",
        "respect",
        "sub-graph",
        "replicas",
        "portion",
        "issue",
        "opportunities",
        "IDC",
        "half",
        "volume",
        "velocity",
        "variability",
        "consideration",
        "work",
        "section",
        "technologies",
        "Authors",
        "definitions",
        "types",
        "technology",
        "four big data streaming tools",
        "big data stream analysis tools",
        "big data stream literature review",
        "authoritative, full-text scientific, technical",
        "data stream analysis reviews",
        "big data stream framework",
        "big data stream processing",
        "big data stream algorithms",
        "big data stream analytics",
        "three standard database indexes",
        "big data stream technologies",
        "big data analytics",
        "machine learning algorithms",
        "full-text data- bases",
        "large data- base",
        "smart intuitive functionality",
        "electronic journal service",
        "big data technologies",
        "academic journal articles",
        "leading information solution",
        "systematic mapping method",
        "following research questions",
        "good search string",
        "Data sources",
        "four categories",
        "peer-reviewed literature",
        "citation database",
        "standard databases",
        "information professionals",
        "bibliographic database",
        "empirical research",
        "same vein",
        "particular focus",
        "anomaly detection",
        "clear explanation",
        "Relevant publications",
        "Science Direct",
        "hensive overview",
        "research output",
        "social sciences",
        "largest abstract",
        "open access",
        "health publications",
        "14 million publications",
        "Life Sciences",
        "Physical Sciences",
        "Health Sciences",
        "wide range",
        "academic researchers",
        "application areas",
        "healthcare profes",
        "evaluation techniques",
        "peer-reviewed journals",
        "3800 journals",
        "status",
        "survey",
        "scope",
        "comprehensive",
        "differences",
        "concept",
        "capabilities",
        "limitations",
        "strengths",
        "benchmarks",
        "population",
        "son",
        "intervention",
        "outcome",
        "keywords",
        "searches",
        "EBSCOhost",
        "rich",
        "abstracts",
        "citations",
        "36,377 titles",
        "11,678 publishers",
        "world",
        "arts",
        "humanities",
        "students",
        "teachers",
        "35,000 books",
        "Engineering",
        "porate",
        "Third Search string refinement result",
        "free online professional network",
        "First search string result",
        "Second search string result",
        "Scopus ScienceDirect EBSCOhost Total",
        "major academic publishers",
        "nine (9) search strings",
        "high impact journals",
        "Inclusion criteria Papers",
        "Table 5 Final Selection",
        "Further refinement",
        "Data retrieval",
        "easy analysis",
        "high-quality e-books",
        "100 million publications",
        "secondary source",
        "rich databases",
        "Boolean ‘OR",
        "2295 arti- cles",
        "three databases",
        "computer science",
        "subject domain",
        "quick overview",
        "Microsoft Excel",
        "three categories",
        "black colour",
        "similar investigations",
        "following categories",
        "primary study",
        "source language",
        "11 million researchers",
        "year range",
        "total number",
        "peer-reviewed conferences",
        "relevant papers",
        "relevant” papers",
        "recent papers",
        "16,711 journals",
        "Table 2",
        "Table 3",
        "Table 4",
        "1989 papers",
        "315 papers",
        "111 papers",
        "45 papers",
        "18 papers",
        "magazine",
        "titles",
        "60,000 audiobooks",
        "iv.",
        "ResearchGate4",
        "scientists",
        "collaborators",
        "authors",
        "subscription",
        "set",
        "sources",
        "interest",
        "stage",
        "PDF",
        "introduction",
        "green",
        "red",
        "colours",
        "end",
        "workshops",
        "technical",
        "symposium",
        "case",
        "part",
        "English",
        "contributions",
        "900,000",
        "1500",
        "alternative big data streaming solutions",
        "effective data management decisions",
        "spike load profile platform",
        "Big data stream platforms",
        "Streaming data sources",
        "many NoSQL databases",
        "specific application interfaces",
        "data loading procedure",
        "enterprise technology vendors",
        "open source community",
        "open source solutions",
        "Workload profile",
        "proprietary solutions",
        "stream applications",
        "high-velocity data",
        "data stores",
        "recent technology",
        "Such platforms",
        "platform distribution",
        "Data access",
        "Several tools",
        "sary tools",
        "single flow",
        "growing demand",
        "pro- jection",
        "different structures",
        "different ways",
        "CAP theorem",
        "consistent loads",
        "consistent flows",
        "web-based services",
        "soft- ware",
        "critical functions",
        "Latency requirement",
        "minimal delay",
        "key-value stores",
        "memory solution",
        "licensing issues",
        "limited maturity",
        "developer communities",
        "modification challenges",
        "large datasets",
        "large scale",
        "network partition",
        "service deployment",
        "service cloud",
        "premise approach",
        "easy integration",
        "consistency requirement",
        "careful selection",
        "serialization technologies",
        "execution",
        "functionalities",
        "response",
        "factors",
        "Shape",
        "capturing",
        "storing",
        "rep",
        "instance",
        "room",
        "flexibility",
        "storage",
        "users",
        "Availability",
        "presence",
        "break",
        "scenario",
        "requests",
        "Infrastructure",
        "option",
        "processing",
        "predictable",
        "workloads",
        "spikes",
        "combination",
        "go",
        "Tables",
        "pricing",
        "innovation",
        "development",
        "lack",
        "support",
        "outdating",
        "problem",
        "big data stream analysis Tools",
        "Multinomial latent dirichlet allocation",
        "Elastic streaming processing engine",
        "real-time social media data",
        "Microsoft azure stream analytics",
        "old-based stream clustering approaches",
        "Table 6 Open source tools",
        "social media analysis",
        "social media streams",
        "high dimensional data",
        "Markov Random Field",
        "Sentiment brand monitoring",
        "IBM InfoSphere streams",
        "Density-based clustering algorithm",
        "Voltage clustering algorithm",
        "little research efforts",
        "maximum similarity threshold",
        "algorithm threshold setting",
        "incremental clustering approaches",
        "Online Spherical K-means",
        "Splunk stream",
        "incoming stream",
        "data grouping",
        "Proprietary tools",
        "WSO2 analytics",
        "Microsoft StreamInsight",
        "clustering algorithms",
        "Spark streaming",
        "Complete Clustering",
        "hierarchical clustering",
        "online clustering",
        "Research Question",
        "true costs",
        "Apache storm",
        "Apache Samza",
        "Apache Aurora",
        "Apache Kylin",
        "dynamic nature",
        "desirable number",
        "prior knowledge",
        "scalable graph",
        "limited space",
        "apriori number",
        "Google MillWheel",
        "TIBCO StreamBase",
        "Kyvos insights",
        "Lambda architecture",
        "Much work",
        "fragmentation issues",
        "tive approach",
        "static values",
        "partitioning algorithms",
        "technology Article",
        "balanced partitioning",
        "time window",
        "Condensed Clusters",
        "existing clusters",
        "Threshold-based techniques",
        "Table 8 Methods",
        "Table 7",
        "understanding",
        "benefits",
        "BlockMon",
        "NoSQL",
        "Photon",
        "MavEStream",
        "EsperTech",
        "Redis",
        "C-SPARQL",
        "SAMOA",
        "CQELS",
        "ETALIS",
        "XSEQ",
        "k-median",
        "k-medoid",
        "expectation-maximization",
        "tendency",
        "decisions",
        "DenStream",
        "OpticStream",
        "Exclusive",
        "outliers",
        "HDDStream",
        "PreDeCon-Stream",
        "PKS-Stream",
        "memory",
        "face",
        "CodeBlue",
        "Anodot",
        "Cloudet",
        "Numenta",
        "Artemis",
        "Striim",
        "AtScale",
        "efficiency",
        "SPADE",
        "learning",
        "LSML",
        "KTS",
        "Dynamic prime-number based security verification",
        "Concept-adapting very fast decision tree",
        "User profile vector update algorithm",
        "online sequential extreme learning machine",
        "major big data streaming tools",
        "Singular spectrum matrix completion",
        "other social media analytics",
        "Tag assignment stream clustering",
        "new semantic technology approaches",
        "MI outlier detection algorithm",
        "Temporal fuzzy concept analysis",
        "social media stream analysis",
        "QRS detection algorithm",
        "19 big data tools",
        "social media post",
        "Parallel K-means clustering",
        "cloud computing algorithm",
        "Locality sensitive hashing",
        "Forward chaining rule",
        "Multi-query optimization strategy",
        "business use case",
        "main performance bottleneck",
        "higher load conditions",
        "Continuous query processing",
        "streaming execution model",
        "online ensemble",
        "Outlier method",
        "event detection",
        "data format",
        "streaming processing",
        "sentiment analysis",
        "overview analysis",
        "Infochimps Cloud",
        "Density cognition",
        "Inc I-MLOF",
        "Adaptive windowing",
        "Nearest neighbour",
        "Markov chains",
        "Many researchers",
        "abbreviated words",
        "contextual meaning",
        "atten- tion",
        "operating system",
        "implementation languages",
        "existing applications",
        "Oracle Database",
        "existing databases",
        "iterative processing",
        "single platform",
        "interoperability constraints",
        "ture preference",
        "noisy, slang",
        "application domain",
        "SQL Server",
        "database support",
        "LSH",
        "TASC",
        "StreamMap",
        "CluStream",
        "HPClustering",
        "D-Stream",
        "DCStream",
        "P-Stream",
        "ADStream",
        "CQR",
        "FPSPAN-growth",
        "OMCA",
        "MQOS",
        "automata",
        "VPA",
        "AWOE",
        "K-anonymity",
        "closeness",
        "ECM-sketch",
        "Block-QuickSort-AdjacentJobMatch",
        "Block-QuickSort-OverlapReplicate",
        "Fuzzy-CSar-AFP",
        "kernels",
        "WOS-ELMK",
        "CVFDT",
        "aspect",
        "incomplete",
        "rithms",
        "quality",
        "10 dimensions",
        "workload",
        "fault-tolerance",
        "throughput",
        "MySQL",
        "example",
        "batch",
        "jobs",
        "dependencies",
        "two",
        "frameworks",
        "fit",
        "choice",
        "kind",
        "once” message delivery mechanisms",
        "three delivery mechanisms",
        "g C PU",
        "long memory latency",
        "S Q L",
        "Ri ch D",
        "D B",
        "RD D",
        "high intensive",
        "memory applications",
        "performance loss",
        "highest performance",
        "transport mechanism",
        "sending end",
        "multiple attempts",
        "transport losses",
        "acknowledgement mechanism",
        "receiving end",
        "ns C",
        "k op",
        "TC P",
        "J VM",
        "k Ve",
        "Ex ac",
        "pa ch",
        "research",
        "benefit",
        "phere",
        "most",
        "least",
        "others",
        "implementation",
        "fire",
        "fashion",
        "ro",
        "hp",
        "lit",
        "tin",
        "Bl",
        "kM",
        "dr",
        "iz",
        "fin",
        "Ka",
        "Fl",
        "S3",
        "Ki",
        "Tw",
        "SQ",
        "Ev",
        "lid",
        "kp",
        "av",
        "rn",
        "ito",
        "lin",
        "H D FS",
        "A LI S",
        "A H",
        "P H",
        "S PA",
        "Q L",
        "Pe rio",
        "SX C",
        "RD F",
        "g Bi",
        "EQ XM",
        "g ch",
        "g SA",
        "M ac",
        "k Lo",
        "g Lo",
        "ar ch",
        "RQ",
        "st",
        "pl",
        "tio",
        "za",
        "ns",
        "po",
        "Ve",
        "ig",
        "Fi",
        "Ca",
        "Re",
        "Ba",
        "iv",
        "nc",
        "gr",
        "Py",
        "ic",
        "ly",
        "Ke",
        "ra",
        "rs",
        "LJ",
        "Cu",
        "lib",
        "rb",
        "Ta",
        "Fa",
        "CQ",
        "Pr",
        "SP",
        "XS",
        "L Sp ac",
        "g IB",
        "S C",
        "g U",
        "TI BC",
        "af ka",
        "ig W",
        "Lo",
        "Li",
        "Xe",
        "Bi",
        "ks",
        "fil",
        "Pi",
        "Lu",
        "vr",
        "Zo",
        "ie",
        "cl",
        "B2",
        "ix",
        "ds",
        "nm",
        "gT",
        "rio",
        "ag",
        "RD",
        "BM",
        "Sy",
        "kfl",
        "op",
        "bd",
        "ar",
        "specific big data streaming technology",
        "specific big data technology",
        "cache locality optimization techniques",
        "batch latency processing problems",
        "many streaming problems",
        "heavy stream processing",
        "memory data store",
        "many critical systems",
        "memory allocation efficiency",
        "100% fault-tolerant persistent state",
        "intuitive state abstractions",
        "available memory size",
        "once” delivery mechanism",
        "data streams",
        "record delivery",
        "duplicate deliveries",
        "other words",
        "full set",
        "incompleteness management",
        "operator space",
        "user codes",
        "transaction ordering",
        "vide simple",
        "succinct solutions",
        "complex application",
        "matrix multiplication",
        "operational flow",
        "imperative language",
        "declarative language",
        "higher rate",
        "block consumption",
        "flow control",
        "batch-oriented tasks",
        "key features",
        "scaling limitations",
        "duplicate messages",
        "Observations",
        "literature",
        "integration",
        "Sonora",
        "checkpointing",
        "S4",
        "Strict",
        "Trident",
        "SQL",
        "logic",
        "batches",
        "deadlock",
        "added",
        "layer",
        "Flink"
      ],
      "masked_text": "\nBig data stream analysis: a systematic \nliterature review\n************1,2* , ******************  and ***************1,4 \n\nIntroduction\nAdvances in information technology have facilitated large volume, high-velocity of data, \nand the ability to store data continuously leading to several computational challenges. \nDue to the nature of big data in terms of volume, velocity, variety, variability, veracity, \nvolatility, and value [1] that are being generated ********, big data computing is a new \ntrend for future computing.\n\nBig data computing can be generally categorized into two types based on the process-\ning requirements, which are big data batch computing and big data stream computing \n\nAbstract \n\n********, big data streams have become ubiquitous due to the fact that a number of \napplications generate a huge amount of data at a great velocity. This made it difficult \nfor existing data mining tools, technologies, methods, and techniques to be applied \ndirectly on big data streams due to the inherent dynamic characteristics of big data. In \nthis paper, a systematic review of big data streams analysis which employed a rigorous \nand methodical approach to look at the trends of big data stream tools and technolo-\ngies as well as methods and techniques employed in analysing big data streams. It \nprovides a global view of big data stream tools and technologies and its comparisons. \nThree major databases, ******, ************* and *****, which indexes journals and \nconferences that are promoted by entities such as ****, ***, ************, and ******** \nwere explored as data sources. Out of the initial 2295 papers that resulted from the \nfirst search string, 47 papers were found to be relevant to our research questions after \nimplementing the inclusion and exclusion criteria. The study found that scalability, \nprivacy and load balancing issues as well as empirical analysis of big data streams and \ntechnologies are still open for further research efforts. We also found that although, sig-\nnificant research efforts have been directed to real-time analysis of big data stream not \nmuch attention has been given to the preprocessing stage of big data streams. Only a \nfew big data streaming tools and technologies can do all of the batch, streaming, and \niterative jobs; there seems to be no big data tool and technology that offers all the key \nfeatures required for now and standard benchmark dataset for big data streaming ana-\nlytics has not been widely adopted. In conclusion, it was recommended that research \nefforts should be geared towards developing scalable frameworks and algorithms that \nwill accommodate data stream computing mode, effective resource allocation strategy \nand parallelization issues to cope with the ever-growing size and complexity of data.\n\nKeywords: Big data stream analysis, Stream computing, Big data streaming tools and \ntechnologies\n\nOpen Access\n\n© **********(s) ****. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original ******(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nSURVEY PAPER\n\n****** et al. J Big Data            (****) ****  \n*****************************************\n\n*Correspondence:   \n**************************; \n**************************** \n1 ********************** \nand Information Sciences, \n*******************, Ota, \nNigeria\nFull list of author information \nis available at the end of the \narticle\n\n************************************\n************************************\n************************************\n*******************************************\n************************************/?doi=10.1186/s40537-019-0210-7&domain=pdf\n\n\nPage 2 of ******** et al. J Big Data            (****) **** \n\n[2]. Big data batch processing is not sufficient when it comes to analysing real-time \napplication scenarios. Most of the data generated in a real-time data stream need real-\ntime data analysis. In addition, the output must be generated with low-latency and any \nincoming data must be reflected in the newly generated output within seconds. This \nnecessitates big data stream analysis [3].\n\nThe demand for stream processing is increasing. The reason being not only that huge \nvolume of data need to be processed but that data must be speedily processed so that \norganisations or businesses can react to changing conditions in real-time.\n\nThis paper presents a systematic review of big data stream analysis. The purpose is to \npresent an overview of research works, findings, as well as implications for research and \npractice. This is necessary to (1) provide an update about the state of research, (2) iden-\ntify areas that are well researched, (3) showcase areas that are lacking and need further \nresearch, and (4) build a common understanding of the challenges that exist for the ben-\nefit of the scientific community.\n\nThe rest of the paper is organized as follows: “Background and related work” section \nprovides information on stream computing and big data stream analysis and the key \nissues involved in it and presents a review on big data streaming analytics. In “Research \nmethod” section, the adopted research methodology is discussed, while “Result” section \npresents the findings of the study. “Discussion” section presents a detailed evaluation \nperformed on big data stream analysis, “Limitation of the review” section highlights the \nlimitations of the study, while “Conclusion and further work” concludes the paper.\n\nBackground and related work\nStream computing\n\nStream computing refers to the processing of massive amount of data generated at high-\nvelocity from multiple sources with low latency in real-time. It is a new paradigm neces-\nsitated because of new sources of data generating scenarios which include ubiquity of \nlocation services, mobile devices, and sensor pervasiveness [4]. It can be applied to the \nhigh-velocity flow of data from real-time sources such as the Internet of Things, Sensors, \nmarket data, mobile, and clickstream.\n\nThe fundamental assumption of this paradigm is that the potential value of data lies in \nits freshness. As a result, data are analysed as soon as they arrive in a stream to produce \nresult as opposed to what obtains in batch computing where data are first stored before \nthey are analysed. There is a crucial need for parallel architectures and scalable com-\nputing platforms [5]. With stream computing, organisations can analyse and respond in \nreal-time to rapidly changing data. Streaming processing frameworks include Storm, S4, \n*****, and ***** [6–8]. The real contrasts between the batch processing and the stream \nprocessing paradigms are outlined in Table 1.\n\nIncorporating streaming data into decision-making process necessitates a program-\nming paradigm called stream computing. With stream computing, fairly static questions \ncan be evaluated on data in motion (i.e. real-time data) continuously [9].\n\nBig data stream analysis\n\nThe essence of big data streaming analytics is the need to analyse and respond to real-\ntime streaming data using continuous queries so that it is possible to continuously \n\n\n\nPage 3 of ******** et al. J Big Data            (****) **** \n\nperform analysis on the fly within the stream. Stream processing solutions must be \nable to handle a real-time, high volume of data from diverse sources putting into con-\nsideration availability, scalability and fault tolerance. Big data stream analysis involves \nassimilation of data as an infinite tuple, analysis and production of actionable results \nusually in a form of stream [10].\n\nIn a stream processor, applications are represented as data flow graph made up of \noperations and interconnected streams as depicted in Fig. 1. In a streaming analytics \nsystem, application comes in a form of continuous queries, data are ingested continu-\nously, analysed and correlated, and stream of results are generated. Streaming analytic \napplications is usually a set of ********* connected by streams. Streaming analytics \nsystems must be able to identify new information, incrementally build models and \naccess whether the new incoming data deviate from model predictions [9].\n\nThe idea of streaming analytics is that each of the received data tuples is processed \nin the data processing node. Such processing includes removing duplicates, filling \nmissing data, data normalization, parsing, feature extraction, which are typically done \nin a single pass due to the high data rates of external feeds. When a new tuple arrives, \nthis node is triggered, and it expels tuples older than the time specified in the sliding \nwindow (sliding window is a typical example of windows used in stream computing \nwhich keeps only the latest tuples up to the time specified in the windows). A window \n\nTable 1 Comparison between batch processing and streaming processing [82]\n\nDimension Batch processing Streaming processing\n\nInput Data chunks Stream of new data or updates\n\nData size Known and finite Infinite or unknown in advance\n\nHardware Multiple CPUs Typical single limited amount of memory\n\nStorage Store Not store or store non-trivial portion in memory\n\nProcessing Processed in multiple rounds A single or few passes over data\n\nTime Much longer ************* or even milliseconds\n\nApplications Widely adopted in almost every domain Web mining, traffic monitoring, sensor networks\n\nFig. 1 Data flow graph of a stream processor. The figure shows how applications (made up of operations and \ninterconnected streams) are represented as data flow graph in a stream processor [10]\n\n operator stream ***** \n\n\n\nPage 4 of ******** et al. J Big Data            (****) **** \n\nis referred to as a logical container for data tuples received. It defines how frequently \ndata is refreshed in the container as well as when data processing is triggered [4].\n\nKey issues in big data stream analysis\n\nBig data stream analysis is relevant when there is a need to obtain useful knowledge \nfrom current happenings in an efficient and speedy manner in order to enable organisa-\ntions to quickly react to problems, or detect new trends which can help improve their \nperformance. However, there are some challenges such as scalability, integration, fault-\ntolerance, timeliness, consistency, heterogeneity and incompleteness, load balancing, \nprivacy issues, and accuracy [3, 11–18] which arises from the nature of big data streams \nthat must be dealt with.\n\nScalability\n\nOne of the main challenges in big data streaming analysis is the issue of scalability. The \nbig data stream is experiencing exponential growth in a way much faster than computer \nresources. The ********** follow *****’s law, but the size of data is exploding. There-\nfore, research efforts should be geared towards developing scalable frameworks and \nalgorithms that will accommodate data stream computing mode, effective resource allo-\ncation strategy and parallelization issues to cope with the ever-growing size and com-\nplexity of data.\n\nIntegration\n\nBuilding a distributed system where each node has a view of the data flow, that is, every \nnode performing analysis with a small number of sources, then aggregating these views \nto build a global view is non-trivial. An integration technique should be designed to ena-\nble efficient operations across different datasets.\n\nFault‑tolerance\n\nHigh fault-tolerance is required in life-critical systems. As data is real-time and infinite \nin big data stream computing environments, a good scalable high fault-tolerance strat-\negy is required that allows an application to continue working despite component failure \nwithout interruption.\n\nTimeliness\n\nTime is of the essence for time-sensitive processes such as mitigating security threats, \nthwarting fraud, or responding to a natural disaster. There is a need for scalable architec-\ntures or platforms that will enable continuous processing of data streams which can be \nused to maximize the timeliness of data. The main challenge is implementing a distrib-\nuted architecture that will aggregate local views of data into global view with minimal \nlatency between communicating nodes.\n\nConsistency\n\nAchieving high consistency (i.e. stability) in big data stream computing environments is \nnon-trivial as it is difficult to determine which data are needed and which nodes should \nbe consistent. Hence a good system structure is required.\n\n\n\nPage 5 of ******** et al. J Big Data            (****) **** \n\nHeterogeneity and incompleteness\n\nBig data streams are heterogeneous in structure, organisations, semantics, accessi-\nbility and granularity. The challenge here is how to handle an always ever-increas-\ning data, extract meaningful content out of it, aggregate and correlate streaming \ndata from multiple sources in real-time. A competent data presentation should be \ndesigned to reflect the structure, diversity and hierarchy of the streaming data.\n\nLoad balancing\n\nA big data stream computing system is expected to be self-adaptive to data streams \nchanges and avoid load shedding. This is challenging as dedicating resources to cover \npeak loads **** is impossible and load shedding is not feasible when the variance \nbetween the average load and the peak load is high. As a result, a distributing envi-\nronment that automatically streams partial data streams to a global centre when local \nresources become insufficient is required.\n\nHigh throughput\n\nDecision with respect to identifying the sub-graph that needs replication, how many \nreplicas are needed and the portion of the data stream to assign to each replica is an \nissue in big data stream computing environment. There is a need for good multiple \ninstances replication if high throughput is to be achieved.\n\nPrivacy\n\nBig data stream analytics created opportunities for analyzing a huge amount of data \nin real-time but also created a big threat to individual privacy. According to the Inter-\nnational **************** (***), not more than half of the entire information that \nneeds protection is effectively protected. The main challenge is proposing techniques \nfor protecting a big data stream dataset before its analysis.\n\nAccuracy\n\nOne of the main objectives of big data stream analysis is to develop effective tech-\nniques that can accurately predict future observations. However, as a result of inher-\nent characteristics of big data such as volume, velocity, variety, variability, veracity, \nvolatility, and value, big data analysis strongly constrain processing algorithms spatio-\ntemporally and hence stream-specific requirements must be taken into consideration \nto ensure high accuracy.\n\nRelated work\n\nThis section discusses some of the previous research efforts that relate to big data \nstreaming analytics.\n\nThe work of [13] presented a review of various tools, technologies and methods \nfor big data analytics by categorizing big data analytics literature according to their \nresearch focus. This paper is different in that it presents a systematic literature review \nthat focused on big data “streaming” analytics.\n\n\n\nPage 6 of ******** et al. J Big Data            (****) **** \n\n******* in [19] presented a systematic review of big data analytics in e-commerce. The \nstudy explored characteristics, definitions, business values, types and challenges of big \ndata analytics in the e-commerce landscape. Likewise, [20] conducted a study that is cen-\ntred on big data analytics in technology and organisational resource management specifi-\ncally focusing on reviews that present big data challenges and big data analytics methods. \nAlthough they are systematic reviews, the focus is not, particularly on big data streaming.\n\nAuthors in [21] presented the status of empirical research and application areas in big \ndata by employing a systematic mapping method. In the same vein, authors in [22] also \nconducted a survey on big data technologies and machine learning algorithms with a \nparticular focus on anomaly detection. A systematic review of literature which aims to \ndetermine the scope, application, and challenges of big data analytics in healthcare was \npresented by [23]. The work of [2] presented a review of four big data streaming tools \nand technologies. While the study conducted in this paper provided a comprehensive \nreview of not only big data streaming tools and technologies but also methods and tech-\nniques employed in analyzing big data streams. In addition, authors [2] did not provide a \nclear explanation of the methodical approach for selecting the reviewed papers.\n\nResearch method\nThe study was grounded in a systematic literature review of tools and technologies \nwith methods and techniques used in analysing big data streams by adopting [24, 25] as \nmodels.\n\nResearch question\n\nThe study tries to answer the following research questions:\n\nResearch Question 1: What are the tools and technologies employed for big data \nstream analysis?\nResearch Question 2: What methods and techniques are used in analysing big data \nstreams?\nResearch Question 3: What do these tools and technologies have in common and \ntheir differences in terms of concept, purpose and capabilities?\nResearch Question 4: What are the limitations and strengths of these tools and tech-\nnologies?\nResearch Question 5: What are the evaluation techniques or benchmarks used for \nevaluating big data streaming tools and technology?\n\nSearch string\n\nCreating a good search string requires structuring in terms of population, compari-\nson, intervention and outcome [24]. Relevant publications were identified by forming \na search string that combined keywords driven by the research questions earlier stated. \nThe searches were conducted by employing three standard database indexes, which are \n******, ************** and *********. The search string is “big data stream analysis” \nOR “big data stream technologies” OR “big data stream framework” OR “big data stream \nalgorithms” OR “big data stream analysis tools” OR “big data stream processing” OR “big \n\n\n\nPage 7 of ******** et al. J Big Data            (****) **** \n\ndata stream analysis reviews” OR “big data stream literature review” OR “big data stream \nanalytics”.\n\nData sources\n\nAs research becomes increasingly interdisciplinary, global and collaborative, it is expedi-\nent to select from rich and standard databases. The databases consulted are as follows:\n\n i. Scopus1: Scopus is a bibliographic database containing abstracts and citations for \nacademic journal articles launched in ****. It covers nearly 36,377 titles from over \n11,678 publishers of which 34,346 are peer-reviewed journals, delivering a compre-\nhensive overview of the world’s research output in the scientific, technical, medi-\ncal, and social sciences (including arts and humanities). It is the largest abstract \nand citation database of peer-reviewed literature.\n\n ii. ScienceDirect2: ScienceDirect is ********’s leading information solution for \n***********, ********, ********, ************************* and healthcare profes-\nsionals. It provides both subscription-based and open access-based to a large data-\nbase combining authoritative, full-text scientific, technical and health publications \nwith smart intuitive functionality. It covers over 14 million publications from over \n3800 journals and more than 35,000 books. The journals are grouped into four \ncategories: Life Sciences, Physical Sciences and Engineering, Health Sciences, and \nSocial Sciences and Humanities.\n\n iii. EBSCOhost3: EBSCOhost covers a wide range of bibliographic and full-text data-\nbases for ***********, providing electronic journal service available to both cor-\nporate and ******** ***********. It has a total of 16,711 journals and magazine \nindexed and abstracted of which 14,914 are peer-reviewed; more than 900,000 \nhigh-quality e-books and titles and over 60,000 audiobooks from more than **** \nmajor academic publishers.\n\n iv. ResearchGate4: A free online professional network for ********** and *********** to \nask and answer questions, share papers and find *************. It covers over 100 \nmillion publications from over 11 million ***********. ResearchGate was used as \na secondary source where the authors could not access some papers due to lack of \nsubscription.\n\nData retrieval\n\nThe search was conducted in ******, ************* and ********* since most of \nthe high impact journals and conferences are indexed in these set of rich databases. \nBoolean ‘OR’ was used in combining the nine (9) search strings. A total of 2295 arti-\ncles from the three databases were retrieved as shown in Table 2.\n\n1 http://www.scopu *****.\n2 http://www.scien cedir *******.\n3 https ://www.ebsco ********.\n4 https ://www.resea archg *******.\n\n*********************\n****************************\n*************************\n*****************************\n\n\nPage 8 of ******** et al. J Big Data            (****) **** \n\nFurther refinement was performed by (i) limiting the search to journals and confer-\nence papers; (ii) selecting computer science and IT related as the subject domain; (iii) \nselecting ***, ****, ************, ******** as sources; and year of publication to \n*********************. ******** range was selected due to the fact that interest in \nbig data stream analysis actually started in ****. At this stage, a total of **** papers \nwere excluded leaving a total of 315 papers (see Table  3). The result of the search \nstring was exported to PDF.\n\nBy going through the title of the papers, 111 seemingly relevant papers were extracted \nexcluding a total number of 213 that were not relevant at this stage (see Table 4).\n\nThe abstracts of 111 papers and introduction (for papers that the abstracts were not \nclear enough) were then read to have a quick overview of the paper and to ascertain \nwhether they are suitable or at variance with the research questions. The citations of \nthe papers were exported to ********* Excel for easy analysis. The papers were grouped \ninto three categories; “relevant”, “may be relevant” and “irrelevant”. The “relevant” papers \nwere marked with black colour, “may be relevant” and “irrelevant” with green and red \ncolours respectively. At the end of this stage, 45 papers were classified as “relevant”, 9 \npapers as “may be relevant” and 11 as “irrelevant”. Looking critically at the abstract again, \n18 papers were excluded by using the exclusion criteria leaving a total of 47 papers (see \nTable 5) which were manually reviewed in line with the research questions.\n\nInclusion criteria\n\nPapers published in journals, peer-reviewed conferences, workshops, technical and \nsymposium from **** and **** were included. In addition, the most recent papers \nwere selected in case of papers with similar investigations and results.\n\nTable 2 First search string result\n\n****************************** Total\n\nNumber of papers **** 65 133 2295\n\nTable ******** search string result\n\n****************************** Total\n\nNumber of papers *************\n\nTable 4 Third Search string refinement result\n\n****************************** Total\n\nNumber of papers ************\n\nTable 5 Final Selection\n\n****************************** Total\n\nNumber of papers ***********\n\n\n\nPage 9 of ******** et al. J Big Data            (****) **** \n\nExclusion criteria\n\nPapers that belong to the following categories were excluded from selection as part of \nthe primary study: (i) papers written in source language other than English; (ii) papers \nwith an abstract and or introduction that does not clearly define the contributions of the \nwork; (iii) papers whose abstract do not relate to big data stream analysis.\n\nResult\nThe findings of the study are *** presented with respect to the research questions that \nguided the execution of the systematic literature review.\n\nResearch Question 1: What are the tools and technologies employed for big data stream \n\nanalysis?\n\nBig data stream platforms provide functionalities and features that enable big data \nstream applications to develop, operate, deploy, and manage big data streams. Such \nplatforms must be able to pull in streams of data, process the data and stream it back \nas a single flow. Several tools and technologies have been employed to analyse big data \nstreams. In response to the growing demand for big data streaming analytics, a large \nnumber of alternative big data streaming solutions have been developed both by the \nopen source community and enterprise technology vendors. According to [26], there are \nsome factors to consider when selecting big data streaming tools and technologies in \norder to make effective data management decisions. These are briefly described below.\n\nShape of the data\n\nStreaming data sources require serialization technologies for capturing, storing and rep-\nresenting such high-velocity data. For instance, some tools and technologies allow pro-\njection of different structures across data stores, giving room for flexibility for storage \nand access of data in different ways. However, the performance of such platforms may \nnot be suitable for high-velocity data.\n\nData access\n\nThere is a need to put into consideration how the data will be accessed by users and \napplications. For instance, many NoSQL databases require specific application interfaces \nfor data access. Hence there is a need to consider the integration of some other neces-\nsary tools for data access.\n\nAvailability and consistency requirement\n\nIf a distributed system is needed, then CAP theorem states that consistency and avail-\nability cannot be both guaranteed in the presence of network partition (i.e. when there is \na break in the network). In such a scenario, consistency is often traded off for availability \nto ensure that requests can always be processed.\n\nWorkload profile required\n\nPlatform as a service deployment may be appropriate for a spike load profile platform. \nIf platform distribution can be deployed on Infrastructure as a service cloud, then this \noption may be preferred as ***** will need to pay only when processing. On-premise \n\n\n\nPage 10 of ******** et al. J Big Data            (****) **** \n\ndeployment may be considered for predictable or consistent loads. But if workloads are \nmixed (i.e. consistent flows or spikes), a combination of cloud and on-premise approach \nmay be considered so as to give room for easy integration of web-based services or soft-\nware and access to critical functions on the go.\n\nLatency requirement\n\nIf a minimal delay or low latency is required, key-value stores may be considered or bet-\nter still, an in-memory solution which allows the process of large datasets in real-time is \nrequired in order to optimize the data loading procedure.\n\nThe tools and technologies for big data stream analysis can be broadly categorized into \ntwo, which are open source and proprietary solutions. These are listed in Tables 6 and 7.\n\nThe selection of big data streaming tools and technologies should be based on the impor-\ntance of each factor earlier mentioned in this section. Proprietary solutions may not be eas-\nily available because of pricing and licensing issues. While open source supports innovation \nand development at a large scale, careful selection must be made especially when choosing \na recent technology still in production due to limited maturity and lack of support from \n******** *********** or ********* communities. In addition, open source solutions *** \nlead to outdating and modification challenges [27]. Moreover, the selection of whether pro-\nprietary or open source or combination of both should depend on the problem to address, \nthe understanding of the true costs, and benefits of both open and proprietary solutions.\n\nTable 6 Open source tools and technologies for big data stream analysis\n\nTools and technology Article\n\nBlockMon [83]\n\nNoSQL [4, 84–86]\n\nSpark streaming [67, 87–91]\n\nApache storm [68, 85, 86, 92–97]\n\n***** [85, 91, 95, 96, 98]\n\n*****! S4 [6, 45, 87, 99]\n\n************ [46, 67, 100]\n\nPhoton [67, 101]\n\nApache Aurora [67, 102]\n\nMavEStream [103]\n\n********* [104, 105]\n\n***** [106]\n\nC-SPARQL [107, 108]\n\n***** [56, 78, 109]\n\nCQELS [108, 110, 111]\n\n****** [112]\n\nXSEQ [73]\n\nApache Kylin [113]\n\nSplunk stream [114]\n\n\n\nPage 11 of ******** et al. J Big Data            (****) **** \n\nResearch Question 2: What methods and techniques are used in analysing big data \n\nstreams?\n\nGiven the real-time nature, velocity and volume of social media streams, the clus-\ntering algorithms that are applied on streaming data must be highly scalable and \nefficient. Also, the dynamic nature of data makes it difficult to know the required or \ndesirable number of clusters in advance. This renders partitioning clustering tech-\nniques (such as k-median, k-means and k-medoid) or expectation-maximization \n(EM) algorithms-based approaches unsuitable for analysing real-time social media \ndata because they require prior knowledge of clusters in advance. In addition, due \nto concept drift inherent in social media streams, scalable graph partitioning algo-\nrithms are not also suitable because of their tendency towards balanced partitioning. \nSocial media streams must be analysed dynamically in order to provide decisions at \nany given time within a limited space and time window [28–30].\n\nDensity-based clustering algorithm (such as DenStream, OpticStream, Flock-\nStream, Exclusive and Complete Clustering) unlike partitioning algorithms does not \nrequire apriori number of clusters in advance and can detect outliers [31]. However, \nthe issue with density-based clustering algorithms is that most of them except for few \nlike HDDStream, PreDeCon-Stream and PKS-Stream (which are memory intensive) \nperform less efficiently in the face of high dimensional data and as a result are not \nsuitable for analyzing social media streams [32].\n\nThreshold-based techniques, hierarchical clustering, and incremental clustering \nor online clustering are more relevant to social media analysis. Several online thresh-\nold-based stream clustering approaches or incremental clustering approaches such as \nMarkov Random Field [33, 34], Online Spherical K-means [35], and Condensed Clusters \n[36] have been adopted. Incremental approaches are suitable for continuously generated \ndata grouping by setting a maximum similarity threshold between the incoming stream \n\nTable 7 Proprietary tools and technologies for big data stream analysis\n\nTools and technology Article\n\nCodeBlue [115]\n\n****** [116]\n\nCloudet [117]\n\nSentiment brand monitoring [118]\n\nNumenta [119]\n\nElastic streaming processing engine [120]\n\n********* azure stream analytics [121]\n\n*** InfoSphere streams [8, 122]\n\nGoogle MillWheel [123]\n\n******* [124]\n\nWSO2 analytics [125]\n\n********* StreamInsight [126]\n\nTIBCO StreamBase [127]\n\nStriim [128]\n\nKyvos insights [129]\n\nAtScale [130, 131]\n\nLambda architecture [57]\n\n\n\nPage 12 of ******** et al. J Big Data            (****) **** \n\nand the existing clusters. Much work has been done in improving the efficiency of online \nclustering algorithms, however, little research efforts have been directed to threshold \nand fragmentation issues. Incremental algorithm threshold setting should employ adap-\ntive approach instead of relying on static values [37, 38]. Some of the methods and tech-\nniques that have been employed in analysing big data streams are outlined in Table 8.\n\nTable 8 Methods and techniques for big data stream analysis\n\nMethods and techniques Article\n\nSPADE [132]\n\nLocally supervised metric learning (LSML) [133]\n\n*** [106]\n\nMultinomial latent dirichlet allocation [106]\n\nVoltage clustering algorithm [106]\n\nLocality sensitive hashing (LSH) [134]\n\nUser profile vector update algorithm [134]\n\nTag assignment stream clustering (TASC) [134]\n\nStreamMap [117]\n\nDensity cognition [117]\n\nQRS detection algorithm [87]\n\nForward chaining rule [110]\n\nStream [135]\n\nCluStream [136, 137]\n\nHPClustering [138]\n\nDenStream [139]\n\nD-Stream [140]\n\nACluStream [141]\n\nDCStream [142]\n\nP-Stream [143]\n\nADStream [144]\n\nContinuous query processing (CQR) [145]\n\nFPSPAN-growth [146]\n\nOutlier method for cloud computing algorithm (OMCA) [147]\n\nMulti-query optimization strategy (MQOS) [148]\n\nParallel K-means clustering [72]\n\nVisibly push down automata (VPA) [73]\n\nIncremental MI outlier detection algorithm (Inc I-MLOF) [149]\n\nAdaptive windowing based online ensemble (AWOE) [74]\n\nDynamic prime-number based security verification [84]\n\nK-anonymity, I-diversity, t-closeness [90]\n\nSingular spectrum matrix completion (SS-MC) [76]\n\nTemporal fuzzy concept analysis [96]\n\nECM-sketch [77]\n\nNearest neighbour [91]\n\nMarkov chains [91]\n\nBlock-QuickSort-AdjacentJobMatch [86]\n\nBlock-QuickSort-OverlapReplicate [86]\n\n*****-CSar-*** [150]\n\nWeighted online sequential extreme learning machine with kernels (WOS-ELMK) [22]\n\nConcept-adapting very fast decision tree (CVFDT) [151]\n\n\n\nPage 13 of ******** et al. J Big Data            (****) **** \n\nMany researchers have looked at the aspect of the real-time analysis of big data \nstreams but not much attention has been directed towards social media stream pre-\nprocessing. For instance, the social media stream is characterized by incomplete, noisy, \nslang, abbreviated words. Also, contextual meaning of social media post is essential for \nimproved event detection, sentiment analysis or any other social media analytics algo-\nrithms in terms of quality and accuracy [36, 39]. There is the need to give more atten-\ntion to the preprocessing stage of social media stream analysis in the face of incomplete, \nnoisy, slang, and abbreviated words that are pertinent to social media streams. These \nchallenges create opportunities application of new semantic technology approaches, \nwhich are more suited to social media streams [40, 41].\n\nResearch Question 3: What do big data streaming tools and technologies have in common \n\nand their differences in terms of concept, purpose, and capabilities?\n\nThe features of various tools and technologies for big data stream were compared in \norder to answer this question. An overview analysis based on ** dimensions, which are \ndatabase support, execution model, workload, fault-tolerance, latency, throughput, reli-\nability, operating system, implementation languages and application domain or areas is \npresented in Table 9.\n\nFor organisations with existing applications that have support for SQL, MySQL, SQL \nServer, Oracle Database, for instance, *** consider choosing big data streaming tools \nand technologies that have support for their existing databases. There are few big data \nstreaming tools and technology that support virtually any data format. An example of \nsuch is Infochimps Cloud.\n\nThe major big data streaming tools and technologies considered are all suitable for \nstreaming execution model, however out of 19 big data tools and technology compared \nand contrasted in this section, only 10.5% is suitable for streaming, batch, and iterative \nprocessing while 47.4% can handle jobs requiring both batch and streaming processing. \nIt is safer for a job to be executed on a single platform which can accommodate all the \ndependencies required in order to avoid interoperability constraints than combining \ntwo or more platforms or frameworks. The best fit with respect to the choice of big data \nstreaming tools and technologies will depend on the state of data to process, infrastruc-\nture preference, business use case, and kind of results interested in.\n\nVirtually all the big data streaming tools and technologies are memory intensive. This \nimplies that the main performance bottleneck at higher load conditions will be due to \nlack of memory [42]. However, research has shown that the benefit of high intensive \nmemory applications outweighs the performance loss due to long memory latency [43].\n\nFrom all the big data streaming tools and technologies reviewed, only ********-\nphere and TIBCO StreamBase support all of the three “at-most-once” “at-least-once” \nand “exactly-once” message delivery mechanisms while others support one or two of the \nthree delivery mechanisms. “At-most-once” is the cheapest with least implementation \noverhead and highest performance because it can be done in a fire-and-forget fashion \nwithout keeping the state in the transport mechanism or at the sending end. “At-least-\nonce” delivery requires multiple attempts in order to counter transport losses which \nmeans keeping the state at the sending end and having an acknowledgement mechanism \nat the receiving end. “Exactly-once” is the most expensive and has consequently worst \n\n\n\nPage 14 of ******** et al. J Big Data            (****) **** \n\nTa\nbl\n\ne \n9 \n\nCo\nm\n\npa\nri\n\nso\nn \n\nof\n b\n\nig\n d\n\nat\na \n\nst\nre\n\nam\nin\n\ng \nto\n\nol\ns \n\nan\nd \n\nte\nch\n\nno\nlo\n\ngi\nes\n\nTo\nol\n\ns \nan\n\nd \nte\n\nch\nno\n\nlo\ngy\n\nD\nat\n\nab\nas\n\ne \nsu\n\npp\nor\n\nt\nEx\n\nec\nut\n\nio\nn \n\nm\nod\n\nel\nW\n\nor\nkl\n\noa\nd\n\nFa\nul\n\nt t\nol\n\ner\nan\n\nce\nLa\n\nte\nnc\n\ny\nTh\n\nro\nug\n\nhp\nut\n\nRe\n***\n\nbi\nlit\n\ny\nO\n\npe\nra\n\ntin\ng \n\nsy\nst\n\nem\nIm\n\npl\nem\n\nen\nta\n\ntio\nn/\n\nsu\npp\n\nor\nte\n\nd \nla\n\nng\nua\n\nge\ns\n\nA\npp\n\nlic\nat\n\nio\nn\n\nBl\noc\n\nkM\non\n\nCa\nss\n\nan\ndr\n\na,\n M\n\non\n-\n\ngo\nD\n\nB,\n X\n\nM\nL\n\nSt\nre\n\nam\nin\n\ng\nM\n\nul\nti-\n\nsl\nic\n\ne \nm\n\nem\n-\n\nor\ny \n\nal\nlo\n\nca\ntio\n\nn \nan\n\nd \nba\n\ntc\nh \n\nal\nlo\n\nca\ntio\n\nns\n\nC\nhe\n\nck\npo\n\nin\nt, \n\nro\nllb\n\nac\nk\n\nVe\nry\n\n lo\nw\n\nH\nig\n\nh\nA\n\nt l\nea\n\nst\n o\n\nnc\ne\n\n**\nnu\n\nx\nC\n\n +\n+\n\n11\n, P\n\nyt\nho\n\nn\nA\n\nno\nm\n\nal\ny \n\nde\nte\n\nct\nio\n\nn,\n \n\nne\ntw\n\nor\nk \n\nop\ntim\n\niz\na-\n\ntio\nn,\n\n m\nul\n\ntim\ned\n\nia\n \n\nco\nnt\n\nen\nt d\n\nel\niv\n\ner\ny,\n\n \nfin\n\nan\nci\n\nal\n m\n\nar\nke\n\nt \nan\n\nal\nys\n\nis\n, w\n\neb\n \n\nan\nal\n\nyt\nic\n\ns\n\nSp\nar\n\nk \nSt\n\nre\nam\n\nin\ng\n\nKa\nfk\n\na,\n H\n\nBa\nse\n\n, \nH\n\niv\ne \n\nFl\num\n\ne,\n \n\nH\nD\n\nF/\nS3\n\n, \nKi\n\nne\nsi\n\ns, \nTC\n\nP \nso\n\nck\net\n\ns, \nTw\n\nit-\nte\n\nr, \nSQ\n\nL\n\nBa\ntc\n\nh,\n It\n\ner\nat\n\niv\ne,\n\n \nSt\n\nre\nam\n\nin\ng\n\nC\n**\n\n/m\nem\n\nor\ny \n\nin\nte\n\nns\niv\n\ne\nRD\n\nD\n b\n\nas\ned\n\n \nC\n\nhe\nck\n\n-p\noi\n\nnt\n-\n\nin\ng,\n\n p\nar\n\nal\nle\n\nl \nre\n\nco\nve\n\nry\n, \n\nre\npl\n\nic\nat\n\nio\nn\n\nLo\nw\n\nH\nig\n\nh\nEx\n\nac\ntly\n\n o\nnc\n\ne\nW\n\nin\ndo\n\nw\ns, \n\nm\nac\n\nO\nS,\n\n L\nin\n\nux\nSc\n\nal\na,\n\n P\nyt\n\nho\nn,\n\n \nJa\n\nva\n, R\n\nEv\nen\n\nt d\net\n\nec\ntio\n\nn,\n \n\nst\nre\n\nam\nin\n\ng \nm\n\nac\nhi\n\nne\n \n\nle\nar\n\nni\nng\n\n, f\nog\n\n c\nom\n\n-\npu\n\ntin\ng,\n\n in\nte\n\nra\nct\n\niv\ne \n\nan\nal\n\nys\nis\n\n, m\nul\n\ntim\ne-\n\ndi\na \n\nan\nal\n\nys\nis\n\n, c\nlu\n\nst\ner\n\n \nan\n\nal\nys\n\nis\n, fi\n\nlte\nrin\n\ng,\n \n\nre\n-p\n\nro\nce\n\nss\nin\n\ng,\n \n\nca\nch\n\ne \nin\n\nva\nlid\n\nat\nio\n\nn\n\nA\npa\n\nch\ne \n\nSt\nor\n\nm\nSp\n\nou\nt, \n\nH\nBa\n\nse\n, \n\nH\niv\n\ne,\n S\n\nQ\nL,\n\n \nCa\n\nss\nan\n\ndr\na,\n\n \nM\n\nem\nca\n\nch\ned\n\nSt\nre\n\nam\nin\n\ng\nC\n\n**\n/m\n\nem\nor\n\ny \nin\n\nte\nns\n\niv\ne\n\nRe\npl\n\nic\nat\n\nio\nn,\n\n \nch\n\nec\nkp\n\noi\nnt\n\n, \nda\n\nta\n re\n\nco\nve\n\nry\n, \n\nU\nps\n\ntr\nea\n\nm\n \n\nba\nck\n\nup\n, \n\nre\nco\n\nrd\n-le\n\nve\nl \n\nac\nkn\n\now\nle\n\ndg\ne-\n\nm\nen\n\nt, \nst\n\nat\nel\n\nes\ns \n\nm\nan\n\nag\nem\n\nen\nt\n\nVe\nry\n\n lo\nw\n\nLo\nw\n\nA\nt l\n\nea\nst\n\n o\nnc\n\ne\nW\n\nin\ndo\n\nw\ns, \n\nm\nac\n\nO\nS,\n\n L\nin\n\nux\nC\n\nlo\nju\n\nre\n, J\n\nav\na,\n\n S\nca\n\nla\n, \n\nC\nlo\n\nju\nre\n\n, n\non\n\n-J\nVM\n\n \nla\n\nng\nua\n\nge\ns\n\nIn\nte\n\nrn\net\n\n o\nf t\n\nhi\nng\n\ns, \nst\n\nre\nam\n\nin\ng \n\nm\nac\n\nhi\nne\n\n \nle\n\nar\nni\n\nng\n, m\n\nul\ntim\n\ne-\ndi\n\na \nan\n\nal\nys\n\nis\n\nYa\nho\n\no!\n S\n\n4\nM\n\nyS\nQ\n\nL,\n N\n\noS\nQ\n\nL,\n \n\nRi\nch\n\n D\nat\n\na \nFo\n\nrm\nat\n\nSt\nre\n\nam\nin\n\ng\nC\n\n**\n/m\n\nem\nor\n\ny \nin\n\nte\nns\n\niv\ne\n\nRe\npl\n\nic\nat\n\nio\nn,\n\n \nch\n\nec\nkp\n\noi\nnt\n\n, \nda\n\nta\n re\n\nco\nve\n\nry\n\nLo\nw\n\nLo\nw\n\nEx\nac\n\ntly\n o\n\nnc\ne\n\n**\nnu\n\nx\nJa\n\nva\n, P\n\nyt\nho\n\nn,\n C\n+\n+\n\n, \nPe\n\nrl\nO\n\nnl\nin\n\ne \nan\n\nal\nyt\n\nic\ns, \n\nm\non\n\nito\nrin\n\ng,\n fr\n\nau\nd \n\nde\nte\n\nct\nio\n\nn,\n fi\n\nna\nnc\n\nia\nl \n\nda\nta\n\n p\nro\n\nce\nss\n\nin\ng,\n\n \nw\n\neb\n p\n\ner\nso\n\nna\n***\n\na-\ntio\n\nn \nan\n\nd \nse\n\nss\nio\n\nn \nm\n\nod\nel\n\nlin\ng\n\n\n\nPage 15 of ******** et al. J Big Data            (****) **** \n\nTa\nbl\n\ne \n9 \n\n(c\non\n\nti\nnu\n\ned\n)\n\nTo\nol\n\ns \nan\n\nd \nte\n\nch\nno\n\nlo\ngy\n\nD\nat\n\nab\nas\n\ne \nsu\n\npp\nor\n\nt\nEx\n\nec\nut\n\nio\nn \n\nm\nod\n\nel\nW\n\nor\nkl\n\noa\nd\n\nFa\nul\n\nt t\nol\n\ner\nan\n\nce\nLa\n\nte\nnc\n\ny\nTh\n\nro\nug\n\nhp\nut\n\nRe\n***\n\nbi\nlit\n\ny\nO\n\npe\nra\n\ntin\ng \n\nsy\nst\n\nem\nIm\n\npl\nem\n\nen\nta\n\ntio\nn/\n\nsu\npp\n\nor\nte\n\nd \nla\n\nng\nua\n\nge\ns\n\nA\npp\n\nlic\nat\n\nio\nn\n\nA\npa\n\nch\ne \n\nSa\nm\n\nza\nKa\n\nfk\na,\n\n H\nD\n\n**\n, \n\nKi\nne\n\nsi\ns, \n\nSt\nre\n\nam\n \n\nco\nns\n\num\ner\n\n, K\ney\n\n-\nva\n\nlu\ne \n\nst\nor\n\nes\n\nSt\nre\n\nam\nin\n\ng,\n b\n\nat\nch\n\n \npr\n\noc\nes\n\nsi\nng\n\nM\nem\n\nor\ny \n\nin\nte\n\nn-\nsi\n\nve\nC\n\nhe\nck\n\npo\nin\n\nt\nVe\n\nry\n lo\n\nw\nH\n\nig\nh\n\nA\nt l\n\nea\nst\n\n o\nnc\n\ne\n**\n\nnu\nx,\n\n W\nin\n\ndo\nw\n\ns\nJa\n\nva\n, S\n\nca\nla\n\n, J\nVM\n\n \nla\n\nng\nua\n\nge\ns\n\nFi\nlte\n\nrin\ng,\n\n re\n-p\n\nro\n-\n\nce\nss\n\nin\ng,\n\n c\nac\n\nhe\n \n\nin\nva\n\nlid\nat\n\nio\nn\n\nA\npa\n\nch\ne \n\nFl\nin\n\nk\nKa\n\nfk\na,\n\n F\nlu\n\nm\ne,\n\n \nH\n\nD\nF/\n\nS3\n, \n\nKi\nne\n\nsi\ns, \n\nTC\nP \n\nso\nck\n\net\ns, \n\nTw\nit-\n\nte\nr, \n\nCa\nss\n\nan\ndr\n\na,\n \n\nRe\ndi\n\ns, \nM\n\non\n-\n\ngo\nD\n\nB,\n H\n\nBa\nse\n\n, \nSQ\n\nL\n\nSt\nre\n\nam\nin\n\ng,\n \n\nba\ntc\n\nh,\n it\n\ner\nat\n\niv\ne,\n\n \nin\n\nte\nra\n\nct\niv\n\ne\n\nM\nem\n\nor\ny \n\nin\nte\n\nn-\nsi\n\nve\nSt\n\nre\nam\n\n re\npl\n\nay\n \n\nan\nd \n\nm\nar\n\nke\nr-\n\nch\nec\n\nkp\noi\n\nnt\n\nVe\nry\n\n lo\nw\n\nH\nig\n\nh\nEx\n\nac\ntly\n\n o\nnc\n\ne\n**\n\nnu\nx,\n\n M\nac\n\nO\nS,\n\n \nW\n\nin\ndo\n\nw\ns\n\nJa\nva\n\n, S\nca\n\nla\n, P\n\nyt\nho\n\nn\nO\n\npt\nim\n\niz\nat\n\nio\nn \n\nof\n \n\ne-\nco\n\nm\nm\n\ner\nce\n\n \nse\n\nar\nch\n\n re\nsu\n\nlt,\n \n\nne\ntw\n\nor\nk/\n\nse\nns\n\nor\n \n\nm\non\n\nito\nrin\n\ng \nan\n\nd \ner\n\nro\nr d\n\net\nec\n\ntio\nn,\n\n \nET\n\nL \nfo\n\nr b\nus\n\nin\nes\n\ns \nin\n\nte\nlli\n\nge\nnc\n\ne \nin\n\nfra\n-\n\nst\nru\n\nct\nur\n\ne,\n m\n\nac\nhi\n\nne\n \n\nle\nar\n\nni\nng\n\nA\npa\n\nch\ne \n\nA\nur\n\nor\na\n\nH\n2,\n\n Ja\nva\n\n m\nap\n\ns, \nM\n\nyB\nat\n\nis\n, \n\nM\nyS\n\nQ\nL,\n\n P\nos\n\nt-\ngr\n\neS\nQ\n\nL\n\nSt\nre\n\nam\nin\n\ng\nM\n\nem\nor\n\ny \nan\n\nd \ndi\n\nsk\n s\n\npa\nce\n\nPe\nrio\n\ndi\nc \n\nre\nco\n\nv-\ner\n\ny \nch\n\nec\nkp\n\noi\nnt\n\n \nan\n\nd \nro\n\nllb\nac\n\nk\n\nLo\nw\n\nH\nig\n\nh\nA\n\nt l\nea\n\nst\n o\n\nnc\ne\n\n**\nnu\n\nx\nPy\n\nth\non\n\nM\non\n\nito\nrin\n\ng \nap\n\npl\nic\n\na-\ntio\n\nns\n s\n\nuc\nh \n\nas\n \n\nfin\nan\n\nci\nal\n\n a\nna\n\nly\nsi\n\ns \nan\n\nd \nm\n\nili\nta\n\nry\n a\n\npp\nli-\n\nca\ntio\n\nns\n\nRe\ndi\n\ns\nKe\n\ny-\nva\n\nlu\ne \n\nst\nor\n\nes\n, \n\nra\nbi\n\ntm\nq,\n\n M\non\n\n-\ngo\n\nD\nB\n\nSt\nre\n\nam\nin\n\ng\nIn\n\n-m\nem\n\nor\ny \n\nbu\nt \n\npe\nrs\n\nis\nte\n\nnt\n o\n\nn-\ndi\n\nsk\n d\n\nat\nab\n\nas\ne\n\nRe\npl\n\nic\na \n\nm\nig\n\nra\n-\n\ntio\nn,\n\n S\nen\n\ntin\nel\n\nLo\nw\n\nH\nig\n\nh\nA\n\nt l\nea\n\nst\n o\n\nnc\ne\n\nU\nbu\n\nnt\nu,\n\n L\nin\n\nux\n, \n\nO\nSX\n\nC\n, C\n\n#,\n Ja\n\nva\n, P\n\nH\nP, \n\nPy\nth\n\non\nW\n\neb\n a\n\nna\nly\n\nsi\ns, \n\nca\nch\n\ne,\n \n\nm\nes\n\nsa\nge\n\n q\nue\n\nue\ns\n\nC\n-S\n\nPA\n**\n\nL\nRD\n\nF, \nSQ\n\nLJ\n, \n\nN\noS\n\nQ\nL,\n\n H\nD\n\nF\nBa\n\ntc\nh,\n\n s\ntr\n\nea\nm\n\nin\ng\n\nLo\nw\n\n m\nem\n\nor\ny \n\nus\nag\n\ne\nA\n\nda\npt\n\nat\nio\n\nn\nVe\n\nry\n lo\n\nw\nH\n\nig\nh\n\nCu\nm\n\nul\nat\n\niv\ne\n\nW\nin\n\ndo\nw\n\ns, \n**\n\nnu\nx,\n\n M\nac\n\nO\nS,\n\n \nA\n\nnd\nro\n\nid\n\nJa\nva\n\n, A\npa\n\nch\ne \n\nJe\nna\n\n \nlib\n\nra\nrie\n\ns\nRe\n\nal\n-t\n\nim\ne \n\nre\nas\n\non\nin\n\ng \nov\n\ner\n s\n\nen\nso\n\nr d\nat\n\na,\n \n\nso\nci\n\nal\n s\n\nem\nan\n\ntic\n \n\nda\nta\n\n, u\nrb\n\nan\n c\n\nom\n-\n\npu\ntin\n\ng\n\nSA\nM\n\nO\nA\n\nH\nBa\n\nse\n, H\n\niv\ne,\n\n C\nas\n\n-\nsa\n\nnd\nra\n\nSt\nre\n\nam\nin\n\ng\nLo\n\nw\n m\n\nem\nor\n\ny \nus\n\nag\ne\n\nU\nps\n\ntr\nea\n\nm\n \n\nba\nck\n\nup\nLo\n\nw\nH\n\nig\nh\n\nEx\nac\n\ntly\n o\n\nnc\ne\n\n**\nnu\n\nx\nJa\n\nva\nC\n\nla\nss\n\n***\nca\n\ntio\nn,\n\n c\nlu\n\nst\ner\n\n-\nin\n\ng,\n s\n\npa\nm\n\n d\net\n\nec\n-\n\ntio\nn,\n\n re\ngr\n\nes\nsi\n\non\n, \n\nfre\nqu\n\nen\nt p\n\nat\nte\n\nrn\n \n\nm\nin\n\nin\ng\n\n\n\nPage 16 of ******** et al. J Big Data            (****) **** \n\nTa\nbl\n\ne \n9 \n\n(c\non\n\nti\nnu\n\ned\n)\n\nTo\nol\n\ns \nan\n\nd \nte\n\nch\nno\n\nlo\ngy\n\nD\nat\n\nab\nas\n\ne \nsu\n\npp\nor\n\nt\nEx\n\nec\nut\n\nio\nn \n\nm\nod\n\nel\nW\n\nor\nkl\n\noa\nd\n\nFa\nul\n\nt t\nol\n\ner\nan\n\nce\nLa\n\nte\nnc\n\ny\nTh\n\nro\nug\n\nhp\nut\n\nRe\n***\n\nbi\nlit\n\ny\nO\n\npe\nra\n\ntin\ng \n\nsy\nst\n\nem\nIm\n\npl\nem\n\nen\nta\n\ntio\nn/\n\nsu\npp\n\nor\nte\n\nd \nla\n\nng\nua\n\nge\ns\n\nA\npp\n\nlic\nat\n\nio\nn\n\n**\nEL\n\nS\nRD\n\nF, \nSQ\n\nLJ\n, \n\nN\noS\n\nQ\nL,\n\n H\nD\n\nF\nBa\n\ntc\nh,\n\n s\ntr\n\nea\nm\n\nin\ng\n\nIn\n-m\n\nem\nor\n\ny\nA\n\nda\npt\n\nat\nio\n\nn\nLo\n\nw\nH\n\nig\nh\n\nCu\nm\n\nul\nat\n\niv\ne\n\nW\nin\n\ndo\nw\n\ns, \n**\n\nnu\nx,\n\n M\nac\n\nO\nS,\n\n \nA\n\nnd\nro\n\nid\n\nJa\nva\n\nRe\nal\n\n-t\nim\n\ne \nre\n\nas\non\n\nin\ng \n\nov\ner\n\n s\nen\n\nso\nr d\n\nat\na,\n\n \nso\n\nci\nal\n\n s\nem\n\nan\ntic\n\n \nda\n\nta\n, u\n\nrb\nan\n\n c\nom\n\n-\npu\n\ntin\ng\n\nET\nA\n\nLI\nS\n\nRD\nF\n\nSt\nre\n\nam\nin\n\ng\nBi\n\nna\nriz\n\nat\nio\n\nn\nA\n\nda\npt\n\nat\nio\n\nn\nLo\n\nw\nLo\n\nw\nCu\n\nm\nul\n\nat\niv\n\ne\nW\n\nin\ndo\n\nw\ns, \n\n**\nnu\n\nx,\n M\n\nac\nO\n\nS,\n \n\nA\nnd\n\nro\nid\n\nPr\nol\n\nog\n, J\n\nav\na,\n\n C\n, \n\nSP\nA\n\n**\nL,\n\n C\n#,\n\n \nET\n\nA\nLI\n\nS \nLa\n\nng\nua\n\nge\n \n\nfo\nr E\n\nve\nnt\n\ns \n(E\n\nLE\n)\n\nEv\nen\n\nt d\net\n\nec\ntio\n\nn,\n \n\nre\nas\n\non\nin\n\ng \nov\n\ner\n \n\nst\nre\n\nam\nin\n\ng \nev\n\nen\nts\n\nXS\nEQ\n\nXM\nL\n\nBa\ntc\n\nh,\n s\n\ntr\nea\n\nm\nin\n\ng\nIn\n\n-m\nem\n\nor\ny \n\nw\nith\n\n \nbu\n\nffe\nrin\n\ng\nch\n\nec\nkp\n\noi\nnt\n\nLo\nw\n\nH\nig\n\nh\nA\n\nt l\nea\n\nst\n o\n\nnc\ne\n\nW\nin\n\ndo\nw\n\ns, \n**\n\nnu\nx\n\nJa\nva\n\n, A\npa\n\nch\ne \n\nXe\nrc\n\nes\nBi\n\nol\nog\n\nic\nal\n\n d\nat\n\na,\n s\n\noc\nia\n\nl \nne\n\ntw\nor\n\nks\n, u\n\nse\nr \n\nbe\nha\n\nvi\nou\n\nr, \nfin\n\nan\nci\n\nal\n \n\nda\nta\n\n a\nna\n\nly\nsi\n\ns, \nfil\n\nte\nrin\n\ng\n\n**\nM\n\n In\nfo\n\nSp\nhe\n\nre\n \n\nst\nre\n\nam\ns\n\nPi\ng,\n\n H\niv\n\ne,\n Ja\n\nql\n, \n\nH\nBa\n\nse\n F\n\nlu\nm\n\ne,\n \n\nLu\nce\n\nne\n, A\n\nvr\no,\n\n \nZo\n\noK\nee\n\npe\nr, \n\nO\noz\n\nie\n, O\n\nra\ncl\n\ne \nD\n\nat\nab\n\nas\ne,\n\n \nD\n\nB2\n, N\n\net\nez\n\nza\n, \n\nM\nyS\n\nQ\nL,\n\n A\nst\n\ner\n, \n\nIn\nfo\n\nrm\nix\n\n.\n\nSt\nre\n\nam\nin\n\ng\nCa\n\npt\nur\n\ne \nda\n\nta\nba\n\nse\n \n\nw\nor\n\nkl\noa\n\nds\n a\n\nnd\n \n\nre\npl\n\nay\n th\n\nem\n in\n\n \na \n\nte\nst\n\n d\nat\n\nab\nas\n\ne \nen\n\nvi\nro\n\nnm\nen\n\nt\n\nA\nut\n\nom\nat\n\nic\n \n\nre\nco\n\nve\nry\n\nLo\nw\n\nH\nig\n\nh\nEx\n\nac\ntly\n\n o\nnc\n\ne,\n \n\nA\nt l\n\nea\nst\n\n \non\n\nce\n, A\n\nt \nm\n\nos\nt o\n\nnc\ne\n\n**\nnu\n\nx,\n C\n\nen\ntO\n\nS\nC\n\n +\n+\n\nJa\nva\n\nSP\nL\n\nSp\nac\n\ne \nw\n\nea\nth\n\ner\n p\n\nre\n-\n\ndi\nct\n\nio\nn,\n\n p\nhy\n\nsi\nol\n\nog\ni-\n\nca\nl d\n\nat\na \n\nst\nre\n\nam\ns \n\nan\nal\n\nys\nis\n\n, t\nra\n\nffi\nc \n\nm\nan\n\nag\nem\n\nen\nt, \n\nre\nal\n\n-\ntim\n\ne \npr\n\ned\nic\n\ntio\nns\n\n, \nev\n\nen\nt d\n\net\nec\n\ntio\nn,\n\n \nvi\n\nsu\nal\n\nis\nat\n\nio\nn\n\nG\noo\n\ngl\ne \n\nM\nill\n\n-\nW\n\nhe\nel\n\nBi\ngT\n\nab\nle\n\n, S\npa\n\nn-\nne\n\nr\nSt\n\nre\nam\n\nin\ng\n\nIn\n-m\n\nem\nor\n\ny \nan\n\nd \nbl\n\noo\nm\n\n fi\nlte\n\nrin\ng\n\nU\nnc\n\noo\nrd\n\nin\nat\n\ned\n \n\npe\nrio\n\ndi\nc,\n\n \nch\n\nec\nkp\n\noi\nnt\n\n, \nup\n\nst\nre\n\nam\n \n\nba\nck\n\nup\n\nLo\nw\n\nH\nig\n\nh\nEx\n\nac\ntly\n\n o\nnc\n\ne\n**\n\nnu\nx\n\nVi\nrt\n\nua\nlly\n\n a\nny\n\n \npr\n\nog\nra\n\nm\nm\n\nin\ng \n\nla\nng\n\nua\nge\n\nA\nno\n\nm\nal\n\ny \nde\n\nte\nct\n\nio\nn,\n\n \nhe\n\nal\nth\n\n m\non\n\nito\nrin\n\ng,\n \n\nim\nag\n\ne \npr\n\noc\nes\n\nsi\nng\n\n, \nne\n\ntw\nor\n\nk \nsw\n\nitc\nh \n\nm\nan\n\nag\nem\n\nen\nt\n\n\n\nPage 17 of ******** et al. J Big Data            (****) **** \n\nTa\nbl\n\ne \n9 \n\n(c\non\n\nti\nnu\n\ned\n)\n\nTo\nol\n\ns \nan\n\nd \nte\n\nch\nno\n\nlo\ngy\n\nD\nat\n\nab\nas\n\ne \nsu\n\npp\nor\n\nt\nEx\n\nec\nut\n\nio\nn \n\nm\nod\n\nel\nW\n\nor\nkl\n\noa\nd\n\nFa\nul\n\nt t\nol\n\ner\nan\n\nce\nLa\n\nte\nnc\n\ny\nTh\n\nro\nug\n\nhp\nut\n\nRe\n***\n\nbi\nlit\n\ny\nO\n\npe\nra\n\ntin\ng \n\nsy\nst\n\nem\nIm\n\npl\nem\n\nen\nta\n\ntio\nn/\n\nsu\npp\n\nor\nte\n\nd \nla\n\nng\nua\n\nge\ns\n\nA\npp\n\nlic\nat\n\nio\nn\n\nIn\nfo\n\nch\nim\n\nps\n \n\ncl\nou\n\nd\nSQ\n\nL,\n N\n\noS\nQ\n\nL,\n \n\nH\niv\n\ne,\n P\n\nig\n \n\nW\nuk\n\non\ng,\n\n \nH\n\nad\noo\n\np,\n \n\nRD\n**\n\nS,\n V\n\nirt\nu-\n\nal\nly\n\n a\nny\n\n d\nat\n\na \nfo\n\nrm\nat\n\nBa\ntc\n\nh,\n s\n\ntr\nea\n\nm\nin\n\ng\nIn\n\n-m\nem\n\nor\ny\n\nU\nps\n\ntr\nea\n\nm\n \n\nba\nck\n\nup\nLo\n\nw\nH\n\nig\nh\n\nEx\nac\n\ntly\n o\n\nnc\ne\n\n**\nnu\n\nx\nJa\n\nva\nD\n\nis\nas\n\nte\nr d\n\nis\nco\n\nve\nry\n\n, \nte\n\nxt\n a\n\nna\nly\n\nsi\ns, \n\nco\nm\n\n-\npl\n\nex\n e\n\nve\nnt\n\n p\nro\n\nce\nss\n\n-\nin\n\ng,\n v\n\nis\nua\n\nlis\nat\n\nio\nn\n\nM\nic\n\nro\nso\n\nft\n \n\nSt\nre\n\nam\nIn\n\nsi\ngh\n\nt\nSQ\n\nL \nSe\n\nrv\ner\n\nSt\nre\n\nam\nin\n\ng\nIn\n\n-m\nem\n\nor\ny\n\nRe\npl\n\nic\nat\n\nio\nn,\n\n \nch\n\nec\nkp\n\noi\nnt\n\n, \nda\n\nta\n re\n\nco\nve\n\nry\n\nVe\nry\n\n lo\nw\n\nH\nig\n\nh\nEx\n\nac\ntly\n\n o\nnc\n\ne\nW\n\nin\ndo\n\nw\ns\n\n.N\nET\n\n, C\n#,\n\n L\nIN\n\nQ\n, R\n\nx\nM\n\nan\nuf\n\nac\ntu\n\nrin\ng \n\npr\noc\n\nes\ns \n\nm\non\n\nito\nr-\n\nin\ng \n\nan\nd \n\nco\nnt\n\nro\nl, \n\nfin\nan\n\nci\nal\n\n d\nat\n\na \nan\n\nal\nys\n\nis\n, o\n\npe\nra\n\n-\ntio\n\nn \nan\n\nal\nyt\n\nic\ns, \n\nw\neb\n\n \nan\n\nal\nyt\n\nic\ns, \n\nev\nen\n\nt \npa\n\ntt\ner\n\nn \nde\n\nte\nct\n\nio\nn\n\nTI\nBC\n\nO\n S\n\ntr\nea\n\nm\n-\n\nBa\nse\n\nO\nra\n\ncl\ne \n\nda\nta\n\nba\nse\n\n, \nSQ\n\nL \nSe\n\nrv\ner\n\n, \nIm\n\npa\nla\n\nBa\ntc\n\nh,\n S\n\ntr\nea\n\nm\nin\n\ng\nIn\n\n-m\nem\n\nor\ny\n\nSy\nnc\n\nhr\non\n\niz\nat\n\nio\nn,\n\n \nre\n\npl\nic\n\nat\nio\n\nn,\n \n\nro\nllb\n\nac\nk\n\nVe\nry\n\n lo\nw\n\nH\nig\n\nh\nA\n\nt l\nea\n\nst\n o\n\nnc\ne/\n\nat\n m\n\nos\nt \n\non\nce\n\n/\nex\n\nac\ntly\n\n o\nnc\n\ne\n\nW\nin\n\ndo\nw\n\ns, \nM\n\nac\nO\n\nS,\n L\n\nin\nux\n\nR,\n Ja\n\nva\nM\n\nis\nsi\n\non\n c\n\nrit\nic\n\nal\n \n\nan\nal\n\nys\nis\n\n, I\noT\n\n a\nna\n\nly\n-\n\nsi\ns, \n\ncl\nic\n\nk-\nst\n\nre\nam\n\n \nan\n\nal\nys\n\nis\n, p\n\nre\ndi\n\nct\niv\n\ne \nan\n\nal\nyt\n\nic\ns, \n\nw\nor\n\nkfl\now\n\n \nop\n\ntim\niz\n\nat\nio\n\nn,\n ri\n\nsk\n \n\nav\noi\n\nda\nnc\n\ne\n\nLa\nm\n\nbd\na \n\nA\nrc\n\nhi\n-\n\nte\nct\n\nur\ne\n\nRD\n**\n\nS,\n C\n\nas\nsa\n\nn-\ndr\n\na,\n K\n\naf\nka\n\n, D\nat\n\na \nW\n\nar\neh\n\nou\nse\n\ns, \nKi\n\nne\nsi\n\ns \nD\n\nat\na \n\nSt\nre\n\nam\n, H\n\nD\n**\n\n, \nH\n\nBa\nse\n\nBa\ntc\n\nh,\n S\n\ntr\nea\n\nm\nin\n\ng\nIn\n\n-m\nem\n\nor\ny/\n\ndi\nsk\n\n \nda\n\nta\nba\n\nse\nRe\n\npl\nic\n\nat\nio\n\nn,\n \n\nch\nec\n\nkp\noi\n\nnt\nLo\n\nw\nLo\n\nw\nEx\n\nac\ntly\n\n o\nnc\n\ne\nU\n\nbu\nnt\n\nu,\n W\n\nin\n-\n\ndo\nw\n\ns, \n**\n\nnu\nx\n\nJa\nva\n\n, C\n#,\n\n P\nyt\n\nho\nn,\n\n \nPi\n\ng \nLa\n\ntin\nIo\n\nT \nan\n\nal\nys\n\nis\n, t\n\nra\nck\n\nin\ng \n\nre\nal\n\n-t\nim\n\ne \nup\n\nda\nte\n\ns, \nfin\n\nan\nci\n\nal\n ri\n\nsk\n m\n\nan\n-\n\nag\nem\n\nen\nt, \n\ncl\nic\n\nk-\nst\n\nre\nam\n\n a\nna\n\nly\nsi\n\ns\n\n\n\nPage 18 of ******** et al. J Big Data            (****) **** \n\nperformance because, in addition to “at-least-once” delivery mechanism, it requires the \nstate to be kept at the receiving end in order to filter duplicate deliveries. In other words, \n“at-most-once” delivery mechanism implies that the message may be lost while “at-\nleast-once” delivery ensures that messages are not lost and “exactly-once” implies that \nmessage can neither be lost nor duplicated. “Exactly-once” is suitable for many critical \nsystems where duplicate messages are unacceptable.\n\nResearch Question 4: What are the limitations and strengths of big data streaming tools \n\nand technologies?\n\nObservations from the literature reveal that specific big data streaming technology *** \nnot provide the full set of features that are required. It is rare to find specific big data \ntechnology that combines key features such as scalability, integration, fault-tolerance, \ntimeliness, consistency, heterogeneity and incompleteness management, and load bal-\nancing. For instance, Spark streaming [16] and Sonora [44] are excellent and efficient \nfor checkpointing but the ******** space available to **** codes are limited. S4 does not \nguarantee 100% fault-tolerant persistent state [45]. Storm does not guarantee the order-\ning of messages due to its “at-least-once” mechanism for record delivery [46, 47]. Strict \ntransaction ordering is required by ******* to operate [48]. While streaming SQL pro-\nvide simple and succinct solutions to many streaming problems, the complex application \nlogic (such as matrix multiplication) and intuitive state abstractions are expressed with \nthe operational flow of an imperative language rather than a declarative language such as \nSQL [49–51].\n\nMoreover, BlockMon uses batches and cache locality optimization techniques for \nmemory allocation efficiency and data speed up access. However, deadlock *** occur \nif data streams are enqueued with a higher rate than that of the block consumption [52]. \nApache Samza solves batch latency processing problems but requires an added layer for \nflow control [53]. Flink is suitable for heavy stream processing and batch-oriented tasks \nalthough it has scaling limitations [46]. *****’ in-memory data store makes it extremely \nfast although this implies that available memory size determines the size of th",
      "merged_content": "\nBig data stream analysis: a systematic \nliterature review\nTaiwo Kolajo1,2* , Olawande Daramola3  and Ayodele Adebiyi1,4 \n\nIntroduction\nAdvances in information technology have facilitated large volume, high-velocity of data, \nand the ability to store data continuously leading to several computational challenges. \nDue to the nature of big data in terms of volume, velocity, variety, variability, veracity, \nvolatility, and value [1] that are being generated recently, big data computing is a new \ntrend for future computing.\n\nBig data computing can be generally categorized into two types based on the process-\ning requirements, which are big data batch computing and big data stream computing \n\nAbstract \n\nRecently, big data streams have become ubiquitous due to the fact that a number of \napplications generate a huge amount of data at a great velocity. This made it difficult \nfor existing data mining tools, technologies, methods, and techniques to be applied \ndirectly on big data streams due to the inherent dynamic characteristics of big data. In \nthis paper, a systematic review of big data streams analysis which employed a rigorous \nand methodical approach to look at the trends of big data stream tools and technolo-\ngies as well as methods and techniques employed in analysing big data streams. It \nprovides a global view of big data stream tools and technologies and its comparisons. \nThree major databases, Scopus, ScienceDirect and EBSCO, which indexes journals and \nconferences that are promoted by entities such as IEEE, ACM, SpringerLink, and Elsevier \nwere explored as data sources. Out of the initial 2295 papers that resulted from the \nfirst search string, 47 papers were found to be relevant to our research questions after \nimplementing the inclusion and exclusion criteria. The study found that scalability, \nprivacy and load balancing issues as well as empirical analysis of big data streams and \ntechnologies are still open for further research efforts. We also found that although, sig-\nnificant research efforts have been directed to real-time analysis of big data stream not \nmuch attention has been given to the preprocessing stage of big data streams. Only a \nfew big data streaming tools and technologies can do all of the batch, streaming, and \niterative jobs; there seems to be no big data tool and technology that offers all the key \nfeatures required for now and standard benchmark dataset for big data streaming ana-\nlytics has not been widely adopted. In conclusion, it was recommended that research \nefforts should be geared towards developing scalable frameworks and algorithms that \nwill accommodate data stream computing mode, effective resource allocation strategy \nand parallelization issues to cope with the ever-growing size and complexity of data.\n\nKeywords: Big data stream analysis, Stream computing, Big data streaming tools and \ntechnologies\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nSURVEY PAPER\n\nKolajo et al. J Big Data            (2019) 6:47  \nhttps://doi.org/10.1186/s40537-019-0210-7\n\n*Correspondence:   \ntaiwo.kolajo@stu.cu.edu.ng; \ntaiwo.kolajo@fulokoja.edu.ng \n1 Department of Computer \nand Information Sciences, \nCovenant University, Ota, \nNigeria\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0001-6780-2495\nhttp://orcid.org/0000-0001-6340-078X\nhttp://orcid.org/0000-0002-3114-6315\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-019-0210-7&domain=pdf\n\n\nPage 2 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\n[2]. Big data batch processing is not sufficient when it comes to analysing real-time \napplication scenarios. Most of the data generated in a real-time data stream need real-\ntime data analysis. In addition, the output must be generated with low-latency and any \nincoming data must be reflected in the newly generated output within seconds. This \nnecessitates big data stream analysis [3].\n\nThe demand for stream processing is increasing. The reason being not only that huge \nvolume of data need to be processed but that data must be speedily processed so that \norganisations or businesses can react to changing conditions in real-time.\n\nThis paper presents a systematic review of big data stream analysis. The purpose is to \npresent an overview of research works, findings, as well as implications for research and \npractice. This is necessary to (1) provide an update about the state of research, (2) iden-\ntify areas that are well researched, (3) showcase areas that are lacking and need further \nresearch, and (4) build a common understanding of the challenges that exist for the ben-\nefit of the scientific community.\n\nThe rest of the paper is organized as follows: “Background and related work” section \nprovides information on stream computing and big data stream analysis and the key \nissues involved in it and presents a review on big data streaming analytics. In “Research \nmethod” section, the adopted research methodology is discussed, while “Result” section \npresents the findings of the study. “Discussion” section presents a detailed evaluation \nperformed on big data stream analysis, “Limitation of the review” section highlights the \nlimitations of the study, while “Conclusion and further work” concludes the paper.\n\nBackground and related work\nStream computing\n\nStream computing refers to the processing of massive amount of data generated at high-\nvelocity from multiple sources with low latency in real-time. It is a new paradigm neces-\nsitated because of new sources of data generating scenarios which include ubiquity of \nlocation services, mobile devices, and sensor pervasiveness [4]. It can be applied to the \nhigh-velocity flow of data from real-time sources such as the Internet of Things, Sensors, \nmarket data, mobile, and clickstream.\n\nThe fundamental assumption of this paradigm is that the potential value of data lies in \nits freshness. As a result, data are analysed as soon as they arrive in a stream to produce \nresult as opposed to what obtains in batch computing where data are first stored before \nthey are analysed. There is a crucial need for parallel architectures and scalable com-\nputing platforms [5]. With stream computing, organisations can analyse and respond in \nreal-time to rapidly changing data. Streaming processing frameworks include Storm, S4, \nKafka, and Spark [6–8]. The real contrasts between the batch processing and the stream \nprocessing paradigms are outlined in Table 1.\n\nIncorporating streaming data into decision-making process necessitates a program-\nming paradigm called stream computing. With stream computing, fairly static questions \ncan be evaluated on data in motion (i.e. real-time data) continuously [9].\n\nBig data stream analysis\n\nThe essence of big data streaming analytics is the need to analyse and respond to real-\ntime streaming data using continuous queries so that it is possible to continuously \n\n\n\nPage 3 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nperform analysis on the fly within the stream. Stream processing solutions must be \nable to handle a real-time, high volume of data from diverse sources putting into con-\nsideration availability, scalability and fault tolerance. Big data stream analysis involves \nassimilation of data as an infinite tuple, analysis and production of actionable results \nusually in a form of stream [10].\n\nIn a stream processor, applications are represented as data flow graph made up of \noperations and interconnected streams as depicted in Fig. 1. In a streaming analytics \nsystem, application comes in a form of continuous queries, data are ingested continu-\nously, analysed and correlated, and stream of results are generated. Streaming analytic \napplications is usually a set of operators connected by streams. Streaming analytics \nsystems must be able to identify new information, incrementally build models and \naccess whether the new incoming data deviate from model predictions [9].\n\nThe idea of streaming analytics is that each of the received data tuples is processed \nin the data processing node. Such processing includes removing duplicates, filling \nmissing data, data normalization, parsing, feature extraction, which are typically done \nin a single pass due to the high data rates of external feeds. When a new tuple arrives, \nthis node is triggered, and it expels tuples older than the time specified in the sliding \nwindow (sliding window is a typical example of windows used in stream computing \nwhich keeps only the latest tuples up to the time specified in the windows). A window \n\nTable 1 Comparison between batch processing and streaming processing [82]\n\nDimension Batch processing Streaming processing\n\nInput Data chunks Stream of new data or updates\n\nData size Known and finite Infinite or unknown in advance\n\nHardware Multiple CPUs Typical single limited amount of memory\n\nStorage Store Not store or store non-trivial portion in memory\n\nProcessing Processed in multiple rounds A single or few passes over data\n\nTime Much longer A few seconds or even milliseconds\n\nApplications Widely adopted in almost every domain Web mining, traffic monitoring, sensor networks\n\nFig. 1 Data flow graph of a stream processor. The figure shows how applications (made up of operations and \ninterconnected streams) are represented as data flow graph in a stream processor [10]\n\n operator stream NYMEX \n\n\n\nPage 4 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nis referred to as a logical container for data tuples received. It defines how frequently \ndata is refreshed in the container as well as when data processing is triggered [4].\n\nKey issues in big data stream analysis\n\nBig data stream analysis is relevant when there is a need to obtain useful knowledge \nfrom current happenings in an efficient and speedy manner in order to enable organisa-\ntions to quickly react to problems, or detect new trends which can help improve their \nperformance. However, there are some challenges such as scalability, integration, fault-\ntolerance, timeliness, consistency, heterogeneity and incompleteness, load balancing, \nprivacy issues, and accuracy [3, 11–18] which arises from the nature of big data streams \nthat must be dealt with.\n\nScalability\n\nOne of the main challenges in big data streaming analysis is the issue of scalability. The \nbig data stream is experiencing exponential growth in a way much faster than computer \nresources. The processors follow Moore’s law, but the size of data is exploding. There-\nfore, research efforts should be geared towards developing scalable frameworks and \nalgorithms that will accommodate data stream computing mode, effective resource allo-\ncation strategy and parallelization issues to cope with the ever-growing size and com-\nplexity of data.\n\nIntegration\n\nBuilding a distributed system where each node has a view of the data flow, that is, every \nnode performing analysis with a small number of sources, then aggregating these views \nto build a global view is non-trivial. An integration technique should be designed to ena-\nble efficient operations across different datasets.\n\nFault‑tolerance\n\nHigh fault-tolerance is required in life-critical systems. As data is real-time and infinite \nin big data stream computing environments, a good scalable high fault-tolerance strat-\negy is required that allows an application to continue working despite component failure \nwithout interruption.\n\nTimeliness\n\nTime is of the essence for time-sensitive processes such as mitigating security threats, \nthwarting fraud, or responding to a natural disaster. There is a need for scalable architec-\ntures or platforms that will enable continuous processing of data streams which can be \nused to maximize the timeliness of data. The main challenge is implementing a distrib-\nuted architecture that will aggregate local views of data into global view with minimal \nlatency between communicating nodes.\n\nConsistency\n\nAchieving high consistency (i.e. stability) in big data stream computing environments is \nnon-trivial as it is difficult to determine which data are needed and which nodes should \nbe consistent. Hence a good system structure is required.\n\n\n\nPage 5 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nHeterogeneity and incompleteness\n\nBig data streams are heterogeneous in structure, organisations, semantics, accessi-\nbility and granularity. The challenge here is how to handle an always ever-increas-\ning data, extract meaningful content out of it, aggregate and correlate streaming \ndata from multiple sources in real-time. A competent data presentation should be \ndesigned to reflect the structure, diversity and hierarchy of the streaming data.\n\nLoad balancing\n\nA big data stream computing system is expected to be self-adaptive to data streams \nchanges and avoid load shedding. This is challenging as dedicating resources to cover \npeak loads 24/7 is impossible and load shedding is not feasible when the variance \nbetween the average load and the peak load is high. As a result, a distributing envi-\nronment that automatically streams partial data streams to a global centre when local \nresources become insufficient is required.\n\nHigh throughput\n\nDecision with respect to identifying the sub-graph that needs replication, how many \nreplicas are needed and the portion of the data stream to assign to each replica is an \nissue in big data stream computing environment. There is a need for good multiple \ninstances replication if high throughput is to be achieved.\n\nPrivacy\n\nBig data stream analytics created opportunities for analyzing a huge amount of data \nin real-time but also created a big threat to individual privacy. According to the Inter-\nnational Data Cooperation (IDC), not more than half of the entire information that \nneeds protection is effectively protected. The main challenge is proposing techniques \nfor protecting a big data stream dataset before its analysis.\n\nAccuracy\n\nOne of the main objectives of big data stream analysis is to develop effective tech-\nniques that can accurately predict future observations. However, as a result of inher-\nent characteristics of big data such as volume, velocity, variety, variability, veracity, \nvolatility, and value, big data analysis strongly constrain processing algorithms spatio-\ntemporally and hence stream-specific requirements must be taken into consideration \nto ensure high accuracy.\n\nRelated work\n\nThis section discusses some of the previous research efforts that relate to big data \nstreaming analytics.\n\nThe work of [13] presented a review of various tools, technologies and methods \nfor big data analytics by categorizing big data analytics literature according to their \nresearch focus. This paper is different in that it presents a systematic literature review \nthat focused on big data “streaming” analytics.\n\n\n\nPage 6 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nAuthors in [19] presented a systematic review of big data analytics in e-commerce. The \nstudy explored characteristics, definitions, business values, types and challenges of big \ndata analytics in the e-commerce landscape. Likewise, [20] conducted a study that is cen-\ntred on big data analytics in technology and organisational resource management specifi-\ncally focusing on reviews that present big data challenges and big data analytics methods. \nAlthough they are systematic reviews, the focus is not, particularly on big data streaming.\n\nAuthors in [21] presented the status of empirical research and application areas in big \ndata by employing a systematic mapping method. In the same vein, authors in [22] also \nconducted a survey on big data technologies and machine learning algorithms with a \nparticular focus on anomaly detection. A systematic review of literature which aims to \ndetermine the scope, application, and challenges of big data analytics in healthcare was \npresented by [23]. The work of [2] presented a review of four big data streaming tools \nand technologies. While the study conducted in this paper provided a comprehensive \nreview of not only big data streaming tools and technologies but also methods and tech-\nniques employed in analyzing big data streams. In addition, authors [2] did not provide a \nclear explanation of the methodical approach for selecting the reviewed papers.\n\nResearch method\nThe study was grounded in a systematic literature review of tools and technologies \nwith methods and techniques used in analysing big data streams by adopting [24, 25] as \nmodels.\n\nResearch question\n\nThe study tries to answer the following research questions:\n\nResearch Question 1: What are the tools and technologies employed for big data \nstream analysis?\nResearch Question 2: What methods and techniques are used in analysing big data \nstreams?\nResearch Question 3: What do these tools and technologies have in common and \ntheir differences in terms of concept, purpose and capabilities?\nResearch Question 4: What are the limitations and strengths of these tools and tech-\nnologies?\nResearch Question 5: What are the evaluation techniques or benchmarks used for \nevaluating big data streaming tools and technology?\n\nSearch string\n\nCreating a good search string requires structuring in terms of population, compari-\nson, intervention and outcome [24]. Relevant publications were identified by forming \na search string that combined keywords driven by the research questions earlier stated. \nThe searches were conducted by employing three standard database indexes, which are \nScopus, Science Direct and EBSCOhost. The search string is “big data stream analysis” \nOR “big data stream technologies” OR “big data stream framework” OR “big data stream \nalgorithms” OR “big data stream analysis tools” OR “big data stream processing” OR “big \n\n\n\nPage 7 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\ndata stream analysis reviews” OR “big data stream literature review” OR “big data stream \nanalytics”.\n\nData sources\n\nAs research becomes increasingly interdisciplinary, global and collaborative, it is expedi-\nent to select from rich and standard databases. The databases consulted are as follows:\n\n i. Scopus1: Scopus is a bibliographic database containing abstracts and citations for \nacademic journal articles launched in 2004. It covers nearly 36,377 titles from over \n11,678 publishers of which 34,346 are peer-reviewed journals, delivering a compre-\nhensive overview of the world’s research output in the scientific, technical, medi-\ncal, and social sciences (including arts and humanities). It is the largest abstract \nand citation database of peer-reviewed literature.\n\n ii. ScienceDirect2: ScienceDirect is Elsevier’s leading information solution for \nresearchers, students, teachers, information professionals and healthcare profes-\nsionals. It provides both subscription-based and open access-based to a large data-\nbase combining authoritative, full-text scientific, technical and health publications \nwith smart intuitive functionality. It covers over 14 million publications from over \n3800 journals and more than 35,000 books. The journals are grouped into four \ncategories: Life Sciences, Physical Sciences and Engineering, Health Sciences, and \nSocial Sciences and Humanities.\n\n iii. EBSCOhost3: EBSCOhost covers a wide range of bibliographic and full-text data-\nbases for researchers, providing electronic journal service available to both cor-\nporate and academic researchers. It has a total of 16,711 journals and magazine \nindexed and abstracted of which 14,914 are peer-reviewed; more than 900,000 \nhigh-quality e-books and titles and over 60,000 audiobooks from more than 1500 \nmajor academic publishers.\n\n iv. ResearchGate4: A free online professional network for scientists and researchers to \nask and answer questions, share papers and find collaborators. It covers over 100 \nmillion publications from over 11 million researchers. ResearchGate was used as \na secondary source where the authors could not access some papers due to lack of \nsubscription.\n\nData retrieval\n\nThe search was conducted in Scopus, ScienceDirect and EBSCOhost since most of \nthe high impact journals and conferences are indexed in these set of rich databases. \nBoolean ‘OR’ was used in combining the nine (9) search strings. A total of 2295 arti-\ncles from the three databases were retrieved as shown in Table 2.\n\n1 http://www.scopu s.com.\n2 http://www.scien cedir ect.com.\n3 https ://www.ebsco host.com.\n4 https ://www.resea archg ate.net.\n\nhttp://www.scopus.com\nhttp://www.sciencedirect.com\nhttps://www.ebscohost.com\nhttps://www.reseaarchgate.net\n\n\nPage 8 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nFurther refinement was performed by (i) limiting the search to journals and confer-\nence papers; (ii) selecting computer science and IT related as the subject domain; (iii) \nselecting ACM, IEEE, SpringerLink, Elsevier as sources; and year of publication to \nbetween 2004 and 2018. The year range was selected due to the fact that interest in \nbig data stream analysis actually started in 2004. At this stage, a total of 1989 papers \nwere excluded leaving a total of 315 papers (see Table  3). The result of the search \nstring was exported to PDF.\n\nBy going through the title of the papers, 111 seemingly relevant papers were extracted \nexcluding a total number of 213 that were not relevant at this stage (see Table 4).\n\nThe abstracts of 111 papers and introduction (for papers that the abstracts were not \nclear enough) were then read to have a quick overview of the paper and to ascertain \nwhether they are suitable or at variance with the research questions. The citations of \nthe papers were exported to Microsoft Excel for easy analysis. The papers were grouped \ninto three categories; “relevant”, “may be relevant” and “irrelevant”. The “relevant” papers \nwere marked with black colour, “may be relevant” and “irrelevant” with green and red \ncolours respectively. At the end of this stage, 45 papers were classified as “relevant”, 9 \npapers as “may be relevant” and 11 as “irrelevant”. Looking critically at the abstract again, \n18 papers were excluded by using the exclusion criteria leaving a total of 47 papers (see \nTable 5) which were manually reviewed in line with the research questions.\n\nInclusion criteria\n\nPapers published in journals, peer-reviewed conferences, workshops, technical and \nsymposium from 2004 and 2018 were included. In addition, the most recent papers \nwere selected in case of papers with similar investigations and results.\n\nTable 2 First search string result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 2097 65 133 2295\n\nTable 3 Second search string result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 196 27 92 315\n\nTable 4 Third Search string refinement result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 64 23 24 111\n\nTable 5 Final Selection\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 25 10 12 47\n\n\n\nPage 9 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nExclusion criteria\n\nPapers that belong to the following categories were excluded from selection as part of \nthe primary study: (i) papers written in source language other than English; (ii) papers \nwith an abstract and or introduction that does not clearly define the contributions of the \nwork; (iii) papers whose abstract do not relate to big data stream analysis.\n\nResult\nThe findings of the study are now presented with respect to the research questions that \nguided the execution of the systematic literature review.\n\nResearch Question 1: What are the tools and technologies employed for big data stream \n\nanalysis?\n\nBig data stream platforms provide functionalities and features that enable big data \nstream applications to develop, operate, deploy, and manage big data streams. Such \nplatforms must be able to pull in streams of data, process the data and stream it back \nas a single flow. Several tools and technologies have been employed to analyse big data \nstreams. In response to the growing demand for big data streaming analytics, a large \nnumber of alternative big data streaming solutions have been developed both by the \nopen source community and enterprise technology vendors. According to [26], there are \nsome factors to consider when selecting big data streaming tools and technologies in \norder to make effective data management decisions. These are briefly described below.\n\nShape of the data\n\nStreaming data sources require serialization technologies for capturing, storing and rep-\nresenting such high-velocity data. For instance, some tools and technologies allow pro-\njection of different structures across data stores, giving room for flexibility for storage \nand access of data in different ways. However, the performance of such platforms may \nnot be suitable for high-velocity data.\n\nData access\n\nThere is a need to put into consideration how the data will be accessed by users and \napplications. For instance, many NoSQL databases require specific application interfaces \nfor data access. Hence there is a need to consider the integration of some other neces-\nsary tools for data access.\n\nAvailability and consistency requirement\n\nIf a distributed system is needed, then CAP theorem states that consistency and avail-\nability cannot be both guaranteed in the presence of network partition (i.e. when there is \na break in the network). In such a scenario, consistency is often traded off for availability \nto ensure that requests can always be processed.\n\nWorkload profile required\n\nPlatform as a service deployment may be appropriate for a spike load profile platform. \nIf platform distribution can be deployed on Infrastructure as a service cloud, then this \noption may be preferred as users will need to pay only when processing. On-premise \n\n\n\nPage 10 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\ndeployment may be considered for predictable or consistent loads. But if workloads are \nmixed (i.e. consistent flows or spikes), a combination of cloud and on-premise approach \nmay be considered so as to give room for easy integration of web-based services or soft-\nware and access to critical functions on the go.\n\nLatency requirement\n\nIf a minimal delay or low latency is required, key-value stores may be considered or bet-\nter still, an in-memory solution which allows the process of large datasets in real-time is \nrequired in order to optimize the data loading procedure.\n\nThe tools and technologies for big data stream analysis can be broadly categorized into \ntwo, which are open source and proprietary solutions. These are listed in Tables 6 and 7.\n\nThe selection of big data streaming tools and technologies should be based on the impor-\ntance of each factor earlier mentioned in this section. Proprietary solutions may not be eas-\nily available because of pricing and licensing issues. While open source supports innovation \nand development at a large scale, careful selection must be made especially when choosing \na recent technology still in production due to limited maturity and lack of support from \nacademic researchers or developer communities. In addition, open source solutions may \nlead to outdating and modification challenges [27]. Moreover, the selection of whether pro-\nprietary or open source or combination of both should depend on the problem to address, \nthe understanding of the true costs, and benefits of both open and proprietary solutions.\n\nTable 6 Open source tools and technologies for big data stream analysis\n\nTools and technology Article\n\nBlockMon [83]\n\nNoSQL [4, 84–86]\n\nSpark streaming [67, 87–91]\n\nApache storm [68, 85, 86, 92–97]\n\nKafka [85, 91, 95, 96, 98]\n\nYahoo! S4 [6, 45, 87, 99]\n\nApache Samza [46, 67, 100]\n\nPhoton [67, 101]\n\nApache Aurora [67, 102]\n\nMavEStream [103]\n\nEsperTech [104, 105]\n\nRedis [106]\n\nC-SPARQL [107, 108]\n\nSAMOA [56, 78, 109]\n\nCQELS [108, 110, 111]\n\nETALIS [112]\n\nXSEQ [73]\n\nApache Kylin [113]\n\nSplunk stream [114]\n\n\n\nPage 11 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nResearch Question 2: What methods and techniques are used in analysing big data \n\nstreams?\n\nGiven the real-time nature, velocity and volume of social media streams, the clus-\ntering algorithms that are applied on streaming data must be highly scalable and \nefficient. Also, the dynamic nature of data makes it difficult to know the required or \ndesirable number of clusters in advance. This renders partitioning clustering tech-\nniques (such as k-median, k-means and k-medoid) or expectation-maximization \n(EM) algorithms-based approaches unsuitable for analysing real-time social media \ndata because they require prior knowledge of clusters in advance. In addition, due \nto concept drift inherent in social media streams, scalable graph partitioning algo-\nrithms are not also suitable because of their tendency towards balanced partitioning. \nSocial media streams must be analysed dynamically in order to provide decisions at \nany given time within a limited space and time window [28–30].\n\nDensity-based clustering algorithm (such as DenStream, OpticStream, Flock-\nStream, Exclusive and Complete Clustering) unlike partitioning algorithms does not \nrequire apriori number of clusters in advance and can detect outliers [31]. However, \nthe issue with density-based clustering algorithms is that most of them except for few \nlike HDDStream, PreDeCon-Stream and PKS-Stream (which are memory intensive) \nperform less efficiently in the face of high dimensional data and as a result are not \nsuitable for analyzing social media streams [32].\n\nThreshold-based techniques, hierarchical clustering, and incremental clustering \nor online clustering are more relevant to social media analysis. Several online thresh-\nold-based stream clustering approaches or incremental clustering approaches such as \nMarkov Random Field [33, 34], Online Spherical K-means [35], and Condensed Clusters \n[36] have been adopted. Incremental approaches are suitable for continuously generated \ndata grouping by setting a maximum similarity threshold between the incoming stream \n\nTable 7 Proprietary tools and technologies for big data stream analysis\n\nTools and technology Article\n\nCodeBlue [115]\n\nAnodot [116]\n\nCloudet [117]\n\nSentiment brand monitoring [118]\n\nNumenta [119]\n\nElastic streaming processing engine [120]\n\nMicrosoft azure stream analytics [121]\n\nIBM InfoSphere streams [8, 122]\n\nGoogle MillWheel [123]\n\nArtemis [124]\n\nWSO2 analytics [125]\n\nMicrosoft StreamInsight [126]\n\nTIBCO StreamBase [127]\n\nStriim [128]\n\nKyvos insights [129]\n\nAtScale [130, 131]\n\nLambda architecture [57]\n\n\n\nPage 12 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nand the existing clusters. Much work has been done in improving the efficiency of online \nclustering algorithms, however, little research efforts have been directed to threshold \nand fragmentation issues. Incremental algorithm threshold setting should employ adap-\ntive approach instead of relying on static values [37, 38]. Some of the methods and tech-\nniques that have been employed in analysing big data streams are outlined in Table 8.\n\nTable 8 Methods and techniques for big data stream analysis\n\nMethods and techniques Article\n\nSPADE [132]\n\nLocally supervised metric learning (LSML) [133]\n\nKTS [106]\n\nMultinomial latent dirichlet allocation [106]\n\nVoltage clustering algorithm [106]\n\nLocality sensitive hashing (LSH) [134]\n\nUser profile vector update algorithm [134]\n\nTag assignment stream clustering (TASC) [134]\n\nStreamMap [117]\n\nDensity cognition [117]\n\nQRS detection algorithm [87]\n\nForward chaining rule [110]\n\nStream [135]\n\nCluStream [136, 137]\n\nHPClustering [138]\n\nDenStream [139]\n\nD-Stream [140]\n\nACluStream [141]\n\nDCStream [142]\n\nP-Stream [143]\n\nADStream [144]\n\nContinuous query processing (CQR) [145]\n\nFPSPAN-growth [146]\n\nOutlier method for cloud computing algorithm (OMCA) [147]\n\nMulti-query optimization strategy (MQOS) [148]\n\nParallel K-means clustering [72]\n\nVisibly push down automata (VPA) [73]\n\nIncremental MI outlier detection algorithm (Inc I-MLOF) [149]\n\nAdaptive windowing based online ensemble (AWOE) [74]\n\nDynamic prime-number based security verification [84]\n\nK-anonymity, I-diversity, t-closeness [90]\n\nSingular spectrum matrix completion (SS-MC) [76]\n\nTemporal fuzzy concept analysis [96]\n\nECM-sketch [77]\n\nNearest neighbour [91]\n\nMarkov chains [91]\n\nBlock-QuickSort-AdjacentJobMatch [86]\n\nBlock-QuickSort-OverlapReplicate [86]\n\nFuzzy-CSar-AFP [150]\n\nWeighted online sequential extreme learning machine with kernels (WOS-ELMK) [22]\n\nConcept-adapting very fast decision tree (CVFDT) [151]\n\n\n\nPage 13 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nMany researchers have looked at the aspect of the real-time analysis of big data \nstreams but not much attention has been directed towards social media stream pre-\nprocessing. For instance, the social media stream is characterized by incomplete, noisy, \nslang, abbreviated words. Also, contextual meaning of social media post is essential for \nimproved event detection, sentiment analysis or any other social media analytics algo-\nrithms in terms of quality and accuracy [36, 39]. There is the need to give more atten-\ntion to the preprocessing stage of social media stream analysis in the face of incomplete, \nnoisy, slang, and abbreviated words that are pertinent to social media streams. These \nchallenges create opportunities application of new semantic technology approaches, \nwhich are more suited to social media streams [40, 41].\n\nResearch Question 3: What do big data streaming tools and technologies have in common \n\nand their differences in terms of concept, purpose, and capabilities?\n\nThe features of various tools and technologies for big data stream were compared in \norder to answer this question. An overview analysis based on 10 dimensions, which are \ndatabase support, execution model, workload, fault-tolerance, latency, throughput, reli-\nability, operating system, implementation languages and application domain or areas is \npresented in Table 9.\n\nFor organisations with existing applications that have support for SQL, MySQL, SQL \nServer, Oracle Database, for instance, may consider choosing big data streaming tools \nand technologies that have support for their existing databases. There are few big data \nstreaming tools and technology that support virtually any data format. An example of \nsuch is Infochimps Cloud.\n\nThe major big data streaming tools and technologies considered are all suitable for \nstreaming execution model, however out of 19 big data tools and technology compared \nand contrasted in this section, only 10.5% is suitable for streaming, batch, and iterative \nprocessing while 47.4% can handle jobs requiring both batch and streaming processing. \nIt is safer for a job to be executed on a single platform which can accommodate all the \ndependencies required in order to avoid interoperability constraints than combining \ntwo or more platforms or frameworks. The best fit with respect to the choice of big data \nstreaming tools and technologies will depend on the state of data to process, infrastruc-\nture preference, business use case, and kind of results interested in.\n\nVirtually all the big data streaming tools and technologies are memory intensive. This \nimplies that the main performance bottleneck at higher load conditions will be due to \nlack of memory [42]. However, research has shown that the benefit of high intensive \nmemory applications outweighs the performance loss due to long memory latency [43].\n\nFrom all the big data streaming tools and technologies reviewed, only IBMInfoS-\nphere and TIBCO StreamBase support all of the three “at-most-once” “at-least-once” \nand “exactly-once” message delivery mechanisms while others support one or two of the \nthree delivery mechanisms. “At-most-once” is the cheapest with least implementation \noverhead and highest performance because it can be done in a fire-and-forget fashion \nwithout keeping the state in the transport mechanism or at the sending end. “At-least-\nonce” delivery requires multiple attempts in order to counter transport losses which \nmeans keeping the state at the sending end and having an acknowledgement mechanism \nat the receiving end. “Exactly-once” is the most expensive and has consequently worst \n\n\n\nPage 14 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nTa\nbl\n\ne \n9 \n\nCo\nm\n\npa\nri\n\nso\nn \n\nof\n b\n\nig\n d\n\nat\na \n\nst\nre\n\nam\nin\n\ng \nto\n\nol\ns \n\nan\nd \n\nte\nch\n\nno\nlo\n\ngi\nes\n\nTo\nol\n\ns \nan\n\nd \nte\n\nch\nno\n\nlo\ngy\n\nD\nat\n\nab\nas\n\ne \nsu\n\npp\nor\n\nt\nEx\n\nec\nut\n\nio\nn \n\nm\nod\n\nel\nW\n\nor\nkl\n\noa\nd\n\nFa\nul\n\nt t\nol\n\ner\nan\n\nce\nLa\n\nte\nnc\n\ny\nTh\n\nro\nug\n\nhp\nut\n\nRe\nlia\n\nbi\nlit\n\ny\nO\n\npe\nra\n\ntin\ng \n\nsy\nst\n\nem\nIm\n\npl\nem\n\nen\nta\n\ntio\nn/\n\nsu\npp\n\nor\nte\n\nd \nla\n\nng\nua\n\nge\ns\n\nA\npp\n\nlic\nat\n\nio\nn\n\nBl\noc\n\nkM\non\n\nCa\nss\n\nan\ndr\n\na,\n M\n\non\n-\n\ngo\nD\n\nB,\n X\n\nM\nL\n\nSt\nre\n\nam\nin\n\ng\nM\n\nul\nti-\n\nsl\nic\n\ne \nm\n\nem\n-\n\nor\ny \n\nal\nlo\n\nca\ntio\n\nn \nan\n\nd \nba\n\ntc\nh \n\nal\nlo\n\nca\ntio\n\nns\n\nC\nhe\n\nck\npo\n\nin\nt, \n\nro\nllb\n\nac\nk\n\nVe\nry\n\n lo\nw\n\nH\nig\n\nh\nA\n\nt l\nea\n\nst\n o\n\nnc\ne\n\nLi\nnu\n\nx\nC\n\n +\n+\n\n11\n, P\n\nyt\nho\n\nn\nA\n\nno\nm\n\nal\ny \n\nde\nte\n\nct\nio\n\nn,\n \n\nne\ntw\n\nor\nk \n\nop\ntim\n\niz\na-\n\ntio\nn,\n\n m\nul\n\ntim\ned\n\nia\n \n\nco\nnt\n\nen\nt d\n\nel\niv\n\ner\ny,\n\n \nfin\n\nan\nci\n\nal\n m\n\nar\nke\n\nt \nan\n\nal\nys\n\nis\n, w\n\neb\n \n\nan\nal\n\nyt\nic\n\ns\n\nSp\nar\n\nk \nSt\n\nre\nam\n\nin\ng\n\nKa\nfk\n\na,\n H\n\nBa\nse\n\n, \nH\n\niv\ne \n\nFl\num\n\ne,\n \n\nH\nD\n\nF/\nS3\n\n, \nKi\n\nne\nsi\n\ns, \nTC\n\nP \nso\n\nck\net\n\ns, \nTw\n\nit-\nte\n\nr, \nSQ\n\nL\n\nBa\ntc\n\nh,\n It\n\ner\nat\n\niv\ne,\n\n \nSt\n\nre\nam\n\nin\ng\n\nC\nPU\n\n/m\nem\n\nor\ny \n\nin\nte\n\nns\niv\n\ne\nRD\n\nD\n b\n\nas\ned\n\n \nC\n\nhe\nck\n\n-p\noi\n\nnt\n-\n\nin\ng,\n\n p\nar\n\nal\nle\n\nl \nre\n\nco\nve\n\nry\n, \n\nre\npl\n\nic\nat\n\nio\nn\n\nLo\nw\n\nH\nig\n\nh\nEx\n\nac\ntly\n\n o\nnc\n\ne\nW\n\nin\ndo\n\nw\ns, \n\nm\nac\n\nO\nS,\n\n L\nin\n\nux\nSc\n\nal\na,\n\n P\nyt\n\nho\nn,\n\n \nJa\n\nva\n, R\n\nEv\nen\n\nt d\net\n\nec\ntio\n\nn,\n \n\nst\nre\n\nam\nin\n\ng \nm\n\nac\nhi\n\nne\n \n\nle\nar\n\nni\nng\n\n, f\nog\n\n c\nom\n\n-\npu\n\ntin\ng,\n\n in\nte\n\nra\nct\n\niv\ne \n\nan\nal\n\nys\nis\n\n, m\nul\n\ntim\ne-\n\ndi\na \n\nan\nal\n\nys\nis\n\n, c\nlu\n\nst\ner\n\n \nan\n\nal\nys\n\nis\n, fi\n\nlte\nrin\n\ng,\n \n\nre\n-p\n\nro\nce\n\nss\nin\n\ng,\n \n\nca\nch\n\ne \nin\n\nva\nlid\n\nat\nio\n\nn\n\nA\npa\n\nch\ne \n\nSt\nor\n\nm\nSp\n\nou\nt, \n\nH\nBa\n\nse\n, \n\nH\niv\n\ne,\n S\n\nQ\nL,\n\n \nCa\n\nss\nan\n\ndr\na,\n\n \nM\n\nem\nca\n\nch\ned\n\nSt\nre\n\nam\nin\n\ng\nC\n\nPU\n/m\n\nem\nor\n\ny \nin\n\nte\nns\n\niv\ne\n\nRe\npl\n\nic\nat\n\nio\nn,\n\n \nch\n\nec\nkp\n\noi\nnt\n\n, \nda\n\nta\n re\n\nco\nve\n\nry\n, \n\nU\nps\n\ntr\nea\n\nm\n \n\nba\nck\n\nup\n, \n\nre\nco\n\nrd\n-le\n\nve\nl \n\nac\nkn\n\now\nle\n\ndg\ne-\n\nm\nen\n\nt, \nst\n\nat\nel\n\nes\ns \n\nm\nan\n\nag\nem\n\nen\nt\n\nVe\nry\n\n lo\nw\n\nLo\nw\n\nA\nt l\n\nea\nst\n\n o\nnc\n\ne\nW\n\nin\ndo\n\nw\ns, \n\nm\nac\n\nO\nS,\n\n L\nin\n\nux\nC\n\nlo\nju\n\nre\n, J\n\nav\na,\n\n S\nca\n\nla\n, \n\nC\nlo\n\nju\nre\n\n, n\non\n\n-J\nVM\n\n \nla\n\nng\nua\n\nge\ns\n\nIn\nte\n\nrn\net\n\n o\nf t\n\nhi\nng\n\ns, \nst\n\nre\nam\n\nin\ng \n\nm\nac\n\nhi\nne\n\n \nle\n\nar\nni\n\nng\n, m\n\nul\ntim\n\ne-\ndi\n\na \nan\n\nal\nys\n\nis\n\nYa\nho\n\no!\n S\n\n4\nM\n\nyS\nQ\n\nL,\n N\n\noS\nQ\n\nL,\n \n\nRi\nch\n\n D\nat\n\na \nFo\n\nrm\nat\n\nSt\nre\n\nam\nin\n\ng\nC\n\nPU\n/m\n\nem\nor\n\ny \nin\n\nte\nns\n\niv\ne\n\nRe\npl\n\nic\nat\n\nio\nn,\n\n \nch\n\nec\nkp\n\noi\nnt\n\n, \nda\n\nta\n re\n\nco\nve\n\nry\n\nLo\nw\n\nLo\nw\n\nEx\nac\n\ntly\n o\n\nnc\ne\n\nLi\nnu\n\nx\nJa\n\nva\n, P\n\nyt\nho\n\nn,\n C\n+\n+\n\n, \nPe\n\nrl\nO\n\nnl\nin\n\ne \nan\n\nal\nyt\n\nic\ns, \n\nm\non\n\nito\nrin\n\ng,\n fr\n\nau\nd \n\nde\nte\n\nct\nio\n\nn,\n fi\n\nna\nnc\n\nia\nl \n\nda\nta\n\n p\nro\n\nce\nss\n\nin\ng,\n\n \nw\n\neb\n p\n\ner\nso\n\nna\nliz\n\na-\ntio\n\nn \nan\n\nd \nse\n\nss\nio\n\nn \nm\n\nod\nel\n\nlin\ng\n\n\n\nPage 15 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nTa\nbl\n\ne \n9 \n\n(c\non\n\nti\nnu\n\ned\n)\n\nTo\nol\n\ns \nan\n\nd \nte\n\nch\nno\n\nlo\ngy\n\nD\nat\n\nab\nas\n\ne \nsu\n\npp\nor\n\nt\nEx\n\nec\nut\n\nio\nn \n\nm\nod\n\nel\nW\n\nor\nkl\n\noa\nd\n\nFa\nul\n\nt t\nol\n\ner\nan\n\nce\nLa\n\nte\nnc\n\ny\nTh\n\nro\nug\n\nhp\nut\n\nRe\nlia\n\nbi\nlit\n\ny\nO\n\npe\nra\n\ntin\ng \n\nsy\nst\n\nem\nIm\n\npl\nem\n\nen\nta\n\ntio\nn/\n\nsu\npp\n\nor\nte\n\nd \nla\n\nng\nua\n\nge\ns\n\nA\npp\n\nlic\nat\n\nio\nn\n\nA\npa\n\nch\ne \n\nSa\nm\n\nza\nKa\n\nfk\na,\n\n H\nD\n\nFS\n, \n\nKi\nne\n\nsi\ns, \n\nSt\nre\n\nam\n \n\nco\nns\n\num\ner\n\n, K\ney\n\n-\nva\n\nlu\ne \n\nst\nor\n\nes\n\nSt\nre\n\nam\nin\n\ng,\n b\n\nat\nch\n\n \npr\n\noc\nes\n\nsi\nng\n\nM\nem\n\nor\ny \n\nin\nte\n\nn-\nsi\n\nve\nC\n\nhe\nck\n\npo\nin\n\nt\nVe\n\nry\n lo\n\nw\nH\n\nig\nh\n\nA\nt l\n\nea\nst\n\n o\nnc\n\ne\nLi\n\nnu\nx,\n\n W\nin\n\ndo\nw\n\ns\nJa\n\nva\n, S\n\nca\nla\n\n, J\nVM\n\n \nla\n\nng\nua\n\nge\ns\n\nFi\nlte\n\nrin\ng,\n\n re\n-p\n\nro\n-\n\nce\nss\n\nin\ng,\n\n c\nac\n\nhe\n \n\nin\nva\n\nlid\nat\n\nio\nn\n\nA\npa\n\nch\ne \n\nFl\nin\n\nk\nKa\n\nfk\na,\n\n F\nlu\n\nm\ne,\n\n \nH\n\nD\nF/\n\nS3\n, \n\nKi\nne\n\nsi\ns, \n\nTC\nP \n\nso\nck\n\net\ns, \n\nTw\nit-\n\nte\nr, \n\nCa\nss\n\nan\ndr\n\na,\n \n\nRe\ndi\n\ns, \nM\n\non\n-\n\ngo\nD\n\nB,\n H\n\nBa\nse\n\n, \nSQ\n\nL\n\nSt\nre\n\nam\nin\n\ng,\n \n\nba\ntc\n\nh,\n it\n\ner\nat\n\niv\ne,\n\n \nin\n\nte\nra\n\nct\niv\n\ne\n\nM\nem\n\nor\ny \n\nin\nte\n\nn-\nsi\n\nve\nSt\n\nre\nam\n\n re\npl\n\nay\n \n\nan\nd \n\nm\nar\n\nke\nr-\n\nch\nec\n\nkp\noi\n\nnt\n\nVe\nry\n\n lo\nw\n\nH\nig\n\nh\nEx\n\nac\ntly\n\n o\nnc\n\ne\nLi\n\nnu\nx,\n\n M\nac\n\nO\nS,\n\n \nW\n\nin\ndo\n\nw\ns\n\nJa\nva\n\n, S\nca\n\nla\n, P\n\nyt\nho\n\nn\nO\n\npt\nim\n\niz\nat\n\nio\nn \n\nof\n \n\ne-\nco\n\nm\nm\n\ner\nce\n\n \nse\n\nar\nch\n\n re\nsu\n\nlt,\n \n\nne\ntw\n\nor\nk/\n\nse\nns\n\nor\n \n\nm\non\n\nito\nrin\n\ng \nan\n\nd \ner\n\nro\nr d\n\net\nec\n\ntio\nn,\n\n \nET\n\nL \nfo\n\nr b\nus\n\nin\nes\n\ns \nin\n\nte\nlli\n\nge\nnc\n\ne \nin\n\nfra\n-\n\nst\nru\n\nct\nur\n\ne,\n m\n\nac\nhi\n\nne\n \n\nle\nar\n\nni\nng\n\nA\npa\n\nch\ne \n\nA\nur\n\nor\na\n\nH\n2,\n\n Ja\nva\n\n m\nap\n\ns, \nM\n\nyB\nat\n\nis\n, \n\nM\nyS\n\nQ\nL,\n\n P\nos\n\nt-\ngr\n\neS\nQ\n\nL\n\nSt\nre\n\nam\nin\n\ng\nM\n\nem\nor\n\ny \nan\n\nd \ndi\n\nsk\n s\n\npa\nce\n\nPe\nrio\n\ndi\nc \n\nre\nco\n\nv-\ner\n\ny \nch\n\nec\nkp\n\noi\nnt\n\n \nan\n\nd \nro\n\nllb\nac\n\nk\n\nLo\nw\n\nH\nig\n\nh\nA\n\nt l\nea\n\nst\n o\n\nnc\ne\n\nLi\nnu\n\nx\nPy\n\nth\non\n\nM\non\n\nito\nrin\n\ng \nap\n\npl\nic\n\na-\ntio\n\nns\n s\n\nuc\nh \n\nas\n \n\nfin\nan\n\nci\nal\n\n a\nna\n\nly\nsi\n\ns \nan\n\nd \nm\n\nili\nta\n\nry\n a\n\npp\nli-\n\nca\ntio\n\nns\n\nRe\ndi\n\ns\nKe\n\ny-\nva\n\nlu\ne \n\nst\nor\n\nes\n, \n\nra\nbi\n\ntm\nq,\n\n M\non\n\n-\ngo\n\nD\nB\n\nSt\nre\n\nam\nin\n\ng\nIn\n\n-m\nem\n\nor\ny \n\nbu\nt \n\npe\nrs\n\nis\nte\n\nnt\n o\n\nn-\ndi\n\nsk\n d\n\nat\nab\n\nas\ne\n\nRe\npl\n\nic\na \n\nm\nig\n\nra\n-\n\ntio\nn,\n\n S\nen\n\ntin\nel\n\nLo\nw\n\nH\nig\n\nh\nA\n\nt l\nea\n\nst\n o\n\nnc\ne\n\nU\nbu\n\nnt\nu,\n\n L\nin\n\nux\n, \n\nO\nSX\n\nC\n, C\n\n#,\n Ja\n\nva\n, P\n\nH\nP, \n\nPy\nth\n\non\nW\n\neb\n a\n\nna\nly\n\nsi\ns, \n\nca\nch\n\ne,\n \n\nm\nes\n\nsa\nge\n\n q\nue\n\nue\ns\n\nC\n-S\n\nPA\nRQ\n\nL\nRD\n\nF, \nSQ\n\nLJ\n, \n\nN\noS\n\nQ\nL,\n\n H\nD\n\nF\nBa\n\ntc\nh,\n\n s\ntr\n\nea\nm\n\nin\ng\n\nLo\nw\n\n m\nem\n\nor\ny \n\nus\nag\n\ne\nA\n\nda\npt\n\nat\nio\n\nn\nVe\n\nry\n lo\n\nw\nH\n\nig\nh\n\nCu\nm\n\nul\nat\n\niv\ne\n\nW\nin\n\ndo\nw\n\ns, \nLi\n\nnu\nx,\n\n M\nac\n\nO\nS,\n\n \nA\n\nnd\nro\n\nid\n\nJa\nva\n\n, A\npa\n\nch\ne \n\nJe\nna\n\n \nlib\n\nra\nrie\n\ns\nRe\n\nal\n-t\n\nim\ne \n\nre\nas\n\non\nin\n\ng \nov\n\ner\n s\n\nen\nso\n\nr d\nat\n\na,\n \n\nso\nci\n\nal\n s\n\nem\nan\n\ntic\n \n\nda\nta\n\n, u\nrb\n\nan\n c\n\nom\n-\n\npu\ntin\n\ng\n\nSA\nM\n\nO\nA\n\nH\nBa\n\nse\n, H\n\niv\ne,\n\n C\nas\n\n-\nsa\n\nnd\nra\n\nSt\nre\n\nam\nin\n\ng\nLo\n\nw\n m\n\nem\nor\n\ny \nus\n\nag\ne\n\nU\nps\n\ntr\nea\n\nm\n \n\nba\nck\n\nup\nLo\n\nw\nH\n\nig\nh\n\nEx\nac\n\ntly\n o\n\nnc\ne\n\nLi\nnu\n\nx\nJa\n\nva\nC\n\nla\nss\n\nifi\nca\n\ntio\nn,\n\n c\nlu\n\nst\ner\n\n-\nin\n\ng,\n s\n\npa\nm\n\n d\net\n\nec\n-\n\ntio\nn,\n\n re\ngr\n\nes\nsi\n\non\n, \n\nfre\nqu\n\nen\nt p\n\nat\nte\n\nrn\n \n\nm\nin\n\nin\ng\n\n\n\nPage 16 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nTa\nbl\n\ne \n9 \n\n(c\non\n\nti\nnu\n\ned\n)\n\nTo\nol\n\ns \nan\n\nd \nte\n\nch\nno\n\nlo\ngy\n\nD\nat\n\nab\nas\n\ne \nsu\n\npp\nor\n\nt\nEx\n\nec\nut\n\nio\nn \n\nm\nod\n\nel\nW\n\nor\nkl\n\noa\nd\n\nFa\nul\n\nt t\nol\n\ner\nan\n\nce\nLa\n\nte\nnc\n\ny\nTh\n\nro\nug\n\nhp\nut\n\nRe\nlia\n\nbi\nlit\n\ny\nO\n\npe\nra\n\ntin\ng \n\nsy\nst\n\nem\nIm\n\npl\nem\n\nen\nta\n\ntio\nn/\n\nsu\npp\n\nor\nte\n\nd \nla\n\nng\nua\n\nge\ns\n\nA\npp\n\nlic\nat\n\nio\nn\n\nCQ\nEL\n\nS\nRD\n\nF, \nSQ\n\nLJ\n, \n\nN\noS\n\nQ\nL,\n\n H\nD\n\nF\nBa\n\ntc\nh,\n\n s\ntr\n\nea\nm\n\nin\ng\n\nIn\n-m\n\nem\nor\n\ny\nA\n\nda\npt\n\nat\nio\n\nn\nLo\n\nw\nH\n\nig\nh\n\nCu\nm\n\nul\nat\n\niv\ne\n\nW\nin\n\ndo\nw\n\ns, \nLi\n\nnu\nx,\n\n M\nac\n\nO\nS,\n\n \nA\n\nnd\nro\n\nid\n\nJa\nva\n\nRe\nal\n\n-t\nim\n\ne \nre\n\nas\non\n\nin\ng \n\nov\ner\n\n s\nen\n\nso\nr d\n\nat\na,\n\n \nso\n\nci\nal\n\n s\nem\n\nan\ntic\n\n \nda\n\nta\n, u\n\nrb\nan\n\n c\nom\n\n-\npu\n\ntin\ng\n\nET\nA\n\nLI\nS\n\nRD\nF\n\nSt\nre\n\nam\nin\n\ng\nBi\n\nna\nriz\n\nat\nio\n\nn\nA\n\nda\npt\n\nat\nio\n\nn\nLo\n\nw\nLo\n\nw\nCu\n\nm\nul\n\nat\niv\n\ne\nW\n\nin\ndo\n\nw\ns, \n\nLi\nnu\n\nx,\n M\n\nac\nO\n\nS,\n \n\nA\nnd\n\nro\nid\n\nPr\nol\n\nog\n, J\n\nav\na,\n\n C\n, \n\nSP\nA\n\nRQ\nL,\n\n C\n#,\n\n \nET\n\nA\nLI\n\nS \nLa\n\nng\nua\n\nge\n \n\nfo\nr E\n\nve\nnt\n\ns \n(E\n\nLE\n)\n\nEv\nen\n\nt d\net\n\nec\ntio\n\nn,\n \n\nre\nas\n\non\nin\n\ng \nov\n\ner\n \n\nst\nre\n\nam\nin\n\ng \nev\n\nen\nts\n\nXS\nEQ\n\nXM\nL\n\nBa\ntc\n\nh,\n s\n\ntr\nea\n\nm\nin\n\ng\nIn\n\n-m\nem\n\nor\ny \n\nw\nith\n\n \nbu\n\nffe\nrin\n\ng\nch\n\nec\nkp\n\noi\nnt\n\nLo\nw\n\nH\nig\n\nh\nA\n\nt l\nea\n\nst\n o\n\nnc\ne\n\nW\nin\n\ndo\nw\n\ns, \nLi\n\nnu\nx\n\nJa\nva\n\n, A\npa\n\nch\ne \n\nXe\nrc\n\nes\nBi\n\nol\nog\n\nic\nal\n\n d\nat\n\na,\n s\n\noc\nia\n\nl \nne\n\ntw\nor\n\nks\n, u\n\nse\nr \n\nbe\nha\n\nvi\nou\n\nr, \nfin\n\nan\nci\n\nal\n \n\nda\nta\n\n a\nna\n\nly\nsi\n\ns, \nfil\n\nte\nrin\n\ng\n\nIB\nM\n\n In\nfo\n\nSp\nhe\n\nre\n \n\nst\nre\n\nam\ns\n\nPi\ng,\n\n H\niv\n\ne,\n Ja\n\nql\n, \n\nH\nBa\n\nse\n F\n\nlu\nm\n\ne,\n \n\nLu\nce\n\nne\n, A\n\nvr\no,\n\n \nZo\n\noK\nee\n\npe\nr, \n\nO\noz\n\nie\n, O\n\nra\ncl\n\ne \nD\n\nat\nab\n\nas\ne,\n\n \nD\n\nB2\n, N\n\net\nez\n\nza\n, \n\nM\nyS\n\nQ\nL,\n\n A\nst\n\ner\n, \n\nIn\nfo\n\nrm\nix\n\n.\n\nSt\nre\n\nam\nin\n\ng\nCa\n\npt\nur\n\ne \nda\n\nta\nba\n\nse\n \n\nw\nor\n\nkl\noa\n\nds\n a\n\nnd\n \n\nre\npl\n\nay\n th\n\nem\n in\n\n \na \n\nte\nst\n\n d\nat\n\nab\nas\n\ne \nen\n\nvi\nro\n\nnm\nen\n\nt\n\nA\nut\n\nom\nat\n\nic\n \n\nre\nco\n\nve\nry\n\nLo\nw\n\nH\nig\n\nh\nEx\n\nac\ntly\n\n o\nnc\n\ne,\n \n\nA\nt l\n\nea\nst\n\n \non\n\nce\n, A\n\nt \nm\n\nos\nt o\n\nnc\ne\n\nLi\nnu\n\nx,\n C\n\nen\ntO\n\nS\nC\n\n +\n+\n\nJa\nva\n\nSP\nL\n\nSp\nac\n\ne \nw\n\nea\nth\n\ner\n p\n\nre\n-\n\ndi\nct\n\nio\nn,\n\n p\nhy\n\nsi\nol\n\nog\ni-\n\nca\nl d\n\nat\na \n\nst\nre\n\nam\ns \n\nan\nal\n\nys\nis\n\n, t\nra\n\nffi\nc \n\nm\nan\n\nag\nem\n\nen\nt, \n\nre\nal\n\n-\ntim\n\ne \npr\n\ned\nic\n\ntio\nns\n\n, \nev\n\nen\nt d\n\net\nec\n\ntio\nn,\n\n \nvi\n\nsu\nal\n\nis\nat\n\nio\nn\n\nG\noo\n\ngl\ne \n\nM\nill\n\n-\nW\n\nhe\nel\n\nBi\ngT\n\nab\nle\n\n, S\npa\n\nn-\nne\n\nr\nSt\n\nre\nam\n\nin\ng\n\nIn\n-m\n\nem\nor\n\ny \nan\n\nd \nbl\n\noo\nm\n\n fi\nlte\n\nrin\ng\n\nU\nnc\n\noo\nrd\n\nin\nat\n\ned\n \n\npe\nrio\n\ndi\nc,\n\n \nch\n\nec\nkp\n\noi\nnt\n\n, \nup\n\nst\nre\n\nam\n \n\nba\nck\n\nup\n\nLo\nw\n\nH\nig\n\nh\nEx\n\nac\ntly\n\n o\nnc\n\ne\nLi\n\nnu\nx\n\nVi\nrt\n\nua\nlly\n\n a\nny\n\n \npr\n\nog\nra\n\nm\nm\n\nin\ng \n\nla\nng\n\nua\nge\n\nA\nno\n\nm\nal\n\ny \nde\n\nte\nct\n\nio\nn,\n\n \nhe\n\nal\nth\n\n m\non\n\nito\nrin\n\ng,\n \n\nim\nag\n\ne \npr\n\noc\nes\n\nsi\nng\n\n, \nne\n\ntw\nor\n\nk \nsw\n\nitc\nh \n\nm\nan\n\nag\nem\n\nen\nt\n\n\n\nPage 17 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nTa\nbl\n\ne \n9 \n\n(c\non\n\nti\nnu\n\ned\n)\n\nTo\nol\n\ns \nan\n\nd \nte\n\nch\nno\n\nlo\ngy\n\nD\nat\n\nab\nas\n\ne \nsu\n\npp\nor\n\nt\nEx\n\nec\nut\n\nio\nn \n\nm\nod\n\nel\nW\n\nor\nkl\n\noa\nd\n\nFa\nul\n\nt t\nol\n\ner\nan\n\nce\nLa\n\nte\nnc\n\ny\nTh\n\nro\nug\n\nhp\nut\n\nRe\nlia\n\nbi\nlit\n\ny\nO\n\npe\nra\n\ntin\ng \n\nsy\nst\n\nem\nIm\n\npl\nem\n\nen\nta\n\ntio\nn/\n\nsu\npp\n\nor\nte\n\nd \nla\n\nng\nua\n\nge\ns\n\nA\npp\n\nlic\nat\n\nio\nn\n\nIn\nfo\n\nch\nim\n\nps\n \n\ncl\nou\n\nd\nSQ\n\nL,\n N\n\noS\nQ\n\nL,\n \n\nH\niv\n\ne,\n P\n\nig\n \n\nW\nuk\n\non\ng,\n\n \nH\n\nad\noo\n\np,\n \n\nRD\nBM\n\nS,\n V\n\nirt\nu-\n\nal\nly\n\n a\nny\n\n d\nat\n\na \nfo\n\nrm\nat\n\nBa\ntc\n\nh,\n s\n\ntr\nea\n\nm\nin\n\ng\nIn\n\n-m\nem\n\nor\ny\n\nU\nps\n\ntr\nea\n\nm\n \n\nba\nck\n\nup\nLo\n\nw\nH\n\nig\nh\n\nEx\nac\n\ntly\n o\n\nnc\ne\n\nLi\nnu\n\nx\nJa\n\nva\nD\n\nis\nas\n\nte\nr d\n\nis\nco\n\nve\nry\n\n, \nte\n\nxt\n a\n\nna\nly\n\nsi\ns, \n\nco\nm\n\n-\npl\n\nex\n e\n\nve\nnt\n\n p\nro\n\nce\nss\n\n-\nin\n\ng,\n v\n\nis\nua\n\nlis\nat\n\nio\nn\n\nM\nic\n\nro\nso\n\nft\n \n\nSt\nre\n\nam\nIn\n\nsi\ngh\n\nt\nSQ\n\nL \nSe\n\nrv\ner\n\nSt\nre\n\nam\nin\n\ng\nIn\n\n-m\nem\n\nor\ny\n\nRe\npl\n\nic\nat\n\nio\nn,\n\n \nch\n\nec\nkp\n\noi\nnt\n\n, \nda\n\nta\n re\n\nco\nve\n\nry\n\nVe\nry\n\n lo\nw\n\nH\nig\n\nh\nEx\n\nac\ntly\n\n o\nnc\n\ne\nW\n\nin\ndo\n\nw\ns\n\n.N\nET\n\n, C\n#,\n\n L\nIN\n\nQ\n, R\n\nx\nM\n\nan\nuf\n\nac\ntu\n\nrin\ng \n\npr\noc\n\nes\ns \n\nm\non\n\nito\nr-\n\nin\ng \n\nan\nd \n\nco\nnt\n\nro\nl, \n\nfin\nan\n\nci\nal\n\n d\nat\n\na \nan\n\nal\nys\n\nis\n, o\n\npe\nra\n\n-\ntio\n\nn \nan\n\nal\nyt\n\nic\ns, \n\nw\neb\n\n \nan\n\nal\nyt\n\nic\ns, \n\nev\nen\n\nt \npa\n\ntt\ner\n\nn \nde\n\nte\nct\n\nio\nn\n\nTI\nBC\n\nO\n S\n\ntr\nea\n\nm\n-\n\nBa\nse\n\nO\nra\n\ncl\ne \n\nda\nta\n\nba\nse\n\n, \nSQ\n\nL \nSe\n\nrv\ner\n\n, \nIm\n\npa\nla\n\nBa\ntc\n\nh,\n S\n\ntr\nea\n\nm\nin\n\ng\nIn\n\n-m\nem\n\nor\ny\n\nSy\nnc\n\nhr\non\n\niz\nat\n\nio\nn,\n\n \nre\n\npl\nic\n\nat\nio\n\nn,\n \n\nro\nllb\n\nac\nk\n\nVe\nry\n\n lo\nw\n\nH\nig\n\nh\nA\n\nt l\nea\n\nst\n o\n\nnc\ne/\n\nat\n m\n\nos\nt \n\non\nce\n\n/\nex\n\nac\ntly\n\n o\nnc\n\ne\n\nW\nin\n\ndo\nw\n\ns, \nM\n\nac\nO\n\nS,\n L\n\nin\nux\n\nR,\n Ja\n\nva\nM\n\nis\nsi\n\non\n c\n\nrit\nic\n\nal\n \n\nan\nal\n\nys\nis\n\n, I\noT\n\n a\nna\n\nly\n-\n\nsi\ns, \n\ncl\nic\n\nk-\nst\n\nre\nam\n\n \nan\n\nal\nys\n\nis\n, p\n\nre\ndi\n\nct\niv\n\ne \nan\n\nal\nyt\n\nic\ns, \n\nw\nor\n\nkfl\now\n\n \nop\n\ntim\niz\n\nat\nio\n\nn,\n ri\n\nsk\n \n\nav\noi\n\nda\nnc\n\ne\n\nLa\nm\n\nbd\na \n\nA\nrc\n\nhi\n-\n\nte\nct\n\nur\ne\n\nRD\nBM\n\nS,\n C\n\nas\nsa\n\nn-\ndr\n\na,\n K\n\naf\nka\n\n, D\nat\n\na \nW\n\nar\neh\n\nou\nse\n\ns, \nKi\n\nne\nsi\n\ns \nD\n\nat\na \n\nSt\nre\n\nam\n, H\n\nD\nFS\n\n, \nH\n\nBa\nse\n\nBa\ntc\n\nh,\n S\n\ntr\nea\n\nm\nin\n\ng\nIn\n\n-m\nem\n\nor\ny/\n\ndi\nsk\n\n \nda\n\nta\nba\n\nse\nRe\n\npl\nic\n\nat\nio\n\nn,\n \n\nch\nec\n\nkp\noi\n\nnt\nLo\n\nw\nLo\n\nw\nEx\n\nac\ntly\n\n o\nnc\n\ne\nU\n\nbu\nnt\n\nu,\n W\n\nin\n-\n\ndo\nw\n\ns, \nLi\n\nnu\nx\n\nJa\nva\n\n, C\n#,\n\n P\nyt\n\nho\nn,\n\n \nPi\n\ng \nLa\n\ntin\nIo\n\nT \nan\n\nal\nys\n\nis\n, t\n\nra\nck\n\nin\ng \n\nre\nal\n\n-t\nim\n\ne \nup\n\nda\nte\n\ns, \nfin\n\nan\nci\n\nal\n ri\n\nsk\n m\n\nan\n-\n\nag\nem\n\nen\nt, \n\ncl\nic\n\nk-\nst\n\nre\nam\n\n a\nna\n\nly\nsi\n\ns\n\n\n\nPage 18 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nperformance because, in addition to “at-least-once” delivery mechanism, it requires the \nstate to be kept at the receiving end in order to filter duplicate deliveries. In other words, \n“at-most-once” delivery mechanism implies that the message may be lost while “at-\nleast-once” delivery ensures that messages are not lost and “exactly-once” implies that \nmessage can neither be lost nor duplicated. “Exactly-once” is suitable for many critical \nsystems where duplicate messages are unacceptable.\n\nResearch Question 4: What are the limitations and strengths of big data streaming tools \n\nand technologies?\n\nObservations from the literature reveal that specific big data streaming technology may \nnot provide the full set of features that are required. It is rare to find specific big data \ntechnology that combines key features such as scalability, integration, fault-tolerance, \ntimeliness, consistency, heterogeneity and incompleteness management, and load bal-\nancing. For instance, Spark streaming [16] and Sonora [44] are excellent and efficient \nfor checkpointing but the operator space available to user codes are limited. S4 does not \nguarantee 100% fault-tolerant persistent state [45]. Storm does not guarantee the order-\ning of messages due to its “at-least-once” mechanism for record delivery [46, 47]. Strict \ntransaction ordering is required by Trident to operate [48]. While streaming SQL pro-\nvide simple and succinct solutions to many streaming problems, the complex application \nlogic (such as matrix multiplication) and intuitive state abstractions are expressed with \nthe operational flow of an imperative language rather than a declarative language such as \nSQL [49–51].\n\nMoreover, BlockMon uses batches and cache locality optimization techniques for \nmemory allocation efficiency and data speed up access. However, deadlock may occur \nif data streams are enqueued with a higher rate than that of the block consumption [52]. \nApache Samza solves batch latency processing problems but requires an added layer for \nflow control [53]. Flink is suitable for heavy stream processing and batch-oriented tasks \nalthough it has scaling limitations [46]. Redis’ in-memory data store makes it extremely \nfast although this implies that available memory size determines the size of the Redis \ndata store [54]. While C-SPARQL and CQELS are excellent for combining static and \nstreaming data, they are not suitable when scalability is required [55]. SAMOA is suit-\nable for machine learning paradigm as it focuses on speed/real-time analytics, scales \nhorizontally and is loosely coupled with its underlying distributed computation platform \n[56]. With Lambda architecture, a real-time layer can complement the batch processing \none thereby reducing maintenance overhead and risk for errors as a result of duplicate \ncode bases. In addition, Lambda architecture handles reprocessing, which is one of the \nkey challenges in stream processing. Two main problems with Lambda architecture are \ncode maintenance in two complex distributed systems that need to produce the same \nresult and high operational complexity [57, 58].\n\nSummarily, there exists various tools and technologies for implementing big data \nstreams and there seems to be no big data streaming tool and technology that offers all \nthe key features required for now. While each tool and technology may have its strengths \nand weaknesses, the choice depends on the objective of the research and data availa-\nbility. A decision in favour of the wrong technology may result in increased overhead \ncost and time. The decision should take into consideration empirical analysis along with \n\n\n\nPage 19 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nsystem requirements. In addition, research efforts should also be directed to how to \nimprove on existing big data streaming tools and technologies to provide key features \nsuch as scalability, integration, fault-tolerance, timeliness, consistency, heterogeneity \nand incompleteness management, and load balancing.\n\nResearch Question 5: What are the evaluation techniques or benchmarks that are used \n\nfor evaluating big data streaming tools and technologies?\n\nThe diversity of big data poses a challenge when it comes to developing big data bench-\nmarks that will be suitable for all workload cases. One cannot stick to one big data \nbenchmark because it has been observed that using only one benchmark on differ-\nent data sets do not give the same result. This implies that benchmark testing should \nbe application specific. Subsequently, in evaluating big data system, the identification \nof workload for an application domain is a prerequisite [59]. Most of the existing big \ndata benchmarks are designed to evaluate a specific type of systems or architectures. For \ninstance, HiBench [60] is suitable for benchmarking Hadoop, Spark and streaming work-\nloads, GridMix [61] and PigMix [62] are for MapReduce Hadoop systems. BigBench [63, \n64] is suitable for benchmarking Teradata Aster DBMS, MapReduce systems, Redshift \ndatabase, Hive, Spark and Impala. Presently, BigDataBench [65, 66] seems to be the only \nbig data benchmark that can evaluate a hybrid of different big data systems.\n\nSo far, many researchers have evaluated their work by making use of synthetic and \nreal-life data. Standard benchmark dataset for big data streaming analytics has not been \nwidely adopted. However, few of the researchers that used standardized benchmarking \nare briefly discussed below. The work of [67] was tested with two benchmarks; Word \nCount and Grep. The result showed that the proposed algorithm can effectively handle \nunstable input and the delay of the total event can be limited to an expected range.\n\nThe tool developed by [68] was tested on both car dataset and Wikinews5 dataset in \ncomparison with sequential processing. It was discovered that their tool (pipeline imple-\nmentation) performed better and faster.\n\nKrawczyk and Wozniak used several benchmark datasets which include Breast-Wis-\nconsin, Pima, Yeast3, Voting records, CYP2C19 isoform, RBF for estimating weights for \nthe new incoming data stream with their proposed method against other standard meth-\nods. They also analysed time and memory requirements. Experimental investigation \nresult proved that the proposed method can achieve better [69].\n\nA benchmark evaluation using an English movie review dataset collected from Rotten \nTomatoes website (a de facto benchmark for analysing sentiment applications) was con-\nducted by [70], the result showed that sentiment analysis engine (SAE) proposed by the \nauthors outperformed the bag of words approach.\n\nAuthors’ suite of ideas in [71] outperformed state-of-the-art searching technique \ncalled EBSM. The work of [72] used various datasets such as KDD-Cup 99, Forest Cover \ntype, Household power consumption, etc. They compared their algorithm—parallel \nK-means clustering with k-means and k-means++, the result showed that their algo-\nrithm performed better in terms of speed.\n\n5 http://en.wikin ews.org.\n\nhttp://en.wikinews.org\n\n\nPage 20 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nMozafari et al. in [73] benchmarked their system, XSeq against other general-purpose \nXML engines. The system outperformed other complex event processing engines by two \norders of magnitude improvement.\n\nAuthors in [74] evaluated their work in terms of time, accuracy and memory using \nForest cover type, Poker hand, and electricity datasets. They compared their method, \nadaptive windowing based online ensemble (AWOE) with other standard methods such \nas accuracy updated ensemble (AUE), online accuracy updated ensemble (OAUE), accu-\nracy weighted ensemble (AWE), dynamic weighted majority (DWM) and Lev Bagging \n(Lev). Their proposed approach outperformed other methods in three perspectives \nwhich include suitability in terms of different type of drifts, better resolved appropriate \nsize of block, and efficiency.\n\nThe evaluation performed by [75] using FACup and Super Tuesday datasets showed \nthat their method, which is a hybrid of topic extraction methods (i.e. a combination of \nfeature pivot and document pivot) has high efficiency and accuracy with respect to recall \nand precision.\n\nEvaluating the performance of low-rank reconstruction and prediction scheme, spe-\ncifically, singular spectrum matrix completion (SS-MC) proposed by [76], SensorScope \nGrand St-Bernard dataset6 and Intel Berkeley Research Lab dataset7 were used. The \nauthors compared their proposed method with three state-of-the-art methods; KNN-\nimputation, RegEM and ADMM version of MC and discovered that their method \noutperformed the other methods in terms of pure reconstruction as well as in the \ndemanding case of simultaneous recovery and prediction.\n\nThe authors in [77] evaluated their work using World Cup 1998 and CAIDA \nAnonymized Internet Traces 2011 datasets. When their method, ECM-Sketch (a sketch \nsynopsis that allows effective summarization of streaming data over both time-based \nand count-based sliding windows) was compared with three state-of-the-art algorithms \n(Sketch variants); ECM-RW, ECM-DW, and ECM-EH, variants using randomized waves, \ndeterministic waves and exponential histograms respectively, their method reduce \nmemory and computational requirements by at least one order of magnitude with a very \nsmall loss in accuracy.\n\nThe work of [78] centred on benchmarking real-time vehicle data streaming models \nfor a smart city using a simulator that emulates the data produced by a given amount of \nsimultaneous drivers. Experiment with the simulator shows that streaming processing \nengine such as Apache Kafka could serve as a replacement to custom-made streaming \nservers to achieve low latency and higher scalability together with cost reduction.\n\nA benchmark among Kyvos Insight, Impala and Spark conducted by [79] shows that \nKyvos Insight performed analytical queries with much lower latencies when there is a \nlarge number of concurrent users due to pre-aggregation and incremental code building \n[80].\n\nAuthors in [81] proposed that in addition to execution time and resource utilization, \nmicroarchitecture-level and energy consumption are key to fully understanding the \nbehaviour of big data frameworks.\n\n6 http://lcav.epfl.ch.page-86035 -en.html.\n7 http://db.csail .mit.edu/labda ta/labda ta.html.\n\nhttp://lcav.epfl.ch.page-86035-en.html\nhttp://db.csail.mit.edu/labdata/labdata.html\n\n\nPage 21 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nIn addition, to strengthen the confidence of big data research evaluation or result, \napplication of empirical methods (i.e. tested or evaluated concept or technology for \nevidence-based result) should be highly encouraged. The current status of empirical \nresearch in big data stream analysis is still at an infant stage. The maturity of a research \nfield is directly proportional to the number of publications with empirical result [20, 21]. \nAccording to [21] that conducted a systematic literature mapping to verify the current \nstatus of empirical research in big data, it was found out that only 151 out of 1778 stud-\nies contained empirical result. As a result, more research efforts should be directed to \nempirical research in order to raise the level of confidence of big data research outputs \nthan it is at present.\n\nMoreover, only a few big data benchmarks are suitable for different workloads at pre-\nsent. Research efforts should be geared towards advancing benchmarks that are suitable \nfor evaluating different big data systems. This would go a long way to reduce cost and \ninteroperability issue.\n\nDiscussion\nFrom the analysis, it was observed that there has been a wave of interest in big data \nstream analysis since 2013. The number of papers produced in 2012 was doubled in \n2013. In the same vein, more than double of the papers in 2013 were produced in 2014. \nThere was a relative surge in 2017 having a total of 98 paper while the year 2018 received \n156 papers (see Tables 9, 10 and Fig. 2). The percentage of papers analyzed from journals \nwas 50%; that of conferences was 41% while that of workshop/technical/symposium was \n9% as depicted in Fig. 3. Figure 4 presented the frequency of research efforts from differ-\nent geographical locations with researchers from China taking the lead.   \n\nThe selection of big data streaming tools and technologies should be based on the \nimportance of each of the factors such as the shape of the data, data access, availabil-\nity and consistent requirements, workload profile required, and latency requirement. \nCareful selection with respect to open source technology must be made especially when \nchoosing a recent technology still in production. Moreover, the problem to address, the \nunderstanding of the true costs, and benefits of both open and proprietary solutions are \nalso vital when making a selection.\n\nA lot of research efforts have been directed to big data stream analysis but social media \nstream preprocessing is still an open issue. Due to inherent characteristics of social \nmedia stream which include incomplete, noisy, slang, abbreviated words, social media \nstreams present a challenge to big data streams analytics algorithms. There is the need \nto give more attention to the preprocessing stage of social media stream analysis in the \nface of incomplete, noisy, slang, and abbreviated words that are pertinent to social media \nstreams in order to improve big data streams analytics result.\n\nOut of 19 big data streaming tools and technologies compared, 100% support stream-\ning, 47.4% can do both batch and streaming processing while only 10.5% support stream-\ning, batch and iterative processing. Depending on the state of the data to be processed, \ninfrastructure preference, business use case, and kind of results that is of interest, choos-\ning a single big data streaming technology platform that supports all the system require-\nments minimizes the effect of interoperability constraints.\n\n\n\nPage 22 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nFrom all the big data streaming tools and technologies reviewed, only IBMInfoS-\nphere and TIBCO StreamBase support all of the three “at-most-once”, “at-least-once”, \nand “exactly-once” message delivery mechanisms while others support one or two of \nthe three delivery mechanisms. Having all the three delivery mechanisms give room for \nflexibility.\n\nIt is rare to find a specific big data technology that combines key features such as scal-\nability, integration, fault-tolerance, timeliness, consistency, heterogeneity and incom-\npleteness management, and load balancing. There seems to be no big data streaming \ntool and technology that offers all the key features required for now. This calls for more \nresearch efforts that are directed to building more robust big data streaming tools and \ntechnologies.\n\nFew big data benchmarks are suitable for a hybrid of big data systems at present and \nstandard benchmark datasets for big data streaming analytics have not been widely \nadopted. Hence, research efforts should be geared towards advancing benchmarks that \nare suitable for evaluating different big data systems.\n\nLimitation of the review\nWhile authors explored Scopus, ScienceDirect and EBSCO databases which index high \nimpact journals and conference papers from IEEE, ACM, SpringerLink, and Elsevier to \nidentify all possible relevant articles, it is possible that some other relevant articles from \nother databases such as Web of Science could have been missed.\n\nThe analysis and synthesis are based on interpretation of selected articles by the \nresearch team. The authors attempted to avoid this by cross-checking papers to deal \nwith bias though that cannot completely rule out the possibility of errors. In addition, \nthe authors implemented the inclusion and exclusion criteria in the selection of articles \nand only relevant articles written in the English Language were selected. Building on the \nunderpinning of the findings of the research, while a lot of research has been done with \nrespect to tools and technologies as well as methods and techniques employed in big \ndata streaming analytics, method of evaluation or benchmarks of the technologies of \nvarious workloads for big data streaming analytics have not received much attention. As \nit could be gathered from the literature reviewed that most of the researchers evaluated \ntheir work using either synthetic or real-life datasets.\n\nConclusion and further work\nAs a result of challenges and opportunities presented by the Information Technology \nrevolution, big data streaming analytics has emerged as the new frontier of competition \nand innovation. Organisations who seize the opportunity of big data streaming analytics \nare provided with insights for robust decision making in real-time thereby making them \nto have an edge over their competitors.\n\nIn this paper, the authors have tried to present a holistic view of big data streaming \nanalytics by conducting a comprehensive literature review to understand and identify \nthe tools and technologies, methods and techniques, benchmarks or methods of evalu-\nation employed, and key issues in big data stream analysis to showcase the signpost of \nfuture research directions.\n\n\n\nPage 23 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nTa\nbl\n\ne \n10\n\n D\nis\n\ntr\nib\n\nut\nio\n\nn \nof\n\n p\nap\n\ner\ns \n\nov\ner\n\n th\ne \n\nst\nud\n\nie\nd \n\nye\nar\n\ns\n\nYe\nar\n\n20\n04\n\n20\n05\n\n20\n06\n\n20\n07\n\n20\n08\n\n20\n09\n\n20\n10\n\n20\n11\n\n20\n12\n\n20\n13\n\n20\n14\n\n20\n15\n\n20\n16\n\n20\n17\n\n20\n18\n\nTo\nta\n\nl\n\nPa\npe\n\nr\n2\n\n1\n2\n\n3\n5\n\n2\n5\n\n4\n5\n\n10\n22\n\n28\n38\n\n98\n15\n\n6\n38\n\n1\n\n\n\nPage 24 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nAlthough a lot of research efforts have been directed towards big data at rest (i.e. \nbig data batch processing), there has been increased interest in analysing big data \nin ",
      "text": [
        "operator stream NYMEX",
        "Magnitude of change in paper distribution over the studied years 180 160 140 120 100 80 156 60 40 98 No of Papers 20 22 28 38 02 -1 2 3 5 2 5 4 5 10 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 Years",
        "Percentage of Publication Type 34,9% Journal Conferences 155, 41% 192, 50% . Workshop/Technical/Sym posium",
        "90 80 70 60 50 40 w 20 0 No of Researchers Frequency of Researchers 10 0 Italy Canada China India USA Germany France Japan Turkey Republic of Korea Countries Ireland Spain Poland Los Angeles Switzerland Iran Greece Norway",
        "Published online: 06 June 2019"
      ],
      "layoutText": [
        "{\"language\":\"en\",\"text\":\"operator stream NYMEX\",\"lines\":[{\"boundingBox\":[{\"x\":340,\"y\":4},{\"x\":518,\"y\":2},{\"x\":519,\"y\":43},{\"x\":340,\"y\":45}],\"text\":\"operator\"},{\"boundingBox\":[{\"x\":779,\"y\":107},{\"x\":919,\"y\":107},{\"x\":918,\"y\":143},{\"x\":779,\"y\":143}],\"text\":\"stream\"},{\"boundingBox\":[{\"x\":21,\"y\":615},{\"x\":131,\"y\":612},{\"x\":132,\"y\":634},{\"x\":21,\"y\":639}],\"text\":\"NYMEX\"}],\"words\":[{\"boundingBox\":[{\"x\":342,\"y\":4},{\"x\":518,\"y\":2},{\"x\":516,\"y\":45},{\"x\":340,\"y\":45}],\"text\":\"operator\"},{\"boundingBox\":[{\"x\":781,\"y\":107},{\"x\":899,\"y\":109},{\"x\":899,\"y\":144},{\"x\":779,\"y\":144}],\"text\":\"stream\"},{\"boundingBox\":[{\"x\":47,\"y\":616},{\"x\":124,\"y\":613},{\"x\":124,\"y\":634},{\"x\":46,\"y\":638}],\"text\":\"NYMEX\"}]}",
        "{\"language\":\"en\",\"text\":\"Magnitude of change in paper distribution over the studied years 180 160 140 120 100 80 156 60 40 98 No of Papers 20 22 28 38 02 -1 2 3 5 2 5 4 5 10 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 Years\",\"lines\":[{\"boundingBox\":[{\"x\":203,\"y\":0},{\"x\":1033,\"y\":0},{\"x\":1033,\"y\":34},{\"x\":203,\"y\":38}],\"text\":\"Magnitude of change in paper distribution over the\"},{\"boundingBox\":[{\"x\":507,\"y\":40},{\"x\":725,\"y\":42},{\"x\":725,\"y\":79},{\"x\":506,\"y\":76}],\"text\":\"studied years\"},{\"boundingBox\":[{\"x\":51,\"y\":77},{\"x\":94,\"y\":77},{\"x\":95,\"y\":102},{\"x\":50,\"y\":102}],\"text\":\"180\"},{\"boundingBox\":[{\"x\":52,\"y\":122},{\"x\":96,\"y\":122},{\"x\":96,\"y\":143},{\"x\":52,\"y\":143}],\"text\":\"160\"},{\"boundingBox\":[{\"x\":50,\"y\":165},{\"x\":96,\"y\":165},{\"x\":96,\"y\":187},{\"x\":50,\"y\":187}],\"text\":\"140\"},{\"boundingBox\":[{\"x\":53,\"y\":212},{\"x\":95,\"y\":212},{\"x\":95,\"y\":232},{\"x\":52,\"y\":231}],\"text\":\"120\"},{\"boundingBox\":[{\"x\":52,\"y\":256},{\"x\":96,\"y\":255},{\"x\":96,\"y\":275},{\"x\":51,\"y\":276}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":65,\"y\":299},{\"x\":94,\"y\":299},{\"x\":94,\"y\":319},{\"x\":65,\"y\":319}],\"text\":\"80\"},{\"boundingBox\":[{\"x\":1129,\"y\":304},{\"x\":1173,\"y\":303},{\"x\":1175,\"y\":326},{\"x\":1129,\"y\":325}],\"text\":\"156\"},{\"boundingBox\":[{\"x\":66,\"y\":342},{\"x\":94,\"y\":342},{\"x\":94,\"y\":364},{\"x\":66,\"y\":363}],\"text\":\"60\"},{\"boundingBox\":[{\"x\":65,\"y\":384},{\"x\":95,\"y\":384},{\"x\":95,\"y\":408},{\"x\":65,\"y\":408}],\"text\":\"40\"},{\"boundingBox\":[{\"x\":1060,\"y\":367},{\"x\":1093,\"y\":366},{\"x\":1094,\"y\":390},{\"x\":1059,\"y\":391}],\"text\":\"98\"},{\"boundingBox\":[{\"x\":0,\"y\":387},{\"x\":3,\"y\":187},{\"x\":34,\"y\":188},{\"x\":29,\"y\":388}],\"text\":\"No of Papers\"},{\"boundingBox\":[{\"x\":65,\"y\":430},{\"x\":93,\"y\":431},{\"x\":94,\"y\":453},{\"x\":65,\"y\":453}],\"text\":\"20\"},{\"boundingBox\":[{\"x\":838,\"y\":451},{\"x\":870,\"y\":451},{\"x\":871,\"y\":473},{\"x\":839,\"y\":473}],\"text\":\"22\"},{\"boundingBox\":[{\"x\":910,\"y\":443},{\"x\":945,\"y\":442},{\"x\":946,\"y\":467},{\"x\":910,\"y\":469}],\"text\":\"28\"},{\"boundingBox\":[{\"x\":985,\"y\":432},{\"x\":1018,\"y\":431},{\"x\":1019,\"y\":456},{\"x\":986,\"y\":457}],\"text\":\"38\"},{\"boundingBox\":[{\"x\":78,\"y\":474},{\"x\":124,\"y\":473},{\"x\":124,\"y\":495},{\"x\":78,\"y\":496}],\"text\":\"02\"},{\"boundingBox\":[{\"x\":167,\"y\":472},{\"x\":201,\"y\":471},{\"x\":201,\"y\":499},{\"x\":166,\"y\":500}],\"text\":\"-1\"},{\"boundingBox\":[{\"x\":245,\"y\":468},{\"x\":499,\"y\":467},{\"x\":499,\"y\":496},{\"x\":246,\"y\":497}],\"text\":\"2 3 5 2\"},{\"boundingBox\":[{\"x\":537,\"y\":467},{\"x\":571,\"y\":467},{\"x\":571,\"y\":498},{\"x\":536,\"y\":498}],\"text\":\"5\"},{\"boundingBox\":[{\"x\":613,\"y\":463},{\"x\":654,\"y\":464},{\"x\":653,\"y\":497},{\"x\":612,\"y\":496}],\"text\":\"4\"},{\"boundingBox\":[{\"x\":684,\"y\":467},{\"x\":796,\"y\":461},{\"x\":798,\"y\":489},{\"x\":684,\"y\":496}],\"text\":\"5 10\"},{\"boundingBox\":[{\"x\":82,\"y\":504},{\"x\":1178,\"y\":504},{\"x\":1178,\"y\":531},{\"x\":82,\"y\":531}],\"text\":\"2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018\"},{\"boundingBox\":[{\"x\":591,\"y\":554},{\"x\":678,\"y\":556},{\"x\":677,\"y\":580},{\"x\":591,\"y\":578}],\"text\":\"Years\"}],\"words\":[{\"boundingBox\":[{\"x\":204,\"y\":1},{\"x\":381,\"y\":2},{\"x\":380,\"y\":38},{\"x\":203,\"y\":36}],\"text\":\"Magnitude\"},{\"boundingBox\":[{\"x\":387,\"y\":2},{\"x\":424,\"y\":2},{\"x\":423,\"y\":38},{\"x\":386,\"y\":38}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":431,\"y\":2},{\"x\":545,\"y\":2},{\"x\":544,\"y\":37},{\"x\":430,\"y\":38}],\"text\":\"change\"},{\"boundingBox\":[{\"x\":552,\"y\":2},{\"x\":583,\"y\":2},{\"x\":583,\"y\":37},{\"x\":551,\"y\":37}],\"text\":\"in\"},{\"boundingBox\":[{\"x\":593,\"y\":2},{\"x\":691,\"y\":2},{\"x\":691,\"y\":36},{\"x\":592,\"y\":37}],\"text\":\"paper\"},{\"boundingBox\":[{\"x\":698,\"y\":2},{\"x\":886,\"y\":2},{\"x\":885,\"y\":32},{\"x\":698,\"y\":36}],\"text\":\"distribution\"},{\"boundingBox\":[{\"x\":899,\"y\":2},{\"x\":972,\"y\":1},{\"x\":972,\"y\":29},{\"x\":899,\"y\":31}],\"text\":\"over\"},{\"boundingBox\":[{\"x\":979,\"y\":1},{\"x\":1033,\"y\":1},{\"x\":1032,\"y\":27},{\"x\":979,\"y\":29}],\"text\":\"the\"},{\"boundingBox\":[{\"x\":508,\"y\":40},{\"x\":628,\"y\":42},{\"x\":626,\"y\":79},{\"x\":507,\"y\":78}],\"text\":\"studied\"},{\"boundingBox\":[{\"x\":636,\"y\":42},{\"x\":726,\"y\":44},{\"x\":724,\"y\":79},{\"x\":634,\"y\":79}],\"text\":\"years\"},{\"boundingBox\":[{\"x\":51,\"y\":77},{\"x\":92,\"y\":77},{\"x\":92,\"y\":102},{\"x\":51,\"y\":102}],\"text\":\"180\"},{\"boundingBox\":[{\"x\":52,\"y\":122},{\"x\":90,\"y\":122},{\"x\":90,\"y\":143},{\"x\":52,\"y\":143}],\"text\":\"160\"},{\"boundingBox\":[{\"x\":51,\"y\":165},{\"x\":92,\"y\":165},{\"x\":92,\"y\":187},{\"x\":51,\"y\":187}],\"text\":\"140\"},{\"boundingBox\":[{\"x\":52,\"y\":212},{\"x\":90,\"y\":212},{\"x\":90,\"y\":232},{\"x\":52,\"y\":231}],\"text\":\"120\"},{\"boundingBox\":[{\"x\":52,\"y\":256},{\"x\":90,\"y\":255},{\"x\":90,\"y\":275},{\"x\":52,\"y\":276}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":65,\"y\":299},{\"x\":90,\"y\":299},{\"x\":90,\"y\":319},{\"x\":65,\"y\":319}],\"text\":\"80\"},{\"boundingBox\":[{\"x\":1129,\"y\":303},{\"x\":1171,\"y\":303},{\"x\":1171,\"y\":326},{\"x\":1129,\"y\":326}],\"text\":\"156\"},{\"boundingBox\":[{\"x\":66,\"y\":342},{\"x\":92,\"y\":342},{\"x\":91,\"y\":364},{\"x\":66,\"y\":363}],\"text\":\"60\"},{\"boundingBox\":[{\"x\":65,\"y\":384},{\"x\":91,\"y\":384},{\"x\":91,\"y\":408},{\"x\":65,\"y\":408}],\"text\":\"40\"},{\"boundingBox\":[{\"x\":1059,\"y\":367},{\"x\":1088,\"y\":366},{\"x\":1089,\"y\":390},{\"x\":1060,\"y\":391}],\"text\":\"98\"},{\"boundingBox\":[{\"x\":0,\"y\":387},{\"x\":0,\"y\":347},{\"x\":28,\"y\":347},{\"x\":27,\"y\":387}],\"text\":\"No\"},{\"boundingBox\":[{\"x\":0,\"y\":338},{\"x\":1,\"y\":304},{\"x\":29,\"y\":305},{\"x\":28,\"y\":339}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":1,\"y\":298},{\"x\":3,\"y\":188},{\"x\":35,\"y\":189},{\"x\":30,\"y\":299}],\"text\":\"Papers\"},{\"boundingBox\":[{\"x\":66,\"y\":430},{\"x\":90,\"y\":430},{\"x\":90,\"y\":453},{\"x\":66,\"y\":452}],\"text\":\"20\"},{\"boundingBox\":[{\"x\":839,\"y\":451},{\"x\":867,\"y\":451},{\"x\":867,\"y\":473},{\"x\":839,\"y\":473}],\"text\":\"22\"},{\"boundingBox\":[{\"x\":912,\"y\":443},{\"x\":940,\"y\":442},{\"x\":941,\"y\":467},{\"x\":913,\"y\":468}],\"text\":\"28\"},{\"boundingBox\":[{\"x\":985,\"y\":432},{\"x\":1015,\"y\":431},{\"x\":1016,\"y\":456},{\"x\":986,\"y\":457}],\"text\":\"38\"},{\"boundingBox\":[{\"x\":78,\"y\":474},{\"x\":116,\"y\":473},{\"x\":117,\"y\":495},{\"x\":78,\"y\":496}],\"text\":\"02\"},{\"boundingBox\":[{\"x\":166,\"y\":472},{\"x\":192,\"y\":471},{\"x\":193,\"y\":499},{\"x\":166,\"y\":500}],\"text\":\"-1\"},{\"boundingBox\":[{\"x\":254,\"y\":470},{\"x\":272,\"y\":469},{\"x\":274,\"y\":497},{\"x\":255,\"y\":497}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":326,\"y\":468},{\"x\":344,\"y\":468},{\"x\":345,\"y\":497},{\"x\":327,\"y\":497}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":401,\"y\":467},{\"x\":419,\"y\":467},{\"x\":420,\"y\":497},{\"x\":402,\"y\":497}],\"text\":\"5\"},{\"boundingBox\":[{\"x\":476,\"y\":468},{\"x\":495,\"y\":468},{\"x\":495,\"y\":496},{\"x\":477,\"y\":497}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":545,\"y\":467},{\"x\":565,\"y\":467},{\"x\":565,\"y\":498},{\"x\":545,\"y\":498}],\"text\":\"5\"},{\"boundingBox\":[{\"x\":622,\"y\":463},{\"x\":642,\"y\":464},{\"x\":641,\"y\":497},{\"x\":622,\"y\":496}],\"text\":\"4\"},{\"boundingBox\":[{\"x\":696,\"y\":467},{\"x\":712,\"y\":467},{\"x\":712,\"y\":495},{\"x\":695,\"y\":496}],\"text\":\"5\"},{\"boundingBox\":[{\"x\":764,\"y\":464},{\"x\":793,\"y\":462},{\"x\":794,\"y\":490},{\"x\":765,\"y\":491}],\"text\":\"10\"},{\"boundingBox\":[{\"x\":84,\"y\":505},{\"x\":141,\"y\":505},{\"x\":140,\"y\":532},{\"x\":83,\"y\":532}],\"text\":\"2004\"},{\"boundingBox\":[{\"x\":156,\"y\":505},{\"x\":213,\"y\":505},{\"x\":212,\"y\":532},{\"x\":155,\"y\":532}],\"text\":\"2005\"},{\"boundingBox\":[{\"x\":230,\"y\":505},{\"x\":289,\"y\":505},{\"x\":288,\"y\":532},{\"x\":228,\"y\":532}],\"text\":\"2006\"},{\"boundingBox\":[{\"x\":303,\"y\":505},{\"x\":363,\"y\":505},{\"x\":362,\"y\":532},{\"x\":302,\"y\":532}],\"text\":\"2007\"},{\"boundingBox\":[{\"x\":377,\"y\":505},{\"x\":436,\"y\":505},{\"x\":435,\"y\":531},{\"x\":376,\"y\":532}],\"text\":\"2008\"},{\"boundingBox\":[{\"x\":453,\"y\":505},{\"x\":512,\"y\":505},{\"x\":511,\"y\":531},{\"x\":452,\"y\":531}],\"text\":\"2009\"},{\"boundingBox\":[{\"x\":527,\"y\":505},{\"x\":586,\"y\":505},{\"x\":585,\"y\":531},{\"x\":526,\"y\":531}],\"text\":\"2010\"},{\"boundingBox\":[{\"x\":600,\"y\":505},{\"x\":660,\"y\":505},{\"x\":659,\"y\":531},{\"x\":599,\"y\":531}],\"text\":\"2011\"},{\"boundingBox\":[{\"x\":676,\"y\":505},{\"x\":735,\"y\":505},{\"x\":734,\"y\":531},{\"x\":675,\"y\":531}],\"text\":\"2012\"},{\"boundingBox\":[{\"x\":750,\"y\":505},{\"x\":809,\"y\":505},{\"x\":808,\"y\":531},{\"x\":749,\"y\":531}],\"text\":\"2013\"},{\"boundingBox\":[{\"x\":824,\"y\":505},{\"x\":883,\"y\":505},{\"x\":882,\"y\":531},{\"x\":823,\"y\":531}],\"text\":\"2014\"},{\"boundingBox\":[{\"x\":899,\"y\":505},{\"x\":957,\"y\":505},{\"x\":956,\"y\":531},{\"x\":898,\"y\":531}],\"text\":\"2015\"},{\"boundingBox\":[{\"x\":973,\"y\":505},{\"x\":1032,\"y\":505},{\"x\":1032,\"y\":531},{\"x\":972,\"y\":531}],\"text\":\"2016\"},{\"boundingBox\":[{\"x\":1047,\"y\":505},{\"x\":1108,\"y\":505},{\"x\":1107,\"y\":531},{\"x\":1046,\"y\":531}],\"text\":\"2017\"},{\"boundingBox\":[{\"x\":1122,\"y\":505},{\"x\":1179,\"y\":505},{\"x\":1178,\"y\":531},{\"x\":1122,\"y\":531}],\"text\":\"2018\"},{\"boundingBox\":[{\"x\":593,\"y\":554},{\"x\":673,\"y\":558},{\"x\":673,\"y\":580},{\"x\":592,\"y\":579}],\"text\":\"Years\"}]}",
        "{\"language\":\"en\",\"text\":\"Percentage of Publication Type 34,9% Journal Conferences 155, 41% 192, 50% . Workshop/Technical/Sym posium\",\"lines\":[{\"boundingBox\":[{\"x\":153,\"y\":2},{\"x\":611,\"y\":3},{\"x\":610,\"y\":31},{\"x\":153,\"y\":29}],\"text\":\"Percentage of Publication Type\"},{\"boundingBox\":[{\"x\":122,\"y\":79},{\"x\":210,\"y\":79},{\"x\":210,\"y\":105},{\"x\":122,\"y\":106}],\"text\":\"34,9%\"},{\"boundingBox\":[{\"x\":503,\"y\":123},{\"x\":610,\"y\":122},{\"x\":611,\"y\":145},{\"x\":503,\"y\":146}],\"text\":\"Journal\"},{\"boundingBox\":[{\"x\":502,\"y\":210},{\"x\":672,\"y\":210},{\"x\":672,\"y\":237},{\"x\":502,\"y\":236}],\"text\":\"Conferences\"},{\"boundingBox\":[{\"x\":20,\"y\":280},{\"x\":143,\"y\":279},{\"x\":143,\"y\":307},{\"x\":20,\"y\":307}],\"text\":\"155, 41%\"},{\"boundingBox\":[{\"x\":281,\"y\":259},{\"x\":403,\"y\":260},{\"x\":403,\"y\":286},{\"x\":281,\"y\":285}],\"text\":\"192, 50%\"},{\"boundingBox\":[{\"x\":502,\"y\":304},{\"x\":520,\"y\":305},{\"x\":519,\"y\":326},{\"x\":501,\"y\":326}],\"text\":\".\"},{\"boundingBox\":[{\"x\":521,\"y\":302},{\"x\":824,\"y\":301},{\"x\":824,\"y\":330},{\"x\":521,\"y\":331}],\"text\":\"Workshop/Technical/Sym\"},{\"boundingBox\":[{\"x\":522,\"y\":340},{\"x\":612,\"y\":338},{\"x\":612,\"y\":363},{\"x\":523,\"y\":365}],\"text\":\"posium\"}],\"words\":[{\"boundingBox\":[{\"x\":154,\"y\":4},{\"x\":313,\"y\":3},{\"x\":313,\"y\":30},{\"x\":154,\"y\":29}],\"text\":\"Percentage\"},{\"boundingBox\":[{\"x\":322,\"y\":3},{\"x\":356,\"y\":3},{\"x\":356,\"y\":30},{\"x\":322,\"y\":30}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":361,\"y\":3},{\"x\":525,\"y\":4},{\"x\":524,\"y\":31},{\"x\":361,\"y\":31}],\"text\":\"Publication\"},{\"boundingBox\":[{\"x\":539,\"y\":4},{\"x\":605,\"y\":5},{\"x\":604,\"y\":32},{\"x\":539,\"y\":31}],\"text\":\"Type\"},{\"boundingBox\":[{\"x\":122,\"y\":80},{\"x\":207,\"y\":79},{\"x\":207,\"y\":106},{\"x\":122,\"y\":107}],\"text\":\"34,9%\"},{\"boundingBox\":[{\"x\":520,\"y\":124},{\"x\":610,\"y\":123},{\"x\":611,\"y\":145},{\"x\":520,\"y\":147}],\"text\":\"Journal\"},{\"boundingBox\":[{\"x\":520,\"y\":211},{\"x\":670,\"y\":212},{\"x\":671,\"y\":237},{\"x\":521,\"y\":237}],\"text\":\"Conferences\"},{\"boundingBox\":[{\"x\":20,\"y\":280},{\"x\":76,\"y\":280},{\"x\":76,\"y\":308},{\"x\":20,\"y\":308}],\"text\":\"155,\"},{\"boundingBox\":[{\"x\":82,\"y\":280},{\"x\":137,\"y\":280},{\"x\":137,\"y\":308},{\"x\":81,\"y\":308}],\"text\":\"41%\"},{\"boundingBox\":[{\"x\":282,\"y\":260},{\"x\":337,\"y\":260},{\"x\":337,\"y\":286},{\"x\":281,\"y\":286}],\"text\":\"192,\"},{\"boundingBox\":[{\"x\":342,\"y\":260},{\"x\":397,\"y\":260},{\"x\":397,\"y\":287},{\"x\":342,\"y\":286}],\"text\":\"50%\"},{\"boundingBox\":[{\"x\":501,\"y\":304},{\"x\":513,\"y\":304},{\"x\":513,\"y\":326},{\"x\":501,\"y\":325}],\"text\":\".\"},{\"boundingBox\":[{\"x\":522,\"y\":303},{\"x\":819,\"y\":302},{\"x\":819,\"y\":331},{\"x\":521,\"y\":331}],\"text\":\"Workshop/Technical/Sym\"},{\"boundingBox\":[{\"x\":523,\"y\":340},{\"x\":600,\"y\":338},{\"x\":601,\"y\":364},{\"x\":523,\"y\":366}],\"text\":\"posium\"}]}",
        "{\"language\":\"en\",\"text\":\"90 80 70 60 50 40 w 20 0 No of Researchers Frequency of Researchers 10 0 Italy Canada China India USA Germany France Japan Turkey Republic of Korea Countries Ireland Spain Poland Los Angeles Switzerland Iran Greece Norway\",\"lines\":[{\"boundingBox\":[{\"x\":52,\"y\":40},{\"x\":87,\"y\":38},{\"x\":88,\"y\":69},{\"x\":53,\"y\":71}],\"text\":\"90\"},{\"boundingBox\":[{\"x\":54,\"y\":78},{\"x\":86,\"y\":78},{\"x\":86,\"y\":102},{\"x\":54,\"y\":103}],\"text\":\"80\"},{\"boundingBox\":[{\"x\":54,\"y\":114},{\"x\":86,\"y\":113},{\"x\":85,\"y\":137},{\"x\":53,\"y\":137}],\"text\":\"70\"},{\"boundingBox\":[{\"x\":55,\"y\":149},{\"x\":85,\"y\":148},{\"x\":84,\"y\":173},{\"x\":54,\"y\":173}],\"text\":\"60\"},{\"boundingBox\":[{\"x\":55,\"y\":187},{\"x\":86,\"y\":186},{\"x\":85,\"y\":211},{\"x\":53,\"y\":211}],\"text\":\"50\"},{\"boundingBox\":[{\"x\":55,\"y\":222},{\"x\":85,\"y\":222},{\"x\":84,\"y\":246},{\"x\":55,\"y\":246}],\"text\":\"40\"},{\"boundingBox\":[{\"x\":56,\"y\":281},{\"x\":55,\"y\":257},{\"x\":72,\"y\":257},{\"x\":72,\"y\":282}],\"text\":\"w\"},{\"boundingBox\":[{\"x\":53,\"y\":295},{\"x\":85,\"y\":294},{\"x\":85,\"y\":318},{\"x\":53,\"y\":319}],\"text\":\"20\"},{\"boundingBox\":[{\"x\":65,\"y\":285},{\"x\":66,\"y\":258},{\"x\":83,\"y\":258},{\"x\":82,\"y\":285}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":1,\"y\":357},{\"x\":2,\"y\":58},{\"x\":30,\"y\":58},{\"x\":29,\"y\":357}],\"text\":\"No of Researchers\"},{\"boundingBox\":[{\"x\":393,\"y\":2},{\"x\":833,\"y\":1},{\"x\":833,\"y\":35},{\"x\":393,\"y\":39}],\"text\":\"Frequency of Researchers\"},{\"boundingBox\":[{\"x\":52,\"y\":334},{\"x\":86,\"y\":333},{\"x\":85,\"y\":356},{\"x\":53,\"y\":357}],\"text\":\"10\"},{\"boundingBox\":[{\"x\":69,\"y\":394},{\"x\":67,\"y\":366},{\"x\":85,\"y\":366},{\"x\":85,\"y\":394}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":92,\"y\":432},{\"x\":134,\"y\":397},{\"x\":151,\"y\":416},{\"x\":108,\"y\":453}],\"text\":\"Italy\"},{\"boundingBox\":[{\"x\":124,\"y\":465},{\"x\":192,\"y\":395},{\"x\":209,\"y\":410},{\"x\":139,\"y\":482}],\"text\":\"Canada\"},{\"boundingBox\":[{\"x\":200,\"y\":446},{\"x\":250,\"y\":395},{\"x\":266,\"y\":412},{\"x\":215,\"y\":463}],\"text\":\"China\"},{\"boundingBox\":[{\"x\":263,\"y\":438},{\"x\":309,\"y\":395},{\"x\":325,\"y\":412},{\"x\":279,\"y\":457}],\"text\":\"India\"},{\"boundingBox\":[{\"x\":331,\"y\":432},{\"x\":369,\"y\":393},{\"x\":387,\"y\":413},{\"x\":351,\"y\":449}],\"text\":\"USA\"},{\"boundingBox\":[{\"x\":345,\"y\":478},{\"x\":431,\"y\":399},{\"x\":446,\"y\":417},{\"x\":360,\"y\":495}],\"text\":\"Germany\"},{\"boundingBox\":[{\"x\":427,\"y\":458},{\"x\":489,\"y\":396},{\"x\":505,\"y\":411},{\"x\":443,\"y\":474}],\"text\":\"France\"},{\"boundingBox\":[{\"x\":494,\"y\":449},{\"x\":548,\"y\":396},{\"x\":566,\"y\":414},{\"x\":511,\"y\":467}],\"text\":\"Japan\"},{\"boundingBox\":[{\"x\":544,\"y\":456},{\"x\":607,\"y\":398},{\"x\":624,\"y\":417},{\"x\":561,\"y\":475}],\"text\":\"Turkey\"},{\"boundingBox\":[{\"x\":505,\"y\":556},{\"x\":664,\"y\":395},{\"x\":685,\"y\":414},{\"x\":524,\"y\":577}],\"text\":\"Republic of Korea\"},{\"boundingBox\":[{\"x\":507,\"y\":597},{\"x\":664,\"y\":597},{\"x\":664,\"y\":624},{\"x\":507,\"y\":625}],\"text\":\"Countries\"},{\"boundingBox\":[{\"x\":657,\"y\":459},{\"x\":723,\"y\":392},{\"x\":744,\"y\":411},{\"x\":677,\"y\":480}],\"text\":\"Ireland\"},{\"boundingBox\":[{\"x\":734,\"y\":445},{\"x\":783,\"y\":396},{\"x\":801,\"y\":415},{\"x\":751,\"y\":465}],\"text\":\"Spain\"},{\"boundingBox\":[{\"x\":781,\"y\":457},{\"x\":844,\"y\":393},{\"x\":861,\"y\":410},{\"x\":798,\"y\":474}],\"text\":\"Poland\"},{\"boundingBox\":[{\"x\":796,\"y\":500},{\"x\":904,\"y\":393},{\"x\":924,\"y\":413},{\"x\":816,\"y\":520}],\"text\":\"Los Angeles\"},{\"boundingBox\":[{\"x\":853,\"y\":502},{\"x\":963,\"y\":392},{\"x\":983,\"y\":412},{\"x\":872,\"y\":521}],\"text\":\"Switzerland\"},{\"boundingBox\":[{\"x\":987,\"y\":431},{\"x\":1026,\"y\":396},{\"x\":1039,\"y\":411},{\"x\":1001,\"y\":448}],\"text\":\"Iran\"},{\"boundingBox\":[{\"x\":1018,\"y\":462},{\"x\":1083,\"y\":397},{\"x\":1098,\"y\":412},{\"x\":1033,\"y\":477}],\"text\":\"Greece\"},{\"boundingBox\":[{\"x\":1070,\"y\":463},{\"x\":1145,\"y\":396},{\"x\":1164,\"y\":416},{\"x\":1088,\"y\":484}],\"text\":\"Norway\"}],\"words\":[{\"boundingBox\":[{\"x\":52,\"y\":40},{\"x\":83,\"y\":38},{\"x\":85,\"y\":69},{\"x\":53,\"y\":71}],\"text\":\"90\"},{\"boundingBox\":[{\"x\":54,\"y\":78},{\"x\":81,\"y\":78},{\"x\":81,\"y\":103},{\"x\":54,\"y\":103}],\"text\":\"80\"},{\"boundingBox\":[{\"x\":54,\"y\":113},{\"x\":81,\"y\":113},{\"x\":81,\"y\":137},{\"x\":54,\"y\":137}],\"text\":\"70\"},{\"boundingBox\":[{\"x\":54,\"y\":148},{\"x\":81,\"y\":148},{\"x\":81,\"y\":173},{\"x\":54,\"y\":173}],\"text\":\"60\"},{\"boundingBox\":[{\"x\":53,\"y\":186},{\"x\":80,\"y\":186},{\"x\":80,\"y\":211},{\"x\":53,\"y\":211}],\"text\":\"50\"},{\"boundingBox\":[{\"x\":55,\"y\":222},{\"x\":81,\"y\":222},{\"x\":81,\"y\":246},{\"x\":55,\"y\":246}],\"text\":\"40\"},{\"boundingBox\":[{\"x\":56,\"y\":281},{\"x\":55,\"y\":271},{\"x\":72,\"y\":270},{\"x\":73,\"y\":281}],\"text\":\"w\"},{\"boundingBox\":[{\"x\":53,\"y\":295},{\"x\":80,\"y\":294},{\"x\":81,\"y\":318},{\"x\":54,\"y\":319}],\"text\":\"20\"},{\"boundingBox\":[{\"x\":66,\"y\":270},{\"x\":66,\"y\":259},{\"x\":83,\"y\":260},{\"x\":83,\"y\":271}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":1,\"y\":357},{\"x\":1,\"y\":314},{\"x\":28,\"y\":314},{\"x\":28,\"y\":357}],\"text\":\"No\"},{\"boundingBox\":[{\"x\":1,\"y\":302},{\"x\":1,\"y\":268},{\"x\":29,\"y\":269},{\"x\":29,\"y\":303}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":1,\"y\":262},{\"x\":2,\"y\":60},{\"x\":29,\"y\":62},{\"x\":29,\"y\":262}],\"text\":\"Researchers\"},{\"boundingBox\":[{\"x\":395,\"y\":2},{\"x\":572,\"y\":3},{\"x\":572,\"y\":39},{\"x\":393,\"y\":39}],\"text\":\"Frequency\"},{\"boundingBox\":[{\"x\":582,\"y\":3},{\"x\":618,\"y\":3},{\"x\":617,\"y\":38},{\"x\":581,\"y\":38}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":624,\"y\":3},{\"x\":833,\"y\":2},{\"x\":833,\"y\":30},{\"x\":624,\"y\":38}],\"text\":\"Researchers\"},{\"boundingBox\":[{\"x\":52,\"y\":334},{\"x\":80,\"y\":333},{\"x\":80,\"y\":356},{\"x\":53,\"y\":357}],\"text\":\"10\"},{\"boundingBox\":[{\"x\":68,\"y\":394},{\"x\":68,\"y\":384},{\"x\":86,\"y\":383},{\"x\":86,\"y\":393}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":93,\"y\":432},{\"x\":133,\"y\":399},{\"x\":149,\"y\":418},{\"x\":109,\"y\":452}],\"text\":\"Italy\"},{\"boundingBox\":[{\"x\":124,\"y\":464},{\"x\":190,\"y\":398},{\"x\":206,\"y\":415},{\"x\":140,\"y\":482}],\"text\":\"Canada\"},{\"boundingBox\":[{\"x\":200,\"y\":445},{\"x\":248,\"y\":398},{\"x\":264,\"y\":415},{\"x\":215,\"y\":462}],\"text\":\"China\"},{\"boundingBox\":[{\"x\":263,\"y\":439},{\"x\":307,\"y\":396},{\"x\":324,\"y\":414},{\"x\":280,\"y\":457}],\"text\":\"India\"},{\"boundingBox\":[{\"x\":332,\"y\":432},{\"x\":364,\"y\":400},{\"x\":383,\"y\":419},{\"x\":351,\"y\":450}],\"text\":\"USA\"},{\"boundingBox\":[{\"x\":346,\"y\":480},{\"x\":430,\"y\":400},{\"x\":447,\"y\":418},{\"x\":362,\"y\":495}],\"text\":\"Germany\"},{\"boundingBox\":[{\"x\":428,\"y\":458},{\"x\":486,\"y\":401},{\"x\":500,\"y\":418},{\"x\":444,\"y\":474}],\"text\":\"France\"},{\"boundingBox\":[{\"x\":494,\"y\":448},{\"x\":545,\"y\":399},{\"x\":563,\"y\":417},{\"x\":511,\"y\":467}],\"text\":\"Japan\"},{\"boundingBox\":[{\"x\":544,\"y\":457},{\"x\":606,\"y\":400},{\"x\":623,\"y\":418},{\"x\":563,\"y\":475}],\"text\":\"Turkey\"},{\"boundingBox\":[{\"x\":505,\"y\":555},{\"x\":582,\"y\":478},{\"x\":601,\"y\":498},{\"x\":525,\"y\":577}],\"text\":\"Republic\"},{\"boundingBox\":[{\"x\":587,\"y\":473},{\"x\":606,\"y\":454},{\"x\":625,\"y\":474},{\"x\":606,\"y\":494}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":610,\"y\":450},{\"x\":664,\"y\":396},{\"x\":683,\"y\":415},{\"x\":629,\"y\":470}],\"text\":\"Korea\"},{\"boundingBox\":[{\"x\":507,\"y\":600},{\"x\":661,\"y\":598},{\"x\":660,\"y\":625},{\"x\":508,\"y\":625}],\"text\":\"Countries\"},{\"boundingBox\":[{\"x\":658,\"y\":459},{\"x\":722,\"y\":395},{\"x\":741,\"y\":415},{\"x\":678,\"y\":480}],\"text\":\"Ireland\"},{\"boundingBox\":[{\"x\":734,\"y\":446},{\"x\":782,\"y\":398},{\"x\":801,\"y\":417},{\"x\":752,\"y\":465}],\"text\":\"Spain\"},{\"boundingBox\":[{\"x\":782,\"y\":457},{\"x\":841,\"y\":398},{\"x\":858,\"y\":415},{\"x\":798,\"y\":474}],\"text\":\"Poland\"},{\"boundingBox\":[{\"x\":797,\"y\":501},{\"x\":824,\"y\":475},{\"x\":844,\"y\":493},{\"x\":817,\"y\":520}],\"text\":\"Los\"},{\"boundingBox\":[{\"x\":828,\"y\":471},{\"x\":904,\"y\":394},{\"x\":925,\"y\":415},{\"x\":848,\"y\":489}],\"text\":\"Angeles\"},{\"boundingBox\":[{\"x\":853,\"y\":503},{\"x\":960,\"y\":396},{\"x\":979,\"y\":416},{\"x\":873,\"y\":521}],\"text\":\"Switzerland\"},{\"boundingBox\":[{\"x\":987,\"y\":432},{\"x\":1019,\"y\":402},{\"x\":1034,\"y\":418},{\"x\":1002,\"y\":448}],\"text\":\"Iran\"},{\"boundingBox\":[{\"x\":1019,\"y\":462},{\"x\":1080,\"y\":401},{\"x\":1094,\"y\":417},{\"x\":1034,\"y\":476}],\"text\":\"Greece\"},{\"boundingBox\":[{\"x\":1070,\"y\":464},{\"x\":1143,\"y\":398},{\"x\":1161,\"y\":420},{\"x\":1089,\"y\":484}],\"text\":\"Norway\"}]}",
        "{\"language\":\"en\",\"text\":\"Published online: 06 June 2019\",\"lines\":[{\"boundingBox\":[{\"x\":4,\"y\":14},{\"x\":889,\"y\":14},{\"x\":889,\"y\":70},{\"x\":4,\"y\":70}],\"text\":\"Published online: 06 June 2019\"}],\"words\":[{\"boundingBox\":[{\"x\":4,\"y\":15},{\"x\":270,\"y\":15},{\"x\":270,\"y\":71},{\"x\":4,\"y\":70}],\"text\":\"Published\"},{\"boundingBox\":[{\"x\":289,\"y\":15},{\"x\":495,\"y\":15},{\"x\":495,\"y\":71},{\"x\":290,\"y\":71}],\"text\":\"online:\"},{\"boundingBox\":[{\"x\":506,\"y\":15},{\"x\":574,\"y\":15},{\"x\":575,\"y\":71},{\"x\":506,\"y\":71}],\"text\":\"06\"},{\"boundingBox\":[{\"x\":594,\"y\":15},{\"x\":725,\"y\":15},{\"x\":726,\"y\":71},{\"x\":594,\"y\":71}],\"text\":\"June\"},{\"boundingBox\":[{\"x\":744,\"y\":15},{\"x\":886,\"y\":15},{\"x\":887,\"y\":70},{\"x\":745,\"y\":71}],\"text\":\"2019\"}]}"
      ],
      "pii_entities": [
        {
          "text": "Taiwo Kolajo",
          "type": "Person",
          "subtype": null,
          "offset": 59,
          "length": 12,
          "score": 0.98
        },
        {
          "text": "Olawande Daramola3",
          "type": "Person",
          "subtype": null,
          "offset": 78,
          "length": 18,
          "score": 0.96
        },
        {
          "text": "Ayodele Adebiyi",
          "type": "Person",
          "subtype": null,
          "offset": 102,
          "length": 15,
          "score": 0.97
        },
        {
          "text": "recently",
          "type": "DateTime",
          "subtype": null,
          "offset": 459,
          "length": 8,
          "score": 0.8
        },
        {
          "text": "Recently",
          "type": "DateTime",
          "subtype": null,
          "offset": 708,
          "length": 8,
          "score": 0.8
        },
        {
          "text": "Scopus",
          "type": "Organization",
          "subtype": null,
          "offset": 1422,
          "length": 6,
          "score": 0.76
        },
        {
          "text": "ScienceDirect",
          "type": "Organization",
          "subtype": null,
          "offset": 1430,
          "length": 13,
          "score": 0.82
        },
        {
          "text": "EBSCO",
          "type": "Organization",
          "subtype": null,
          "offset": 1448,
          "length": 5,
          "score": 0.74
        },
        {
          "text": "IEEE",
          "type": "Organization",
          "subtype": null,
          "offset": 1533,
          "length": 4,
          "score": 0.97
        },
        {
          "text": "ACM",
          "type": "Organization",
          "subtype": null,
          "offset": 1539,
          "length": 3,
          "score": 0.97
        },
        {
          "text": "SpringerLink",
          "type": "Organization",
          "subtype": null,
          "offset": 1544,
          "length": 12,
          "score": 0.97
        },
        {
          "text": "Elsevier",
          "type": "Organization",
          "subtype": null,
          "offset": 1562,
          "length": 8,
          "score": 0.97
        },
        {
          "text": "The Author",
          "type": "Organization",
          "subtype": null,
          "offset": 2910,
          "length": 10,
          "score": 0.68
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 2924,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "author",
          "type": "PersonType",
          "subtype": null,
          "offset": 3220,
          "length": 6,
          "score": 0.89
        },
        {
          "text": "Kolajo",
          "type": "Person",
          "subtype": null,
          "offset": 3345,
          "length": 6,
          "score": 0.97
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 3382,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "6:47",
          "type": "DateTime",
          "subtype": "Time",
          "offset": 3388,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "https://doi.org/10.1186/s40537-019-0210-7",
          "type": "URL",
          "subtype": null,
          "offset": 3395,
          "length": 41,
          "score": 0.8
        },
        {
          "text": "taiwo.kolajo@stu.cu.edu.ng",
          "type": "Email",
          "subtype": null,
          "offset": 3458,
          "length": 26,
          "score": 0.8
        },
        {
          "text": "taiwo.kolajo@fulokoja.edu.ng",
          "type": "Email",
          "subtype": null,
          "offset": 3487,
          "length": 28,
          "score": 0.8
        },
        {
          "text": "Department of Computer",
          "type": "Organization",
          "subtype": null,
          "offset": 3519,
          "length": 22,
          "score": 0.72
        },
        {
          "text": "Covenant University",
          "type": "Organization",
          "subtype": null,
          "offset": 3570,
          "length": 19,
          "score": 0.92
        },
        {
          "text": "http://orcid.org/0000-0001-6780-2495",
          "type": "URL",
          "subtype": null,
          "offset": 3679,
          "length": 36,
          "score": 0.8
        },
        {
          "text": "http://orcid.org/0000-0001-6340-078X",
          "type": "URL",
          "subtype": null,
          "offset": 3716,
          "length": 36,
          "score": 0.8
        },
        {
          "text": "http://orcid.org/0000-0002-3114-6315",
          "type": "URL",
          "subtype": null,
          "offset": 3753,
          "length": 36,
          "score": 0.8
        },
        {
          "text": "http://creativecommons.org/licenses/by/4.0/",
          "type": "URL",
          "subtype": null,
          "offset": 3790,
          "length": 43,
          "score": 0.8
        },
        {
          "text": "http://crossmark.crossref.org/dialog",
          "type": "URL",
          "subtype": null,
          "offset": 3834,
          "length": 36,
          "score": 0.8
        },
        {
          "text": "30Kolajo",
          "type": "Person",
          "subtype": null,
          "offset": 3925,
          "length": 8,
          "score": 0.91
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 3964,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "6:47",
          "type": "DateTime",
          "subtype": "Time",
          "offset": 3970,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Kafka",
          "type": "Person",
          "subtype": null,
          "offset": 6778,
          "length": 5,
          "score": 0.97
        },
        {
          "text": "Spark",
          "type": "Person",
          "subtype": null,
          "offset": 6789,
          "length": 5,
          "score": 0.9
        },
        {
          "text": "30Kolajo",
          "type": "Person",
          "subtype": null,
          "offset": 7370,
          "length": 8,
          "score": 0.93
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 7409,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "6:47",
          "type": "DateTime",
          "subtype": "Time",
          "offset": 7415,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "operators",
          "type": "PersonType",
          "subtype": null,
          "offset": 8189,
          "length": 9,
          "score": 0.61
        },
        {
          "text": "A few seconds",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 9459,
          "length": 13,
          "score": 0.8
        },
        {
          "text": "NYMEX",
          "type": "Organization",
          "subtype": null,
          "offset": 9807,
          "length": 5,
          "score": 0.51
        },
        {
          "text": "30Kolajo",
          "type": "Person",
          "subtype": null,
          "offset": 9827,
          "length": 8,
          "score": 0.95
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 9866,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "6:47",
          "type": "DateTime",
          "subtype": "Time",
          "offset": 9872,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "processors",
          "type": "PersonType",
          "subtype": null,
          "offset": 10855,
          "length": 10,
          "score": 0.65
        },
        {
          "text": "Moore",
          "type": "Person",
          "subtype": null,
          "offset": 10873,
          "length": 5,
          "score": 0.97
        },
        {
          "text": "30Kolajo",
          "type": "Person",
          "subtype": null,
          "offset": 12610,
          "length": 8,
          "score": 0.94
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 12649,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "6:47",
          "type": "DateTime",
          "subtype": "Time",
          "offset": 12655,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "24/7",
          "type": "DateTime",
          "subtype": "Date",
          "offset": 13310,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Data Cooperation",
          "type": "Organization",
          "subtype": null,
          "offset": 14126,
          "length": 16,
          "score": 0.73
        },
        {
          "text": "IDC",
          "type": "Organization",
          "subtype": "Sports",
          "offset": 14144,
          "length": 3,
          "score": 0.9
        },
        {
          "text": "30Kolajo",
          "type": "Person",
          "subtype": null,
          "offset": 15268,
          "length": 8,
          "score": 0.92
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 15307,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "6:47",
          "type": "DateTime",
          "subtype": "Time",
          "offset": 15313,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Authors",
          "type": "PersonType",
          "subtype": null,
          "offset": 15320,
          "length": 7,
          "score": 0.89
        },
        {
          "text": "Scopus",
          "type": "Organization",
          "subtype": null,
          "offset": 17927,
          "length": 6,
          "score": 0.73
        },
        {
          "text": "Science Direct",
          "type": "Organization",
          "subtype": null,
          "offset": 17935,
          "length": 14,
          "score": 0.86
        },
        {
          "text": "EBSCOhost",
          "type": "Organization",
          "subtype": null,
          "offset": 17954,
          "length": 9,
          "score": 0.75
        },
        {
          "text": "30Kolajo",
          "type": "Person",
          "subtype": null,
          "offset": 18202,
          "length": 8,
          "score": 0.94
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 18241,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "6:47",
          "type": "DateTime",
          "subtype": "Time",
          "offset": 18247,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "2004",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 18678,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Elsevier",
          "type": "Organization",
          "subtype": null,
          "offset": 19069,
          "length": 8,
          "score": 0.85
        },
        {
          "text": "researchers",
          "type": "PersonType",
          "subtype": null,
          "offset": 19114,
          "length": 11,
          "score": 0.95
        },
        {
          "text": "students",
          "type": "PersonType",
          "subtype": null,
          "offset": 19127,
          "length": 8,
          "score": 0.96
        },
        {
          "text": "teachers",
          "type": "PersonType",
          "subtype": null,
          "offset": 19137,
          "length": 8,
          "score": 0.98
        },
        {
          "text": "information professionals",
          "type": "PersonType",
          "subtype": null,
          "offset": 19147,
          "length": 25,
          "score": 0.65
        },
        {
          "text": "researchers",
          "type": "PersonType",
          "subtype": null,
          "offset": 19743,
          "length": 11,
          "score": 0.92
        },
        {
          "text": "academic",
          "type": "PersonType",
          "subtype": null,
          "offset": 19827,
          "length": 8,
          "score": 0.73
        },
        {
          "text": "researchers",
          "type": "PersonType",
          "subtype": null,
          "offset": 19836,
          "length": 11,
          "score": 0.76
        },
        {
          "text": "1500",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 20048,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "scientists",
          "type": "PersonType",
          "subtype": null,
          "offset": 20141,
          "length": 10,
          "score": 0.81
        },
        {
          "text": "researchers",
          "type": "PersonType",
          "subtype": null,
          "offset": 20156,
          "length": 11,
          "score": 0.91
        },
        {
          "text": "collaborators",
          "type": "PersonType",
          "subtype": null,
          "offset": 20220,
          "length": 13,
          "score": 0.95
        },
        {
          "text": "researchers",
          "type": "PersonType",
          "subtype": null,
          "offset": 20297,
          "length": 11,
          "score": 0.97
        },
        {
          "text": "Scopus",
          "type": "Organization",
          "subtype": null,
          "offset": 20477,
          "length": 6,
          "score": 0.64
        },
        {
          "text": "ScienceDirect",
          "type": "Organization",
          "subtype": null,
          "offset": 20485,
          "length": 13,
          "score": 0.74
        },
        {
          "text": "EBSCOhost",
          "type": "Organization",
          "subtype": null,
          "offset": 20503,
          "length": 9,
          "score": 0.54
        },
        {
          "text": "s.com",
          "type": "Organization",
          "subtype": null,
          "offset": 20786,
          "length": 5,
          "score": 0.9
        },
        {
          "text": "ect.com",
          "type": "Organization",
          "subtype": null,
          "offset": 20818,
          "length": 7,
          "score": 0.9
        },
        {
          "text": "host.com",
          "type": "Organization",
          "subtype": null,
          "offset": 20848,
          "length": 8,
          "score": 0.9
        },
        {
          "text": "ate.net",
          "type": "Organization",
          "subtype": null,
          "offset": 20885,
          "length": 7,
          "score": 0.9
        },
        {
          "text": "http://www.scopus.com",
          "type": "URL",
          "subtype": null,
          "offset": 20895,
          "length": 21,
          "score": 0.8
        },
        {
          "text": "http://www.sciencedirect.com",
          "type": "URL",
          "subtype": null,
          "offset": 20917,
          "length": 28,
          "score": 0.8
        },
        {
          "text": "https://www.ebscohost.com",
          "type": "URL",
          "subtype": null,
          "offset": 20946,
          "length": 25,
          "score": 0.8
        },
        {
          "text": "https://www.reseaarchgate.net",
          "type": "URL",
          "subtype": null,
          "offset": 20972,
          "length": 29,
          "score": 0.8
        },
        {
          "text": "30Kolajo",
          "type": "Person",
          "subtype": null,
          "offset": 21014,
          "length": 8,
          "score": 0.96
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 21053,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "6:47",
          "type": "DateTime",
          "subtype": "Time",
          "offset": 21059,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "ACM",
          "type": "Organization",
          "subtype": null,
          "offset": 21250,
          "length": 3,
          "score": 0.86
        },
        {
          "text": "IEEE",
          "type": "Organization",
          "subtype": null,
          "offset": 21255,
          "length": 4,
          "score": 0.94
        },
        {
          "text": "SpringerLink",
          "type": "Organization",
          "subtype": null,
          "offset": 21261,
          "length": 12,
          "score": 0.93
        },
        {
          "text": "Elsevier",
          "type": "Organization",
          "subtype": null,
          "offset": 21275,
          "length": 8,
          "score": 0.93
        },
        {
          "text": "between 2004 and 2018",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 21324,
          "length": 21,
          "score": 0.8
        },
        {
          "text": "The year",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 21347,
          "length": 8,
          "score": 0.8
        },
        {
          "text": "2004",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 21454,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "1989",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 21486,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Microsoft",
          "type": "Organization",
          "subtype": null,
          "offset": 22074,
          "length": 9,
          "score": 0.72
        },
        {
          "text": "2004",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 22786,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "2018",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 22795,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Scopus ScienceDirect EBSCOhost",
          "type": "Organization",
          "subtype": null,
          "offset": 22962,
          "length": 30,
          "score": 0.58
        },
        {
          "text": "2097",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 23017,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "3 Second",
          "type": "DateTime",
          "subtype": "Duration",
          "offset": 23041,
          "length": 8,
          "score": 0.8
        },
        {
          "text": "Scopus ScienceDirect EBSCOhost",
          "type": "Organization",
          "subtype": null,
          "offset": 23072,
          "length": 30,
          "score": 0.58
        },
        {
          "text": "196 27 92 315",
          "type": "PhoneNumber",
          "subtype": null,
          "offset": 23127,
          "length": 13,
          "score": 0.8
        },
        {
          "text": "Scopus ScienceDirect EBSCOhost",
          "type": "Organization",
          "subtype": null,
          "offset": 23189,
          "length": 30,
          "score": 0.58
        },
        {
          "text": "64 23 24 111",
          "type": "PhoneNumber",
          "subtype": null,
          "offset": 23244,
          "length": 12,
          "score": 0.8
        },
        {
          "text": "Scopus ScienceDirect EBSCOhost",
          "type": "Organization",
          "subtype": null,
          "offset": 23283,
          "length": 30,
          "score": 0.58
        },
        {
          "text": "25 10 12 47",
          "type": "PhoneNumber",
          "subtype": null,
          "offset": 23338,
          "length": 11,
          "score": 0.8
        },
        {
          "text": "30Kolajo",
          "type": "Person",
          "subtype": null,
          "offset": 23363,
          "length": 8,
          "score": 0.95
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 23402,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "6:47",
          "type": "DateTime",
          "subtype": "Time",
          "offset": 23408,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "now",
          "type": "DateTime",
          "subtype": null,
          "offset": 23820,
          "length": 3,
          "score": 0.8
        },
        {
          "text": "users",
          "type": "PersonType",
          "subtype": null,
          "offset": 26134,
          "length": 5,
          "score": 0.93
        },
        {
          "text": "30Kolajo",
          "type": "Person",
          "subtype": null,
          "offset": 26205,
          "length": 8,
          "score": 0.96
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 26244,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "6:47",
          "type": "DateTime",
          "subtype": "Time",
          "offset": 26250,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "academic",
          "type": "PersonType",
          "subtype": null,
          "offset": 27474,
          "length": 8,
          "score": 0.81
        },
        {
          "text": "researchers",
          "type": "PersonType",
          "subtype": null,
          "offset": 27483,
          "length": 11,
          "score": 0.71
        },
        {
          "text": "developer",
          "type": "PersonType",
          "subtype": null,
          "offset": 27498,
          "length": 9,
          "score": 0.88
        },
        {
          "text": "may",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 27556,
          "length": 3,
          "score": 0.8
        },
        {
          "text": "Kafka",
          "type": "Person",
          "subtype": null,
          "offset": 28032,
          "length": 5,
          "score": 0.98
        },
        {
          "text": "Yahoo",
          "type": "Organization",
          "subtype": null,
          "offset": 28060,
          "length": 5,
          "score": 0.6
        },
        {
          "text": "Apache Samza",
          "type": "Person",
          "subtype": null,
          "offset": 28087,
          "length": 12,
          "score": 0.55
        },
        {
          "text": "EsperTech",
          "type": "Organization",
          "subtype": null,
          "offset": 28176,
          "length": 9,
          "score": 0.6
        },
        {
          "text": "Redis",
          "type": "Person",
          "subtype": null,
          "offset": 28198,
          "length": 5,
          "score": 0.54
        },
        {
          "text": "SAMOA",
          "type": "Person",
          "subtype": null,
          "offset": 28232,
          "length": 5,
          "score": 0.51
        },
        {
          "text": "ETALIS",
          "type": "Organization",
          "subtype": null,
          "offset": 28276,
          "length": 6,
          "score": 0.63
        },
        {
          "text": "30Kolajo",
          "type": "Person",
          "subtype": null,
          "offset": 28355,
          "length": 8,
          "score": 0.95
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 28394,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "6:47",
          "type": "DateTime",
          "subtype": "Time",
          "offset": 28400,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "Anodot",
          "type": "Person",
          "subtype": null,
          "offset": 30579,
          "length": 6,
          "score": 0.52
        },
        {
          "text": "Microsoft",
          "type": "Organization",
          "subtype": null,
          "offset": 30700,
          "length": 9,
          "score": 0.63
        },
        {
          "text": "IBM",
          "type": "Organization",
          "subtype": null,
          "offset": 30740,
          "length": 3,
          "score": 0.63
        },
        {
          "text": "Artemis",
          "type": "Person",
          "subtype": null,
          "offset": 30797,
          "length": 7,
          "score": 0.72
        },
        {
          "text": "Microsoft",
          "type": "Organization",
          "subtype": null,
          "offset": 30834,
          "length": 9,
          "score": 0.82
        },
        {
          "text": "30Kolajo",
          "type": "Person",
          "subtype": null,
          "offset": 30984,
          "length": 8,
          "score": 0.95
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 31023,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "6:47",
          "type": "DateTime",
          "subtype": "Time",
          "offset": 31029,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "KTS",
          "type": "Organization",
          "subtype": null,
          "offset": 31633,
          "length": 3,
          "score": 0.68
        },
        {
          "text": "Fuzzy",
          "type": "Organization",
          "subtype": null,
          "offset": 32799,
          "length": 5,
          "score": 0.65
        },
        {
          "text": "AFP",
          "type": "Organization",
          "subtype": null,
          "offset": 32810,
          "length": 3,
          "score": 0.57
        },
        {
          "text": "30Kolajo",
          "type": "Person",
          "subtype": null,
          "offset": 32972,
          "length": 8,
          "score": 0.95
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 33011,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "6:47",
          "type": "DateTime",
          "subtype": "Time",
          "offset": 33017,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "10",
          "type": "DateTime",
          "subtype": "Date",
          "offset": 34168,
          "length": 2,
          "score": 0.8
        },
        {
          "text": "may",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 34517,
          "length": 3,
          "score": 0.8
        },
        {
          "text": "IBMInfoS",
          "type": "Organization",
          "subtype": null,
          "offset": 35957,
          "length": 8,
          "score": 0.72
        },
        {
          "text": "30Kolajo",
          "type": "Person",
          "subtype": null,
          "offset": 36667,
          "length": 8,
          "score": 0.95
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 36706,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "6:47",
          "type": "DateTime",
          "subtype": "Time",
          "offset": 36712,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "lia",
          "type": "Person",
          "subtype": null,
          "offset": 36996,
          "length": 3,
          "score": 0.57
        },
        {
          "text": "Li",
          "type": "Person",
          "subtype": null,
          "offset": 37367,
          "length": 2,
          "score": 0.59
        },
        {
          "text": "PU",
          "type": "Organization",
          "subtype": null,
          "offset": 37797,
          "length": 2,
          "score": 0.52
        },
        {
          "text": "PU",
          "type": "Organization",
          "subtype": null,
          "offset": 38461,
          "length": 2,
          "score": 0.52
        },
        {
          "text": "PU",
          "type": "Organization",
          "subtype": null,
          "offset": 39064,
          "length": 2,
          "score": 0.52
        },
        {
          "text": "Li",
          "type": "Person",
          "subtype": null,
          "offset": 39198,
          "length": 2,
          "score": 0.59
        },
        {
          "text": "liz",
          "type": "Person",
          "subtype": null,
          "offset": 39395,
          "length": 3,
          "score": 0.57
        },
        {
          "text": "30Kolajo",
          "type": "Person",
          "subtype": null,
          "offset": 39462,
          "length": 8,
          "score": 0.95
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 39501,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "6:47",
          "type": "DateTime",
          "subtype": "Time",
          "offset": 39507,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "lia",
          "type": "Person",
          "subtype": null,
          "offset": 39714,
          "length": 3,
          "score": 0.57
        },
        {
          "text": "FS",
          "type": "Organization",
          "subtype": null,
          "offset": 39876,
          "length": 2,
          "score": 0.84
        },
        {
          "text": "Li",
          "type": "Person",
          "subtype": null,
          "offset": 40100,
          "length": 2,
          "score": 0.59
        },
        {
          "text": "Li",
          "type": "Person",
          "subtype": null,
          "offset": 40645,
          "length": 2,
          "score": 0.59
        },
        {
          "text": "Li",
          "type": "Person",
          "subtype": null,
          "offset": 41254,
          "length": 2,
          "score": 0.59
        },
        {
          "text": "RQ",
          "type": "Organization",
          "subtype": null,
          "offset": 41803,
          "length": 2,
          "score": 0.52
        },
        {
          "text": "Li",
          "type": "Person",
          "subtype": null,
          "offset": 41984,
          "length": 2,
          "score": 0.59
        },
        {
          "text": "Li",
          "type": "Person",
          "subtype": null,
          "offset": 42366,
          "length": 2,
          "score": 0.59
        },
        {
          "text": "ifi",
          "type": "Organization",
          "subtype": null,
          "offset": 42392,
          "length": 3,
          "score": 0.66
        },
        {
          "text": "30Kolajo",
          "type": "Person",
          "subtype": null,
          "offset": 42538,
          "length": 8,
          "score": 0.94
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 42577,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "6:47",
          "type": "DateTime",
          "subtype": "Time",
          "offset": 42583,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "lia",
          "type": "Person",
          "subtype": null,
          "offset": 42790,
          "length": 3,
          "score": 0.57
        },
        {
          "text": "CQ",
          "type": "Organization",
          "subtype": null,
          "offset": 42913,
          "length": 2,
          "score": 0.53
        },
        {
          "text": "Li",
          "type": "Person",
          "subtype": null,
          "offset": 43076,
          "length": 2,
          "score": 0.59
        },
        {
          "text": "Li",
          "type": "Person",
          "subtype": null,
          "offset": 43378,
          "length": 2,
          "score": 0.59
        },
        {
          "text": "RQ",
          "type": "Organization",
          "subtype": null,
          "offset": 43452,
          "length": 2,
          "score": 0.52
        },
        {
          "text": "Li",
          "type": "Person",
          "subtype": null,
          "offset": 43766,
          "length": 2,
          "score": 0.59
        },
        {
          "text": "IB",
          "type": "Organization",
          "subtype": null,
          "offset": 43953,
          "length": 2,
          "score": 0.7
        },
        {
          "text": "Li",
          "type": "Person",
          "subtype": null,
          "offset": 44468,
          "length": 2,
          "score": 0.59
        },
        {
          "text": "Li",
          "type": "Person",
          "subtype": null,
          "offset": 45010,
          "length": 2,
          "score": 0.59
        },
        {
          "text": "30Kolajo",
          "type": "Person",
          "subtype": null,
          "offset": 45238,
          "length": 8,
          "score": 0.94
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 45277,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "6:47",
          "type": "DateTime",
          "subtype": "Time",
          "offset": 45283,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "lia",
          "type": "Person",
          "subtype": null,
          "offset": 45490,
          "length": 3,
          "score": 0.57
        },
        {
          "text": "BM",
          "type": "Organization",
          "subtype": null,
          "offset": 45718,
          "length": 2,
          "score": 0.54
        },
        {
          "text": "Li",
          "type": "Person",
          "subtype": null,
          "offset": 45882,
          "length": 2,
          "score": 0.59
        },
        {
          "text": "BM",
          "type": "Organization",
          "subtype": null,
          "offset": 47121,
          "length": 2,
          "score": 0.54
        },
        {
          "text": "FS",
          "type": "Organization",
          "subtype": null,
          "offset": 47233,
          "length": 2,
          "score": 0.84
        },
        {
          "text": "Li",
          "type": "Person",
          "subtype": null,
          "offset": 47427,
          "length": 2,
          "score": 0.59
        },
        {
          "text": "30Kolajo",
          "type": "Person",
          "subtype": null,
          "offset": 47654,
          "length": 8,
          "score": 0.95
        },
        {
          "text": "2019",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 47693,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "6:47",
          "type": "DateTime",
          "subtype": "Time",
          "offset": 47699,
          "length": 4,
          "score": 0.8
        },
        {
          "text": "may",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 48397,
          "length": 3,
          "score": 0.8
        },
        {
          "text": "operator",
          "type": "PersonType",
          "subtype": null,
          "offset": 48784,
          "length": 8,
          "score": 0.63
        },
        {
          "text": "user",
          "type": "PersonType",
          "subtype": null,
          "offset": 48812,
          "length": 4,
          "score": 0.63
        },
        {
          "text": "Trident",
          "type": "Organization",
          "subtype": "Sports",
          "offset": 49065,
          "length": 7,
          "score": 0.95
        },
        {
          "text": "may",
          "type": "DateTime",
          "subtype": "DateRange",
          "offset": 49552,
          "length": 3,
          "score": 0.8
        },
        {
          "text": "Redis",
          "type": "Organization",
          "subtype": null,
          "offset": 49873,
          "length": 5,
          "score": 0.64
        }
      ]
    }
  ]
}